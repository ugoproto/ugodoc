{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Let there be light! A Data Science documentation website. ugodoc is a corpus; a catalog of books, manuals, articles, presentations, videos, podcasts, summaries, notes, code snippets, excerpts, websites, etc. The \u2018docs\u2019 is a searchable knowledge-based system. You type a keyword, it leads to several sources, you identify the document, and you go retrieve the document; whether it is a digital or a material document. Fast and easy! The corpus is unstructured. Knowledge is rather cumulated, layer after layer, resulting in a hotchpotch of information. Information may be repeted among many documents, with different explanations, some more comprehensive. Newer entries might also supplement or contradict older entries.","title":"Home"},{"location":"AWS/","text":"Foreword A step-by-step tutorial to set up and use Jupyter Notebook on Amazon Web Services (AWS) Elastic Compute Cloud (EC2) for machine learning. From DataCamp. Also, to find out more about AWS EC2 and a data science environment on a Windows instance, consult this article from DataCamp . Among the services: Amazon EC2; Amazon QuickSight; Amazon RDS; AWS Lambda. You can learn how to launch a virtual machine, create a web app, connect an IoT card, register a domain, etc. There are tutorial videos and workshops for web sites (host, node.js web app, Python web app, etc.), DevOps (CI/CD Pipeline, repos, etc.), Big Data (Hadoop+Hive, ML, log analytics solution pipeline), databases, wireless, etc. For a deeper understanding of AWS EC2: Getting an Amazon Web Services Account \u00b6 On the Amazon Web Services or in [French]( Amazon Web Services , click on \u201cSign In to the Console\u201d. Sign in if you have account. If you don\u2019t, you will need to make one. Setting Up a Cloud Computing Server \u00b6 Amazon offers an enormous range of cloud computing services. Pick EC2 service (short for Elastic Cloud Compute). The upper left part of your browser window offers a menu for services. Open this menu and select EC2, which should be the first option. This brings you to a page with information about status of all EC2 computing instances you have running. Select the button in the middle of this page to Launch Instance. Each computing instance comes preloaded with different software (in \u201cmachine images\u201d), and we need to decide what we want preloaded. In our case, we want to use an image from the AWS Marketplace that is optimized for deep learning. Select AWS Marketplace on the left menu bar. Then enter \u201cdeep learning ubuntu\u201d in the search space. This will bring up some options including Amazon\u2019s official \u201cDeep Learning AMI (Ubuntu)\u201d (Version 14.0 or higher). Or select it from the Quick Launch left menu bar. You will now be presented with a menu of instance types. Each instance type has a different price, and a different computational capabilities. The instances with GPU\u2019s are those that start with either g2 or p2. We recommend the \u00abp2.xlarge\u00bb, which costs a little less than a dollar an hour. If you want something less expensive, the \u00abg2.2xlarge\u00bb is about $0.65/hr, though it isn\u2019t quite as fast or as powerful. After you have selected your instance type, select Configure Instance Details at the bottom of the page. Then select 6. Configure Security Group on the menu towards the top of the page. Here, we will set up your instance to make it easy to access from your computer. Each row describes rules for how one can access your instance. We will use Jupyter notebooks, which are served on port 8888. If you don\u2019t understand this yet, you will see how to make it work soon. For now, click the Add Rule button. In the new column select TCP for the protocol, 8888 for the Port, and 0.0.0.0/0 for the last column, which is the source. Finally click the Review and Launch button on the bottom of your screen. Then press Launch again. You will now get to the last bit of security\u2026 which is selecting a key pair. The key is a file you have on your computer, which must match a file stored on the server. This is how you prevent others from using the server you just set up. So, don\u2019t share keys with others or put the file anywhere public. Select Create a new key pair (or choose an existing pair) and type in a name for your key (\u00abCloudAWSTutorial.pem\u00bb). Then press the Download Key Pair button to get a copy of your key, which you will need to access your server. You will soon be brought to a screen that looks like what you this: The long string of characters in blue is the name of your instance. You can select that to see your EC2 dashboard, which shows this server running. Connecting to Your Server \u00b6 We will now connect to our server using a protocol called ssh. From there, we will start a Jupyter notebook server, which we can use through the browser. On a Mac or Linux computer, you can use the ssh command. In Windows, many people use an application called PuTTY for ssh connections. You may need to change the access priveleges for the file for your key. In MacOS and Linux this can be done with the command: chmod 400 my_private_key.pem . Then, still at the command line, specify that you want this key to be available for authentication when logging into your server with: ssh-add my_private_key.pem . To log into our server, we will need the servers IP address. This can be found in the EC2 dashboard in your browser. Copy it\u2026 Log into this at the command line with the command: ssh ubuntu @18.219.71.98 . You will want to replace 18.219.71.98 with your server\u2019s IP address. Once logged into your server, use the ipython command to start an IPython shell. Once in IPython, try importing keras (or another package) to ensure everything works. You will see some message indicating the CUDA is being used\u2026 which mean keras is accessing the GPU. If you are comfortable with Linux, IPython, and a Linux text editor, you can work from here. Some people choose to use two windows. One to ssh to my server and use IPython, and another to ssh to the server and use the text editor. Setting up Your Jupyter Notebook \u00b6 However, most people will find it easier to set up Jupyter notebooks and program through the browser. For this, we will want to set up a new password. The first step is to get the hash of whatever password you want to use. You do that with the following in IPython: 1 2 3 from IPython.lib import passwd passwd ( 'use a better password than this' ) Copy the hash that is output after you set your password. You will need that again in a moment. Type exit to get out of IPython (back in the virtual terminal). Then use a text editor to edit ~/.jupyter/jupyter_notebook_config.py. Use the command: nano ~/.jupyter/jupyter_notebook_config.py . At the top of that file, paste the following. You can copy all of this exactly, and just replace the password. Remember, don\u2019t use your actual password. Copy in the hash of your password that you created earlier: 1 2 3 4 5 6 7 8 c = get_config() c.IPKernelApp.pylab = 'inline' c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem' c.NotebookApp.ip = '*' c.NotebookApp.open_browser = False c.NotebookApp.password = u'sha1:a9985e250966:a1401151cca1771a9fbc88d276fde3482330aa93' c.NotebookApp.port = 8888 Exit and save the file. Connecting to Jupyter in the Browser \u00b6 Go to the browser of your choice, and enter the address of your instance (available in the EC2 dashboard) followed by :8888. You may see a security message at this point. You shouldn\u2019t need to worry about this. In Chrome, you can click an \u201cadvanced\u201d button in your main browser window to bypass this security message. You will now be prompted for your password. Type in your password. This is not the hash of your password, but rather the raw password that you previously typed into IPython to get the hash value. Congrats, you are logged in! Using The Notebook \u00b6 From within the browser, select on the menu to create a new Jupyter notebook. Getting these notebook capabilities required a lot of setup. Fortunately, it\u2019s mostly a 1-time effort. You can now stop your server in the EC2 dashboard whenever you aren\u2019t using it, and you will stop paying for it. Stopping an instance will keep all of your setup effort, but the instance will stop working until you restart it. In some ways, this is like a pause button. If you selected terminate it would lose all information about that instance, and you would have to redo the setup next time. Since you selected stop, you can now go back to the ec2 dashboard on another day and click start. Once you click start, it\u2019s pretty easy to get up and running again. Your instance will get a new IP address. You will need to ssh into that new IP address. You can then go back to your browser, and immediately start working again.","title":"AWS"},{"location":"AWS/#getting-an-amazon-web-services-account","text":"On the Amazon Web Services or in [French]( Amazon Web Services , click on \u201cSign In to the Console\u201d. Sign in if you have account. If you don\u2019t, you will need to make one.","title":"Getting an Amazon Web Services Account"},{"location":"AWS/#setting-up-a-cloud-computing-server","text":"Amazon offers an enormous range of cloud computing services. Pick EC2 service (short for Elastic Cloud Compute). The upper left part of your browser window offers a menu for services. Open this menu and select EC2, which should be the first option. This brings you to a page with information about status of all EC2 computing instances you have running. Select the button in the middle of this page to Launch Instance. Each computing instance comes preloaded with different software (in \u201cmachine images\u201d), and we need to decide what we want preloaded. In our case, we want to use an image from the AWS Marketplace that is optimized for deep learning. Select AWS Marketplace on the left menu bar. Then enter \u201cdeep learning ubuntu\u201d in the search space. This will bring up some options including Amazon\u2019s official \u201cDeep Learning AMI (Ubuntu)\u201d (Version 14.0 or higher). Or select it from the Quick Launch left menu bar. You will now be presented with a menu of instance types. Each instance type has a different price, and a different computational capabilities. The instances with GPU\u2019s are those that start with either g2 or p2. We recommend the \u00abp2.xlarge\u00bb, which costs a little less than a dollar an hour. If you want something less expensive, the \u00abg2.2xlarge\u00bb is about $0.65/hr, though it isn\u2019t quite as fast or as powerful. After you have selected your instance type, select Configure Instance Details at the bottom of the page. Then select 6. Configure Security Group on the menu towards the top of the page. Here, we will set up your instance to make it easy to access from your computer. Each row describes rules for how one can access your instance. We will use Jupyter notebooks, which are served on port 8888. If you don\u2019t understand this yet, you will see how to make it work soon. For now, click the Add Rule button. In the new column select TCP for the protocol, 8888 for the Port, and 0.0.0.0/0 for the last column, which is the source. Finally click the Review and Launch button on the bottom of your screen. Then press Launch again. You will now get to the last bit of security\u2026 which is selecting a key pair. The key is a file you have on your computer, which must match a file stored on the server. This is how you prevent others from using the server you just set up. So, don\u2019t share keys with others or put the file anywhere public. Select Create a new key pair (or choose an existing pair) and type in a name for your key (\u00abCloudAWSTutorial.pem\u00bb). Then press the Download Key Pair button to get a copy of your key, which you will need to access your server. You will soon be brought to a screen that looks like what you this: The long string of characters in blue is the name of your instance. You can select that to see your EC2 dashboard, which shows this server running.","title":"Setting Up a Cloud Computing Server"},{"location":"AWS/#connecting-to-your-server","text":"We will now connect to our server using a protocol called ssh. From there, we will start a Jupyter notebook server, which we can use through the browser. On a Mac or Linux computer, you can use the ssh command. In Windows, many people use an application called PuTTY for ssh connections. You may need to change the access priveleges for the file for your key. In MacOS and Linux this can be done with the command: chmod 400 my_private_key.pem . Then, still at the command line, specify that you want this key to be available for authentication when logging into your server with: ssh-add my_private_key.pem . To log into our server, we will need the servers IP address. This can be found in the EC2 dashboard in your browser. Copy it\u2026 Log into this at the command line with the command: ssh ubuntu @18.219.71.98 . You will want to replace 18.219.71.98 with your server\u2019s IP address. Once logged into your server, use the ipython command to start an IPython shell. Once in IPython, try importing keras (or another package) to ensure everything works. You will see some message indicating the CUDA is being used\u2026 which mean keras is accessing the GPU. If you are comfortable with Linux, IPython, and a Linux text editor, you can work from here. Some people choose to use two windows. One to ssh to my server and use IPython, and another to ssh to the server and use the text editor.","title":"Connecting to Your Server"},{"location":"AWS/#setting-up-your-jupyter-notebook","text":"However, most people will find it easier to set up Jupyter notebooks and program through the browser. For this, we will want to set up a new password. The first step is to get the hash of whatever password you want to use. You do that with the following in IPython: 1 2 3 from IPython.lib import passwd passwd ( 'use a better password than this' ) Copy the hash that is output after you set your password. You will need that again in a moment. Type exit to get out of IPython (back in the virtual terminal). Then use a text editor to edit ~/.jupyter/jupyter_notebook_config.py. Use the command: nano ~/.jupyter/jupyter_notebook_config.py . At the top of that file, paste the following. You can copy all of this exactly, and just replace the password. Remember, don\u2019t use your actual password. Copy in the hash of your password that you created earlier: 1 2 3 4 5 6 7 8 c = get_config() c.IPKernelApp.pylab = 'inline' c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem' c.NotebookApp.ip = '*' c.NotebookApp.open_browser = False c.NotebookApp.password = u'sha1:a9985e250966:a1401151cca1771a9fbc88d276fde3482330aa93' c.NotebookApp.port = 8888 Exit and save the file.","title":"Setting up Your Jupyter Notebook"},{"location":"AWS/#connecting-to-jupyter-in-the-browser","text":"Go to the browser of your choice, and enter the address of your instance (available in the EC2 dashboard) followed by :8888. You may see a security message at this point. You shouldn\u2019t need to worry about this. In Chrome, you can click an \u201cadvanced\u201d button in your main browser window to bypass this security message. You will now be prompted for your password. Type in your password. This is not the hash of your password, but rather the raw password that you previously typed into IPython to get the hash value. Congrats, you are logged in!","title":"Connecting to Jupyter in the Browser"},{"location":"AWS/#using-the-notebook","text":"From within the browser, select on the menu to create a new Jupyter notebook. Getting these notebook capabilities required a lot of setup. Fortunately, it\u2019s mostly a 1-time effort. You can now stop your server in the EC2 dashboard whenever you aren\u2019t using it, and you will stop paying for it. Stopping an instance will keep all of your setup effort, but the instance will stop working until you restart it. In some ways, this is like a pause button. If you selected terminate it would lose all information about that instance, and you would have to redo the setup next time. Since you selected stop, you can now go back to the ec2 dashboard on another day and click start. Once you click start, it\u2019s pretty easy to get up and running again. Your instance will get a new IP address. You will need to ssh into that new IP address. You can then go back to your browser, and immediately start working again.","title":"Using The Notebook"},{"location":"Charts/","text":"Foreword Notes. Charts \u00b6 Chart & Diagrams, 2 pages . PDF only. Chart & Diagrams, 11x17 . PDF only. Which Chart v6 . PDF only. Charts . PDF. Colors \u00b6","title":"Charts"},{"location":"Charts/#charts","text":"Chart & Diagrams, 2 pages . PDF only. Chart & Diagrams, 11x17 . PDF only. Which Chart v6 . PDF only. Charts . PDF.","title":"Charts"},{"location":"Charts/#colors","text":"","title":"Colors"},{"location":"Cloud/","text":"Foreword A note on cloud computing. What Is Data Science, and What Does a Data Scientist Do? \u00b6 From this article : Actionable insights (via dashboards, reports, visualizations, \u2026); Anomaly detection (e.g., fraud detection); Automated processes and decision-making (e.g., credit card approval); Classification (e.g., spam or not spam); Forecasts (e.g., sales and revenue); Optimization (e.g., risk management); Pattern detection and grouping (e.g., classification without known classes); Prediction (predict a value based on inputs); Recognition (image, text, audio, video, facial, \u2026); Recommendations (e.g., Amazon and Netflix recommendations); Scoring and ranking (e.g., FICO score); Segmentation (e.g., demographic-based marketing). Scalable Data Science Beyond The Local Machine \u00b6 As advanced analytics becomes more prevalent and data science teams grow, collaborative needs also extend to include those outside of data science teams. What Is the Cloud Exactly? \u00b6 Computers connected together that share resources are called a network, with the Internet itself being the largest. Computers in a network are usually called nodes. Computers are often not located on-premise, meaning that applications and data are often hosted on computers located in a data center. Group of computers are connected to the same network and are working together to accomplish the same task or set of tasks, this is referred to as a cluster. A cluster can be thought of as a single computer, but can offer massive improvements in performance, availability, and scalability as compared to a single machine. The term distributed computing or distributed systems refers to software and systems that are written to leverage clusters to perform specific tasks, such as Spark. A cloud describes the situation where a single party owns, administers, and manages a group of networked computers and shared resources typically to host and provide software-based solutions. Given this definition, while the Internet is definitely considered a network, it is not a cloud since it\u2019s not owned by a single party. Data Science in the Cloud \u00b6 Once the development environment is setup, the typical data science workflow or process begins. The iterative workflow steps typically include: Acquiring data; Parsing, munging, wrangling, transforming, and sanitizing data; Analyzing and mining the data, such as Exploratory Data Analysis (EDA), summary statistics, etc.; Building, validating, and testing models, such as predictive, recommendations, etc.; Note: If not building models, then identifying patterns or trends, generating actionable insights, extracting useful information, creating reports, and so on. Here, this tutorial will consider either case \u201ccreating a deliverable\u201d; Tuning and optimizing models or deliverables. Sometimes however, it is not practical or desirable to perform all data science or big data-related tasks on ones local development environment. Here is a list of some of the main reasons why: Datasets are too large and will not fit into the development environment\u2019s system memory (RAM) for model training or other analytics; The development environment\u2019s processing power (CPU) is unable to perform tasks in a reasonable or sufficient amount of time, or at all for that matter; The deliverable needs to be deployed to a production environment, and possibly incorporated as a component into a larger application (for example, web application, SaaS platform, \u2026); It is simply preferred to use a faster, and more powerful machine (CPU, RAM, \u2026) and not impose the necessary load on the local development machine. Typically, people offload the computing work to either an on-premise machine, or cloud-based virtual machine. There are many cloud and service-based offerings available: AWS EC2; Google Cloud ML; AzureML; Rackspace; Algorithmia; DominoLab; Continuum; and many more. Read on application types, requirements, and components Advantages \u00b6 Working in the cloud has two main advantages: Scalability: you can tailor the power (RAM, CPU, GPU) of your instance to your immediate needs. Starting with a small and cheap instance and adding memory, storage, CPUs or GPUs as your project evolves. Reproducibility: a key condition of any data science project. Allowing other data scientists to review your models and reproduce your research is a necessary condition of a successful implementation. By setting up a working environment on a virtual instance, you make sure your work can easily be shared, reproduced, and vetted by other team members.","title":"Cloud Computing"},{"location":"Cloud/#what-is-data-science-and-what-does-a-data-scientist-do","text":"From this article : Actionable insights (via dashboards, reports, visualizations, \u2026); Anomaly detection (e.g., fraud detection); Automated processes and decision-making (e.g., credit card approval); Classification (e.g., spam or not spam); Forecasts (e.g., sales and revenue); Optimization (e.g., risk management); Pattern detection and grouping (e.g., classification without known classes); Prediction (predict a value based on inputs); Recognition (image, text, audio, video, facial, \u2026); Recommendations (e.g., Amazon and Netflix recommendations); Scoring and ranking (e.g., FICO score); Segmentation (e.g., demographic-based marketing).","title":"What Is Data Science, and What Does a Data Scientist Do?"},{"location":"Cloud/#scalable-data-science-beyond-the-local-machine","text":"As advanced analytics becomes more prevalent and data science teams grow, collaborative needs also extend to include those outside of data science teams.","title":"Scalable Data Science Beyond The Local Machine"},{"location":"Cloud/#what-is-the-cloud-exactly","text":"Computers connected together that share resources are called a network, with the Internet itself being the largest. Computers in a network are usually called nodes. Computers are often not located on-premise, meaning that applications and data are often hosted on computers located in a data center. Group of computers are connected to the same network and are working together to accomplish the same task or set of tasks, this is referred to as a cluster. A cluster can be thought of as a single computer, but can offer massive improvements in performance, availability, and scalability as compared to a single machine. The term distributed computing or distributed systems refers to software and systems that are written to leverage clusters to perform specific tasks, such as Spark. A cloud describes the situation where a single party owns, administers, and manages a group of networked computers and shared resources typically to host and provide software-based solutions. Given this definition, while the Internet is definitely considered a network, it is not a cloud since it\u2019s not owned by a single party.","title":"What Is the Cloud Exactly?"},{"location":"Cloud/#data-science-in-the-cloud","text":"Once the development environment is setup, the typical data science workflow or process begins. The iterative workflow steps typically include: Acquiring data; Parsing, munging, wrangling, transforming, and sanitizing data; Analyzing and mining the data, such as Exploratory Data Analysis (EDA), summary statistics, etc.; Building, validating, and testing models, such as predictive, recommendations, etc.; Note: If not building models, then identifying patterns or trends, generating actionable insights, extracting useful information, creating reports, and so on. Here, this tutorial will consider either case \u201ccreating a deliverable\u201d; Tuning and optimizing models or deliverables. Sometimes however, it is not practical or desirable to perform all data science or big data-related tasks on ones local development environment. Here is a list of some of the main reasons why: Datasets are too large and will not fit into the development environment\u2019s system memory (RAM) for model training or other analytics; The development environment\u2019s processing power (CPU) is unable to perform tasks in a reasonable or sufficient amount of time, or at all for that matter; The deliverable needs to be deployed to a production environment, and possibly incorporated as a component into a larger application (for example, web application, SaaS platform, \u2026); It is simply preferred to use a faster, and more powerful machine (CPU, RAM, \u2026) and not impose the necessary load on the local development machine. Typically, people offload the computing work to either an on-premise machine, or cloud-based virtual machine. There are many cloud and service-based offerings available: AWS EC2; Google Cloud ML; AzureML; Rackspace; Algorithmia; DominoLab; Continuum; and many more. Read on application types, requirements, and components","title":"Data Science in the Cloud"},{"location":"Cloud/#advantages","text":"Working in the cloud has two main advantages: Scalability: you can tailor the power (RAM, CPU, GPU) of your instance to your immediate needs. Starting with a small and cheap instance and adding memory, storage, CPUs or GPUs as your project evolves. Reproducibility: a key condition of any data science project. Allowing other data scientists to review your models and reproduce your research is a necessary condition of a successful implementation. By setting up a working environment on a virtual instance, you make sure your work can easily be shared, reproduced, and vetted by other team members.","title":"Advantages"},{"location":"Codecademy Learn SQL/","text":"Foreword Code Snippets. From Codecademy. Specifying Comments \u00b6 Line comment. This is indicated by two negative signs (eg. -- ). The remainder of the text on the line is the comment. Block comment. The start of the block comment is indicated by /* , the end of the comment by the same string. A block comment can cover text in part of a line, or can span multiple lines. Rem or @ . For Oracle, a line starting with either REM or @ is a comment line. Manipulation \u00b6 SQL, for Structured Query Language, is a programming language designed to manage data stored in relational databases. SQL operates through simple, declarative statements. This keeps data accurate and secure, and helps maintain the integrity of databases, regardless of size. The SQL language is widely used today across web frameworks and database applications. Knowing SQL gives you the freedom to explore your data, and the power to make better decisions. By learning SQL, you will also learn concepts that apply to nearly every data storage system. The statements covered in this course, use SQLite Relational Database Management System (RDBMS). You can learn more about RDBMS\u2019s here. You can also access a glossary of all the SQL commands taught in this course here. 1 2 3 4 5 6 --Show, create, use database SHOW databases ; CREATE DATABASE dbname ; SHOW databases ; USE dbname ; 1 2 3 --Show tables SHOW tables ; 1 2 3 4 5 6 7 --Create the table: CREATE TABLE table_name ( column_1 data_type , column_2 data_type , column_3 data_type ); 1 CREATE TABLE celebs ( id INTEGER , name TEXT , age INTEGER ); 1 2 3 --Add a row: INSERT INTO celebs ( id , name , age ) VALUES ( 1 , \"Justin Bieber\" , 21 ); 1 2 3 --View: SELECT * From celebs ; 1 2 3 4 5 --Insert more rows: INSERT INTO celebs ( id , name , age ) VALUES ( 2 , \"Beyonce Knowles\" , 33 ); INSERT INTO celebs ( id , name , age ) VALUES ( 3 , \"Jeremy Lin\" , 26 ); INSERT INTO celebs ( id , name , age ) VALUES ( 4 , \"Taylor Swift\" , 26 ); 1 2 3 --View: SELECT name FROM celebs ; 1 2 3 --Edit a row: UPDATE celebs SET age = 22 WHERE id = 1 ; 1 2 3 --View: SELECT * FROM celebs ; 1 2 3 --Add a new column: ALTER TABLE celebs ADD COLUMN twitter_handle TEXT ; 1 2 3 --View: SELECT * FROM celebs ; 1 2 3 --Update the table: UPDATE celebs SET twitter_handle = '@taylorswift13' WHERE id = 4 ; 1 2 3 --View: SELECT * FROM celebs ; 1 2 3 --Delete rows. NULL for missing or unknown data: DELETE FROM celebs WHERE twitter_handle IS NULL ; 1 2 3 --View: SELECT * FROM celebs ; Project Create Table \u00b6 In this project you will create your own friends table and add and delete data from it. The instructions provided are a general guideline, but feel free to add more columns, insert more data, or create more tables. Notes: plan a project by drawing an ULM schema. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 CREATE TABLE friends ( id INTEGER , name TEXT , birthday DATE ); SELECT * FROM friends ; DROP tables friends ; CREATE TABLE friends ( id INTEGER , name TEXT , birthday DATE ); SHOW tables ; INSERT INTO friends ( id , name , birthday ) VALUES ( 1 , \"Jane Doe\" , '1993-05-19' ); INSERT INTO friends ( id , name , birthday ) VALUES ( 2 , \"Jade Donot\" , '1995-06-12' ); INSERT INTO friends ( id , name , birthday ) VALUES ( 3 , \"Jack Doom\" , '1990-10-01' ); INSERT INTO friends ( id , name , birthday ) VALUES ( 4 , \"John Doe\" , '1988-12-09' ); UPDATE friends SET name = \"Jane Smith\" WHERE id = 1 ; ALTER TABLE friends ADD COLUMN email TEXT ; UPDATE friends SET email = \"jdoe@example.com\" WHERE id = 1 ; UPDATE friends SET email = \"jade@example.com\" WHERE id = 2 ; UPDATE friends SET email = \"doom@example.com\" WHERE id = 3 ; UPDATE friends SET email = \"johndoe@example.com\" WHERE id = 4 ; DELETE FROM friends WHERE id = 1 ; Queries \u00b6 One of the core purposes of the SQL language is to retrieve information stored in a database. This is commonly referred to as querying. Queries allow us to communicate with the database by asking questions and having the result set return data relevant to the question. In this lesson, you will be querying a database with one table named movies. Let\u2019s get started. 1 2 3 --Find rows: SELECT name , imdb_rating FROM movies ; 1 2 3 --Find rows. SELECT DISTINCT for unique values in the result set. It filters out all duplicate values: SELECT DISTINCT genre FROM movies ; 1 2 3 --Find rows with WHERE: SELECT * FROM movies WHERE imdb_rating > 8 ; Clauses: = , equals. != , not equals. > , greater than. < , less than. >= , greater than or equal to. <= , less than or equal to. 1 2 3 --Find rows with LIKE. Se_en represents a pattern with a wildcard character. The _ means you can substitute any individual character here without breaking the pattern. The names Seven and Se7en both match this pattern: SELECT * FROM movies WHERE name LIKE 'Se_en' ; 1 2 3 4 --Find rows with LIKE. % is another wildcard character that can be used with LIKE. % is a wildcard character that matches zero or more missing letters in the pattern. A% matches all movies with names that begin with \"A\". %a matches all movies that end with \"a\": SELECT * FROM movies WHERE name LIKE 'a%' ; SELECT * FROM movies WHERE name LIKE '%man%' ; 1 2 3 4 --Find rows with BETWEEN. The values can be numbers, text or dates. Names that begin with letters \"A\" up to but not including \"J\". Years between 1990 up to and including 2000: SELECT * FROM movies WHERE name BETWEEN 'A' AND 'J' ; SELECT * FROM movies WHERE year BETWEEN 1990 AND 2000 ; 1 2 3 --Find rows with BETWEEN, AND: SELECT * FROM movies WHERE year BETWEEN 1990 AND 2000 AND genre = 'comedy' ; 1 2 3 --Find rows with BETWEEN, OR: SELECT * FROM movies WHERE genre = 'comedy' OR year < 1980 ; 1 2 3 --Find rows with BETWEEN, ORDERED BY, DESC, ASC: SELECT * FROM movies ORDER BY imdb_rating DESC ; 1 2 3 --Find rows, BETWEEN, ORDERED BY, ASC. LIMIT is a clause that lets you specify the maximum number of rows the result set will have: SELECT * FROM movies ORDER BY imdb_rating ASC LIMIT 3 ; Project Writing Queries \u00b6 In this project you will write queries to retrieve information from the movies table. The instructions provided are a general guideline, but feel free to experiment writing your own queries. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 --Return all of the unique years in the movies table. SELECT DISTINCT year FROM movies ; --Return all of the unique years in the movies table sorted from oldest to newest. SELECT DISTINCT year FROM movies ORDER BY year ASC ; --Return all movies that are dramas. SELECT * FROM movies WHERE genre = \"drama\" ; --Return all of the movies with names that contain \"bride\". SELECT * FROM movies WHERE name LIKE '%Br%' ; --Return all of the movies that were made between 2000 and 2015. SELECT * FROM movies WHERE year >= 2000 AND year <= 2015 ; --Return all of the movies that were made in 1995 or have an IMDb rating of 9. SELECT * FROM movies WHERE year = 1995 OR imdb_rating = 9 ; --Return the name and IMDb rating of every movie made after 2009 in alphabetical order. SELECT name , imdb_rating FROM movies WHERE year > 2009 ORDER BY name ASC ; --Return 3 movies with an IMDb rating of 7. SELECT * FROM movies WHERE imdb_rating = 7 LIMIT 3 ; --Return 10 movies with an IMDb rating greater than 6, are comedies, were made after 1995, and sorted by IMDb rating from highest to lowest. (Hint: you can use AND more than once in a statement). SELECT * FROM movies WHERE imdb_rating > 6 AND genre = 'comedy' AND year > 1995 ORDER BY imdb_rating ASC LIMIT 10 ; --Return all movies named 'Cast Away'. SELECT * FROM movies WHERE name = 'Cast Away' ; --Return all movies with an IMDb rating not equal to 7. SELECT * FROM movies WHERE imdb_rating != 7 ; --Return all movies with a horror genre and an IMDb rating less than 6. SELECT * FROM movies WHERE genre = 'horror' AND imdb_rating < 6 ; --Return 10 movies with an IMDb rating greater than 8 sorted by their genre. SELECT * FROM movies WHERE imdb_rating > 8 ORDER BY genre ASC LIMIT 10 ; --Return all movies that include 'King' in the name. SELECT * FROM movies WHERE name LIKE '%King%' ; --Return all movies with names that end with the word 'Out' SELECT * FROM movies WHERE name LIKE '%Out' ; --Return all movies with names that begin with the word \"The\" sorted by IMDb rating from highest to lowest SELECT * FROM movies WHERE name LIKE 'The%' ORDER BY imdb_rating DESC ; --Return all of the movies. SELECT * FROM movies ; --Return the name and id of each movie with an id greater than 125. SELECT name , id FROM movies WHERE id > 125 ; --Return all movies with names that begin with 'X-Men' SELECT * FROM movies WHERE name LIKE 'X-Men%' ; --Return the first 10 movies sorted in reverse alphabetical order. SELECT * FROM movies DESC LIMIT 10 ; --Return the id, name, and genre of all movies that are romances. SELECT id , name , genre FROM movies WHERE genre = 'romance' ; --Return all of the Twilight movies in order from the year they were released from oldest to newest. SELECT * FROM movies WHERE name LIKE '%Twilight%' ORDER BY year ASC ; --Return all of the movies that were released in 2012 that are comedies. SELECT * FROM movies WHERE year = 2012 AND genre = 'comedy' ; Aggregate Functions \u00b6 Aggregate functions compute a single result from a set of input values. For instance, when we need the sum or average of a particular column, we can use aggregate functions to quickly compute it for us. We will be learning about different aggregate functions in this lesson. For this lesson we have given you a table named fake_apps which is made up of data for fake mobile applications. 1 2 3 --View: SELECT * FROM fake_apps ; 1 2 3 --Calculate the number of rows in a table. where the column is not NULL. Here, we want to count every row so we pass * as an argument: SELECT COUNT ( * ) FROM fake_apps ; 1 2 3 --Count * or columns): SELECT COUNT ( * ) FROM fake_apps WHERE price = 0 ; 1 2 3 --Aggregate. GROUP BY is a clause in SQL that is only used with aggregate functions. It is used in collaboration with the SELECT statement to arrange identical data into groups. Here, our aggregate function is COUNT() and we are passing price as an argument to GROUP BY. SQL will count the total number of apps for each price in the table: SELECT price , COUNT ( * ) FROM fake_apps GROUP BY price ; 1 2 3 --It calculates the total number of apartments in each neighborhood in the cities table and groups them by neighborhood. SELECT neighborhood , SUM ( apartments ) FROM cities GROUP BY neighborhood ; 1 2 3 --Sum up, aggregate by category: SELECT category , SUM ( downloads ) FROM fake_apps GROUP BY category ; 1 2 3 4 5 6 --Find the maximum, minimum, global, categorical: SELECT MAX ( downloads ) FROM fake_apps ; SELECT name , category , MAX ( downloads ) FROM fake_apps GROUP BY category ; SELECT MIN ( downloads ) FROM fake_apps ; SELECT name , category , MIN ( downloads ) FROM fake_apps GROUP BY category ; 1 2 3 4 --It returns the title, genre and checkout count for the book with the most checkouts in the library_books table. SELECT title , genre , MAX ( checkouts ) FROM library_books GROUP BY genre ; 1 2 3 4 --Find the average, or category average: SELECT AVG ( downloads ) FROM fake_apps ; SELECT price , AVG ( downloads ) FROM fake_apps GROUP BY price ; 1 2 3 --Round up to x decimal(s): SELECT price , ROUND ( AVG ( downloads ), 2 ) FROM fake_apps GROUP BY price ; Project Fake Apps \u00b6 In this project you will write queries with aggregate functions to retrieve information from the fake_apps table. The instructions provided are a general guideline, but feel free to experiment writing your own queries, creating your own tables, or adding your own apps to the existing table! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 --Return the total number of apps in the table fake_apps. SELECT COUNT ( * ) FROM fake_apps ; --Return the name, category, and price of the app that has been downloaded the least amount of times. SELECT name , category , price , MIN ( downloads ) FROM fake_apps ; --Return the total number of apps for each category. SELECT COUNT ( * ) FROM fake_apps GROUP BY category ; --Return the name and category of the app that has been downloaded the most amount of times. SELECT name , category , MAX ( downloads ) FROM fake_apps ; --Return the name and category of the app that has been downloaded the least amount of times. SELECT name , category , MIN ( downloads ) FROM fake_apps ; --Return the average price for an app in each category. SELECT AVG ( price ) FROM fake_apps GROUP BY category ; --Return the average price for an app in each category. Round the averages to two decimal places. SELECT ROUND ( AVG ( price ), 2 ) FROM fake_apps GROUP BY category ; --Return the maximum price for an app. SELECT MAX ( price ) FROM fake_apps ; --Return the minimum number of downloads for an app. SELECT MIN ( downloads ) FROM fake_apps ; --Return the total number of downloads for apps that belong to the Games category. SELECT MIN ( downloads ) FROM fake_apps WHERE category = 'Games' ; --Return the total number of apps that are free. SELECT COUNT ( * ) FROM fake_apps WHERE price = 0 ; --Return the total number of apps that cost 14.99. SELECT COUNT ( * ) FROM fake_apps WHERE price = 14 . 99 ; --Return the sum of the total number of downloads for apps that belong to the Music category. SELECT SUM ( downloads ) FROM fake_apps WHERE category = 'Music' ; --Return the sum of the total number of downloads for apps that belong to the Business category. SELECT SUM ( downloads ) FROM fake_apps WHERE category = 'Business' ; --Return the name of each category with the total number of apps that belong to it. SELECT name , COUNT ( * ) FROM fake_apps GROUP BY category ; --Return the price and average number of downloads grouped by price. SELECT price , AVG ( downloads ) FROM fake_apps GROUP BY price ; --Return the price and average number of downloads grouped by price. Round the averages to the nearest integer. SELECT ROUND ( price , 0 ), AVG ( downloads ) FROM fake_apps GROUP BY price ; --Return the name and category and price of the most expensive app for each category. SELECT name , category , price , MAX ( price ) FROM fake_apps GROUP BY category ; --Return the total number of apps whose name begin with the letter 'A'. SELECT COUNT ( * ) FROM fake_apps WHERE name LIKE 'A%' ; --Return the total number of downloads for apps belonging to the Sports or Health & Fitness category. SELECT SUM ( downloads ) FROM fake_apps WHERE category = 'Sports' OR category = 'Health & Fitness' ; Multiple Tables \u00b6 Most of the time, data is distributed across multiple tables in the database. Imagine a database with two tables, artists and albums. An artist can produce many different albums, and an album is produced by an artist. The data in these tables are related to each other. Through SQL, we can write queries that combine data from multiple tables that are related to one another. This is one of the most powerful features of relational databases. 1 2 3 4 5 6 7 8 --We have created a table named albums for you. Create a second table named artists. Using the CREATE TABLE statement we added a PRIMARY KEY to the id column. A primary key serves as a unique identifier for each row or record in a given table. The primary key is literally an id value for a record. We're going to use this value to connect artists to the albums they have produced. By specifying that the id column is the PRIMARY KEY, SQL makes sure that none of the values in this column are NULL and each value in this column is unique: CREATE TABLE artists ( id INTEGER PRIMARY KEY , name TEXT ); --View both tables: SELECT * FROM albums ; SELECT * FROM artists ; 1 2 3 --Query: SELECT * FROM artists WHERE id = 3 ; 1 2 3 --A foreign key is a column that contains the primary key of another table in the database. We use foreign keys and primary keys to connect rows in two different tables. One table's foreign key holds the value of another table's primary key. Unlike primary keys, foreign keys do not need to be unique and can be NULL. Here, artist_id is a foreign key in the albums table. We can see that Michael Jackson has an id of 3 in the artists table. All of the albums by Michael Jackson also have a 3 in the artist_id column in the albums table. This is how SQL is linking data between the two tables. The relationship between the artists table and the albums table is the id value of the artists. SELECT * FROM albums WHERE artist_id = 3 ; 1 2 3 --One way to query multiple tables is to write a SELECT statement with multiple table names separated by a comma. This is also known as a CROSS JOIN. Here, albums and artists are the different tables we are querying. When querying more than one table, column names need to be specified by table_name.column_name. Here, the result set includes the name and year columns from the albums table and the name column from the artists table. Unfortunately the result of this cross join is not very useful. It combines every row of the artists table with every row of the albums table. It would be more useful to only combine the rows where the album was created by the artist. SELECT albums . name , albums . year , artists . name FROM albums , artists ; 1 2 3 --In SQL, joins are used to combine rows from two or more tables. The most common type of join in SQL is an INNER JOIN. An inner join will combine rows from different tables if the join condition is true. Let's look at the syntax to see how it works. SELECT * specifies the columns our result set will have. Here, we want to include every column in both tables. FROM albums specifies the first table we are querying. JOIN artists ON specifies the type of join we are going to use as well as the name of the second table. Here, we want to do an inner join and the second table we want to query is artists. albums.artist_id = artists.id is the join condition that describes how the two tables are related to each other. Here, SQL uses the foreign key column artist_id in the albums table to match it with exactly one row in the artists table with the same value in the id column. We know it will only match one row in the artists table because id is the PRIMARY KEY of artists. SELECT * FROM albums JOIN artists ON albums . artist_id = artists . id ; 1 2 3 --OUTER JOINS (LEFT or RIGHT) also combine rows from two or more tables, but unlike inner joins, they do not require the join condition to be met. Instead, every row in the left table is returned in the result set, and if the join condition is not met, then NULL values are used to fill in the columns from the right table. The left table is simply the first table that appears in the statement. Here, the left table is albums. Likewise, the right table is the second table that appears. Here, artists is the right table. SELECT * FROM albums LEFT JOIN artists ON albums . artist_id = artists . id ; 1 2 3 --AS is a keyword in SQL that allows you to rename a column or table using an alias. The new name can be anything you want as long as you put it inside of single quotes. Here we want to rename the albums.name column as 'Album', and the artists.name column as 'Artist'. It is important to note that the columns have not been renamed in either table. The aliases only appear in the result set. SELECT albums . name , albums . year , artists . name FROM albums JOIN artists ON albums . artist_id = artists . id WHERE albums . year > 1980 ; 1 SELECT albums . name AS 'Album' , albums . year , artists . name AS 'Artist' FROM albums JOIN artists ON albums . artist_id = artists . id WHERE albums . year > 1980 ; Recap \u00b6 Primary Key is a column that serves a unique identifier for row in the table. Values in this column must be unique and cannot be NULL . Foreign Key is a column that contains the primary key to another table in the database. It is used to identify a particular row in the referenced table. Joins are used in SQL to combine data from multiple tables. INNER JOIN will combine rows from different tables if the join condition is true. LEFT OUTER JOIN will return every row in the left table, and if the join condition is not met, NULL values are used to fill in the columns from the right table. AS is a keyword in SQL that allows you to rename a column or table in the result set using an alias. Project Querying Tables \u00b6 In this project you will practice querying multiple tables using joins. The instructions provided are a general guideline, but feel free to experiment creating your own tables, adding data, and writing more queries. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 --Create a table named tracks with an id, title, and album_id column. The id column should be the PRIMARY KEY. CREATE TABLE tracks ( id INTEGER PRIMARY KEY , title TEXT , albums_id INTEGER ); --\"Smooth Criminal\" is a track from Michael Jackson's \"Bad\" album. Add this track to the database. INSERT INTO tracks ( id , title , albums_id ) VALUES ( 1 , \"Smooth Criminal\" , 8 ); --Add more tracks to the database. INSERT INTO tracks ( id , title , albums_id ) VALUES ( 2 , \"aaa\" , 1 ); INSERT INTO tracks ( id , title , albums_id ) VALUES ( 3 , \"bbb\" , 1 ); INSERT INTO tracks ( id , title , albums_id ) VALUES ( 4 , \"ccc\" , 8 ); INSERT INTO tracks ( id , title , albums_id ) VALUES ( 5 , \"ddd\" , 8 ); INSERT INTO tracks ( id , title , albums_id ) VALUES ( 6 , \"eee\" , 8 ); --Combine the albums and tracks tables using an INNER JOIN. Order the query by album_id. SELECT * FROM albums JOIN tracks ON albums . artist_id = tracks . id ; --Combine the albums and artists table using a LEFT OUTER JOIN. Let albums be the left table. SELECT * FROM albums LEFT JOIN artists ON albums . artist_id = artists . id ; --Combine the albums and artists table using a LEFT OUTER JOIN. Let artists be the left table. SELECT * FROM artists LEFT JOIN albums ON albums . artist_id = artists . id ; --Use any join you like to combine the albums and tracks table. Rename the album_id column to Albums. SELECT * FROM albums LEFT JOIN tracks ON albums . artist_id = tracks . id ;","title":"Codecademy, Learn SQL"},{"location":"Codecademy Learn SQL/#specifying-comments","text":"Line comment. This is indicated by two negative signs (eg. -- ). The remainder of the text on the line is the comment. Block comment. The start of the block comment is indicated by /* , the end of the comment by the same string. A block comment can cover text in part of a line, or can span multiple lines. Rem or @ . For Oracle, a line starting with either REM or @ is a comment line.","title":"Specifying Comments"},{"location":"Codecademy Learn SQL/#manipulation","text":"SQL, for Structured Query Language, is a programming language designed to manage data stored in relational databases. SQL operates through simple, declarative statements. This keeps data accurate and secure, and helps maintain the integrity of databases, regardless of size. The SQL language is widely used today across web frameworks and database applications. Knowing SQL gives you the freedom to explore your data, and the power to make better decisions. By learning SQL, you will also learn concepts that apply to nearly every data storage system. The statements covered in this course, use SQLite Relational Database Management System (RDBMS). You can learn more about RDBMS\u2019s here. You can also access a glossary of all the SQL commands taught in this course here. 1 2 3 4 5 6 --Show, create, use database SHOW databases ; CREATE DATABASE dbname ; SHOW databases ; USE dbname ; 1 2 3 --Show tables SHOW tables ; 1 2 3 4 5 6 7 --Create the table: CREATE TABLE table_name ( column_1 data_type , column_2 data_type , column_3 data_type ); 1 CREATE TABLE celebs ( id INTEGER , name TEXT , age INTEGER ); 1 2 3 --Add a row: INSERT INTO celebs ( id , name , age ) VALUES ( 1 , \"Justin Bieber\" , 21 ); 1 2 3 --View: SELECT * From celebs ; 1 2 3 4 5 --Insert more rows: INSERT INTO celebs ( id , name , age ) VALUES ( 2 , \"Beyonce Knowles\" , 33 ); INSERT INTO celebs ( id , name , age ) VALUES ( 3 , \"Jeremy Lin\" , 26 ); INSERT INTO celebs ( id , name , age ) VALUES ( 4 , \"Taylor Swift\" , 26 ); 1 2 3 --View: SELECT name FROM celebs ; 1 2 3 --Edit a row: UPDATE celebs SET age = 22 WHERE id = 1 ; 1 2 3 --View: SELECT * FROM celebs ; 1 2 3 --Add a new column: ALTER TABLE celebs ADD COLUMN twitter_handle TEXT ; 1 2 3 --View: SELECT * FROM celebs ; 1 2 3 --Update the table: UPDATE celebs SET twitter_handle = '@taylorswift13' WHERE id = 4 ; 1 2 3 --View: SELECT * FROM celebs ; 1 2 3 --Delete rows. NULL for missing or unknown data: DELETE FROM celebs WHERE twitter_handle IS NULL ; 1 2 3 --View: SELECT * FROM celebs ;","title":"Manipulation"},{"location":"Codecademy Learn SQL/#project-create-table","text":"In this project you will create your own friends table and add and delete data from it. The instructions provided are a general guideline, but feel free to add more columns, insert more data, or create more tables. Notes: plan a project by drawing an ULM schema. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 CREATE TABLE friends ( id INTEGER , name TEXT , birthday DATE ); SELECT * FROM friends ; DROP tables friends ; CREATE TABLE friends ( id INTEGER , name TEXT , birthday DATE ); SHOW tables ; INSERT INTO friends ( id , name , birthday ) VALUES ( 1 , \"Jane Doe\" , '1993-05-19' ); INSERT INTO friends ( id , name , birthday ) VALUES ( 2 , \"Jade Donot\" , '1995-06-12' ); INSERT INTO friends ( id , name , birthday ) VALUES ( 3 , \"Jack Doom\" , '1990-10-01' ); INSERT INTO friends ( id , name , birthday ) VALUES ( 4 , \"John Doe\" , '1988-12-09' ); UPDATE friends SET name = \"Jane Smith\" WHERE id = 1 ; ALTER TABLE friends ADD COLUMN email TEXT ; UPDATE friends SET email = \"jdoe@example.com\" WHERE id = 1 ; UPDATE friends SET email = \"jade@example.com\" WHERE id = 2 ; UPDATE friends SET email = \"doom@example.com\" WHERE id = 3 ; UPDATE friends SET email = \"johndoe@example.com\" WHERE id = 4 ; DELETE FROM friends WHERE id = 1 ;","title":"Project Create Table"},{"location":"Codecademy Learn SQL/#queries","text":"One of the core purposes of the SQL language is to retrieve information stored in a database. This is commonly referred to as querying. Queries allow us to communicate with the database by asking questions and having the result set return data relevant to the question. In this lesson, you will be querying a database with one table named movies. Let\u2019s get started. 1 2 3 --Find rows: SELECT name , imdb_rating FROM movies ; 1 2 3 --Find rows. SELECT DISTINCT for unique values in the result set. It filters out all duplicate values: SELECT DISTINCT genre FROM movies ; 1 2 3 --Find rows with WHERE: SELECT * FROM movies WHERE imdb_rating > 8 ; Clauses: = , equals. != , not equals. > , greater than. < , less than. >= , greater than or equal to. <= , less than or equal to. 1 2 3 --Find rows with LIKE. Se_en represents a pattern with a wildcard character. The _ means you can substitute any individual character here without breaking the pattern. The names Seven and Se7en both match this pattern: SELECT * FROM movies WHERE name LIKE 'Se_en' ; 1 2 3 4 --Find rows with LIKE. % is another wildcard character that can be used with LIKE. % is a wildcard character that matches zero or more missing letters in the pattern. A% matches all movies with names that begin with \"A\". %a matches all movies that end with \"a\": SELECT * FROM movies WHERE name LIKE 'a%' ; SELECT * FROM movies WHERE name LIKE '%man%' ; 1 2 3 4 --Find rows with BETWEEN. The values can be numbers, text or dates. Names that begin with letters \"A\" up to but not including \"J\". Years between 1990 up to and including 2000: SELECT * FROM movies WHERE name BETWEEN 'A' AND 'J' ; SELECT * FROM movies WHERE year BETWEEN 1990 AND 2000 ; 1 2 3 --Find rows with BETWEEN, AND: SELECT * FROM movies WHERE year BETWEEN 1990 AND 2000 AND genre = 'comedy' ; 1 2 3 --Find rows with BETWEEN, OR: SELECT * FROM movies WHERE genre = 'comedy' OR year < 1980 ; 1 2 3 --Find rows with BETWEEN, ORDERED BY, DESC, ASC: SELECT * FROM movies ORDER BY imdb_rating DESC ; 1 2 3 --Find rows, BETWEEN, ORDERED BY, ASC. LIMIT is a clause that lets you specify the maximum number of rows the result set will have: SELECT * FROM movies ORDER BY imdb_rating ASC LIMIT 3 ;","title":"Queries"},{"location":"Codecademy Learn SQL/#project-writing-queries","text":"In this project you will write queries to retrieve information from the movies table. The instructions provided are a general guideline, but feel free to experiment writing your own queries. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 --Return all of the unique years in the movies table. SELECT DISTINCT year FROM movies ; --Return all of the unique years in the movies table sorted from oldest to newest. SELECT DISTINCT year FROM movies ORDER BY year ASC ; --Return all movies that are dramas. SELECT * FROM movies WHERE genre = \"drama\" ; --Return all of the movies with names that contain \"bride\". SELECT * FROM movies WHERE name LIKE '%Br%' ; --Return all of the movies that were made between 2000 and 2015. SELECT * FROM movies WHERE year >= 2000 AND year <= 2015 ; --Return all of the movies that were made in 1995 or have an IMDb rating of 9. SELECT * FROM movies WHERE year = 1995 OR imdb_rating = 9 ; --Return the name and IMDb rating of every movie made after 2009 in alphabetical order. SELECT name , imdb_rating FROM movies WHERE year > 2009 ORDER BY name ASC ; --Return 3 movies with an IMDb rating of 7. SELECT * FROM movies WHERE imdb_rating = 7 LIMIT 3 ; --Return 10 movies with an IMDb rating greater than 6, are comedies, were made after 1995, and sorted by IMDb rating from highest to lowest. (Hint: you can use AND more than once in a statement). SELECT * FROM movies WHERE imdb_rating > 6 AND genre = 'comedy' AND year > 1995 ORDER BY imdb_rating ASC LIMIT 10 ; --Return all movies named 'Cast Away'. SELECT * FROM movies WHERE name = 'Cast Away' ; --Return all movies with an IMDb rating not equal to 7. SELECT * FROM movies WHERE imdb_rating != 7 ; --Return all movies with a horror genre and an IMDb rating less than 6. SELECT * FROM movies WHERE genre = 'horror' AND imdb_rating < 6 ; --Return 10 movies with an IMDb rating greater than 8 sorted by their genre. SELECT * FROM movies WHERE imdb_rating > 8 ORDER BY genre ASC LIMIT 10 ; --Return all movies that include 'King' in the name. SELECT * FROM movies WHERE name LIKE '%King%' ; --Return all movies with names that end with the word 'Out' SELECT * FROM movies WHERE name LIKE '%Out' ; --Return all movies with names that begin with the word \"The\" sorted by IMDb rating from highest to lowest SELECT * FROM movies WHERE name LIKE 'The%' ORDER BY imdb_rating DESC ; --Return all of the movies. SELECT * FROM movies ; --Return the name and id of each movie with an id greater than 125. SELECT name , id FROM movies WHERE id > 125 ; --Return all movies with names that begin with 'X-Men' SELECT * FROM movies WHERE name LIKE 'X-Men%' ; --Return the first 10 movies sorted in reverse alphabetical order. SELECT * FROM movies DESC LIMIT 10 ; --Return the id, name, and genre of all movies that are romances. SELECT id , name , genre FROM movies WHERE genre = 'romance' ; --Return all of the Twilight movies in order from the year they were released from oldest to newest. SELECT * FROM movies WHERE name LIKE '%Twilight%' ORDER BY year ASC ; --Return all of the movies that were released in 2012 that are comedies. SELECT * FROM movies WHERE year = 2012 AND genre = 'comedy' ;","title":"Project Writing Queries"},{"location":"Codecademy Learn SQL/#aggregate-functions","text":"Aggregate functions compute a single result from a set of input values. For instance, when we need the sum or average of a particular column, we can use aggregate functions to quickly compute it for us. We will be learning about different aggregate functions in this lesson. For this lesson we have given you a table named fake_apps which is made up of data for fake mobile applications. 1 2 3 --View: SELECT * FROM fake_apps ; 1 2 3 --Calculate the number of rows in a table. where the column is not NULL. Here, we want to count every row so we pass * as an argument: SELECT COUNT ( * ) FROM fake_apps ; 1 2 3 --Count * or columns): SELECT COUNT ( * ) FROM fake_apps WHERE price = 0 ; 1 2 3 --Aggregate. GROUP BY is a clause in SQL that is only used with aggregate functions. It is used in collaboration with the SELECT statement to arrange identical data into groups. Here, our aggregate function is COUNT() and we are passing price as an argument to GROUP BY. SQL will count the total number of apps for each price in the table: SELECT price , COUNT ( * ) FROM fake_apps GROUP BY price ; 1 2 3 --It calculates the total number of apartments in each neighborhood in the cities table and groups them by neighborhood. SELECT neighborhood , SUM ( apartments ) FROM cities GROUP BY neighborhood ; 1 2 3 --Sum up, aggregate by category: SELECT category , SUM ( downloads ) FROM fake_apps GROUP BY category ; 1 2 3 4 5 6 --Find the maximum, minimum, global, categorical: SELECT MAX ( downloads ) FROM fake_apps ; SELECT name , category , MAX ( downloads ) FROM fake_apps GROUP BY category ; SELECT MIN ( downloads ) FROM fake_apps ; SELECT name , category , MIN ( downloads ) FROM fake_apps GROUP BY category ; 1 2 3 4 --It returns the title, genre and checkout count for the book with the most checkouts in the library_books table. SELECT title , genre , MAX ( checkouts ) FROM library_books GROUP BY genre ; 1 2 3 4 --Find the average, or category average: SELECT AVG ( downloads ) FROM fake_apps ; SELECT price , AVG ( downloads ) FROM fake_apps GROUP BY price ; 1 2 3 --Round up to x decimal(s): SELECT price , ROUND ( AVG ( downloads ), 2 ) FROM fake_apps GROUP BY price ;","title":"Aggregate Functions"},{"location":"Codecademy Learn SQL/#project-fake-apps","text":"In this project you will write queries with aggregate functions to retrieve information from the fake_apps table. The instructions provided are a general guideline, but feel free to experiment writing your own queries, creating your own tables, or adding your own apps to the existing table! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 --Return the total number of apps in the table fake_apps. SELECT COUNT ( * ) FROM fake_apps ; --Return the name, category, and price of the app that has been downloaded the least amount of times. SELECT name , category , price , MIN ( downloads ) FROM fake_apps ; --Return the total number of apps for each category. SELECT COUNT ( * ) FROM fake_apps GROUP BY category ; --Return the name and category of the app that has been downloaded the most amount of times. SELECT name , category , MAX ( downloads ) FROM fake_apps ; --Return the name and category of the app that has been downloaded the least amount of times. SELECT name , category , MIN ( downloads ) FROM fake_apps ; --Return the average price for an app in each category. SELECT AVG ( price ) FROM fake_apps GROUP BY category ; --Return the average price for an app in each category. Round the averages to two decimal places. SELECT ROUND ( AVG ( price ), 2 ) FROM fake_apps GROUP BY category ; --Return the maximum price for an app. SELECT MAX ( price ) FROM fake_apps ; --Return the minimum number of downloads for an app. SELECT MIN ( downloads ) FROM fake_apps ; --Return the total number of downloads for apps that belong to the Games category. SELECT MIN ( downloads ) FROM fake_apps WHERE category = 'Games' ; --Return the total number of apps that are free. SELECT COUNT ( * ) FROM fake_apps WHERE price = 0 ; --Return the total number of apps that cost 14.99. SELECT COUNT ( * ) FROM fake_apps WHERE price = 14 . 99 ; --Return the sum of the total number of downloads for apps that belong to the Music category. SELECT SUM ( downloads ) FROM fake_apps WHERE category = 'Music' ; --Return the sum of the total number of downloads for apps that belong to the Business category. SELECT SUM ( downloads ) FROM fake_apps WHERE category = 'Business' ; --Return the name of each category with the total number of apps that belong to it. SELECT name , COUNT ( * ) FROM fake_apps GROUP BY category ; --Return the price and average number of downloads grouped by price. SELECT price , AVG ( downloads ) FROM fake_apps GROUP BY price ; --Return the price and average number of downloads grouped by price. Round the averages to the nearest integer. SELECT ROUND ( price , 0 ), AVG ( downloads ) FROM fake_apps GROUP BY price ; --Return the name and category and price of the most expensive app for each category. SELECT name , category , price , MAX ( price ) FROM fake_apps GROUP BY category ; --Return the total number of apps whose name begin with the letter 'A'. SELECT COUNT ( * ) FROM fake_apps WHERE name LIKE 'A%' ; --Return the total number of downloads for apps belonging to the Sports or Health & Fitness category. SELECT SUM ( downloads ) FROM fake_apps WHERE category = 'Sports' OR category = 'Health & Fitness' ;","title":"Project Fake Apps"},{"location":"Codecademy Learn SQL/#multiple-tables","text":"Most of the time, data is distributed across multiple tables in the database. Imagine a database with two tables, artists and albums. An artist can produce many different albums, and an album is produced by an artist. The data in these tables are related to each other. Through SQL, we can write queries that combine data from multiple tables that are related to one another. This is one of the most powerful features of relational databases. 1 2 3 4 5 6 7 8 --We have created a table named albums for you. Create a second table named artists. Using the CREATE TABLE statement we added a PRIMARY KEY to the id column. A primary key serves as a unique identifier for each row or record in a given table. The primary key is literally an id value for a record. We're going to use this value to connect artists to the albums they have produced. By specifying that the id column is the PRIMARY KEY, SQL makes sure that none of the values in this column are NULL and each value in this column is unique: CREATE TABLE artists ( id INTEGER PRIMARY KEY , name TEXT ); --View both tables: SELECT * FROM albums ; SELECT * FROM artists ; 1 2 3 --Query: SELECT * FROM artists WHERE id = 3 ; 1 2 3 --A foreign key is a column that contains the primary key of another table in the database. We use foreign keys and primary keys to connect rows in two different tables. One table's foreign key holds the value of another table's primary key. Unlike primary keys, foreign keys do not need to be unique and can be NULL. Here, artist_id is a foreign key in the albums table. We can see that Michael Jackson has an id of 3 in the artists table. All of the albums by Michael Jackson also have a 3 in the artist_id column in the albums table. This is how SQL is linking data between the two tables. The relationship between the artists table and the albums table is the id value of the artists. SELECT * FROM albums WHERE artist_id = 3 ; 1 2 3 --One way to query multiple tables is to write a SELECT statement with multiple table names separated by a comma. This is also known as a CROSS JOIN. Here, albums and artists are the different tables we are querying. When querying more than one table, column names need to be specified by table_name.column_name. Here, the result set includes the name and year columns from the albums table and the name column from the artists table. Unfortunately the result of this cross join is not very useful. It combines every row of the artists table with every row of the albums table. It would be more useful to only combine the rows where the album was created by the artist. SELECT albums . name , albums . year , artists . name FROM albums , artists ; 1 2 3 --In SQL, joins are used to combine rows from two or more tables. The most common type of join in SQL is an INNER JOIN. An inner join will combine rows from different tables if the join condition is true. Let's look at the syntax to see how it works. SELECT * specifies the columns our result set will have. Here, we want to include every column in both tables. FROM albums specifies the first table we are querying. JOIN artists ON specifies the type of join we are going to use as well as the name of the second table. Here, we want to do an inner join and the second table we want to query is artists. albums.artist_id = artists.id is the join condition that describes how the two tables are related to each other. Here, SQL uses the foreign key column artist_id in the albums table to match it with exactly one row in the artists table with the same value in the id column. We know it will only match one row in the artists table because id is the PRIMARY KEY of artists. SELECT * FROM albums JOIN artists ON albums . artist_id = artists . id ; 1 2 3 --OUTER JOINS (LEFT or RIGHT) also combine rows from two or more tables, but unlike inner joins, they do not require the join condition to be met. Instead, every row in the left table is returned in the result set, and if the join condition is not met, then NULL values are used to fill in the columns from the right table. The left table is simply the first table that appears in the statement. Here, the left table is albums. Likewise, the right table is the second table that appears. Here, artists is the right table. SELECT * FROM albums LEFT JOIN artists ON albums . artist_id = artists . id ; 1 2 3 --AS is a keyword in SQL that allows you to rename a column or table using an alias. The new name can be anything you want as long as you put it inside of single quotes. Here we want to rename the albums.name column as 'Album', and the artists.name column as 'Artist'. It is important to note that the columns have not been renamed in either table. The aliases only appear in the result set. SELECT albums . name , albums . year , artists . name FROM albums JOIN artists ON albums . artist_id = artists . id WHERE albums . year > 1980 ; 1 SELECT albums . name AS 'Album' , albums . year , artists . name AS 'Artist' FROM albums JOIN artists ON albums . artist_id = artists . id WHERE albums . year > 1980 ;","title":"Multiple Tables"},{"location":"Codecademy Learn SQL/#recap","text":"Primary Key is a column that serves a unique identifier for row in the table. Values in this column must be unique and cannot be NULL . Foreign Key is a column that contains the primary key to another table in the database. It is used to identify a particular row in the referenced table. Joins are used in SQL to combine data from multiple tables. INNER JOIN will combine rows from different tables if the join condition is true. LEFT OUTER JOIN will return every row in the left table, and if the join condition is not met, NULL values are used to fill in the columns from the right table. AS is a keyword in SQL that allows you to rename a column or table in the result set using an alias.","title":"Recap"},{"location":"Codecademy Learn SQL/#project-querying-tables","text":"In this project you will practice querying multiple tables using joins. The instructions provided are a general guideline, but feel free to experiment creating your own tables, adding data, and writing more queries. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 --Create a table named tracks with an id, title, and album_id column. The id column should be the PRIMARY KEY. CREATE TABLE tracks ( id INTEGER PRIMARY KEY , title TEXT , albums_id INTEGER ); --\"Smooth Criminal\" is a track from Michael Jackson's \"Bad\" album. Add this track to the database. INSERT INTO tracks ( id , title , albums_id ) VALUES ( 1 , \"Smooth Criminal\" , 8 ); --Add more tracks to the database. INSERT INTO tracks ( id , title , albums_id ) VALUES ( 2 , \"aaa\" , 1 ); INSERT INTO tracks ( id , title , albums_id ) VALUES ( 3 , \"bbb\" , 1 ); INSERT INTO tracks ( id , title , albums_id ) VALUES ( 4 , \"ccc\" , 8 ); INSERT INTO tracks ( id , title , albums_id ) VALUES ( 5 , \"ddd\" , 8 ); INSERT INTO tracks ( id , title , albums_id ) VALUES ( 6 , \"eee\" , 8 ); --Combine the albums and tracks tables using an INNER JOIN. Order the query by album_id. SELECT * FROM albums JOIN tracks ON albums . artist_id = tracks . id ; --Combine the albums and artists table using a LEFT OUTER JOIN. Let albums be the left table. SELECT * FROM albums LEFT JOIN artists ON albums . artist_id = artists . id ; --Combine the albums and artists table using a LEFT OUTER JOIN. Let artists be the left table. SELECT * FROM artists LEFT JOIN albums ON albums . artist_id = artists . id ; --Use any join you like to combine the albums and tracks table. Rename the album_id column to Albums. SELECT * FROM albums LEFT JOIN tracks ON albums . artist_id = tracks . id ;","title":"Project Querying Tables"},{"location":"Codecademy Learn the Command Line Notes/","text":"Foreword Commands and code snippets. From Codecademy. Navigation \u00b6 Bash means \u2018Bourne again shell\u2019. There exists other bash programs : ksh, tcsh, zsh, etc. Common terminals: Ubuntu gnome, KDE konsole, eterm, txvt, kvt, nxterm, eterm. A CLI has its advantages over a GUI. Terminal bash ; open another \u2018bash\u2019 in the same bash window. exit ; close one instance of \u2018bash\u2019. export ; export variable to the next bash. export MESSAGE=\"Hi\" . Variables are associated with one bash. One change in one bash does not affect the other bash. man <cmd> ; call the manual, help about a command. man cp ; open the manual about command cp (or any other command). cp --help ; command cp arguments (or any other command). command cp ; get info on command cp (or any other command). type cp ; find where command cp is located and what kind of command (an alias or else ) (or any other command). which cp ; locate command cp (or any other command). which make ; check out where make is. pwd ; print working directory. dir ; list of directory. up / down arrow; resume a past command. tab ; autocomplete. Change dir cd/aaa ; change to directory aaa . cd ~ , cd ~user ; go back home to user directory. cd .. ; go up one level. cd ../.. ; go up two levels. cd ../../.. ; go up three levels. cd temp ; go to directory temp . cd stuff ; if we are in dir temp , switch to directory stuff . cd temp/stuff ; switch directly to directory stuff . cd 'en co re' ; switch to directory en co re . Make dir mkdir aaa ; make directory aaa . mkdir -p aaa/bbb ; make directory aaa with sub-directory bbb . mkdir 'en co re' ; make directory en co re . Remove dir rmdir aaa ; remove directory aaa . rmdir 'en co re' ; remove directory en co re . rmdir aaa/bbb ; remove both directory and sub-directory (if both empty). rmdir -p aaa/bbb ; remove all (you cannot remove a directory if it is non-empty (filled with sub-directories and files)) . rmdir -p aaa/* ; remove all. Switch dir pushd ; save current location; then, change location (another directory). popd ; return to pushd. (for example, you are in aaa , type pushd , go to bbb , type pushb , go to ccc , type popd , return to bbb , type popd , return to aaa ) . Make file touch iamcoo.txt ; create an empty text file. touch temp/ismvoo.txt ; create an empty text file in the temp directory. Edit file less test.txt ; open the file and display its content. Within less : q ; quit. arrows, pgUp , pgDn ; move. h ; help. For other files extensions like tar and GNU archive, use tvf . Manipulation \u00b6 List file & dir ls ; list of directories and files. ls /bin/bash ; list of a remote directory and files. ls -a ; list all. ls -t ; list in alphanumeric order, when they were last modified. ls -l ; list in long format. ls -la ; list all in long format. ls -alt ; list all in long format and ordered. ls /aaa /bbb ; list both directories. ls -r ; list in reverse. ls ../paint/ ; list an upper directory. Copy file & dir cp aaa.txt bbb.txt ; copy aaa.txt file, paste it or create a copy named bbb.txt . cp aaa.txt dir/ ; copy aaa.txt into directory dir . cp aaa.txt bbb.txt dir/ ; copy aaa.txt and bbb.txt into directory dir . cp \\*.txt dir/ ; copy all .txt files into directory dir . cp aaa.txt dir/dir2 ; copy aaa.txt into sub-directory dir2 . cp -r aaa bbb ; copy dir1 , create a copy names dir2 with the exact same content. cp -i ; interactive, to prompt the user. cp f\\* ../paint/ ; copy all files beginning with f to an upper directory. Wildcards \\* ; wildcard for any string. ? ; wildcard for any character. \\*.txt ; all files finishing with .txt . r\\* ; all files beginning with r . ??a.txt ; all files beginning with two characters + a.txt . backup[[:digit:]] ; all file beginning with backup + any digit. [abc]\\* ; all files beginning with either a , b or c [[:upper:]]\\* ; all files beginning with an upper case. \\*[![:lower:]] ; all file not finishing with a lower case. Move file & dir mv aaa.txt bbb.txt ; move file or cut aaa.txt and paste bbb.txt . mv aaa.txt dir/ ; move file aaa.txt into directory dir . mv aaa.txt bbb.txt dir/ ; move files aaa.txt and bbb.txt into directory dir . mv \\*.txt dir/ ; move all .txt files into directory dir . mv aaa.txt dir/dir2 ; move file aaa.txt into sub-directory dir2 . mv -i ; interactive, prompt the user. Remove file & dir rm aaa.txt ; remove file aaa.txt . rm test1.txt test2.txt ; remove both files. rm aaa/\\* ; remove all files in the directory aaa . rm -r aaa ; remove directory aaa , must be empty. rm -rf aaa ; remove directory aaa and its files. rm -i ; interactive, prompt the user. Redirection \u00b6 Let\u2019s begin by taking a closer look at input and output. In the terminal, after the shell prompt, type : 1 $ echo 'Hello' The echo command accepts the string \u2018Hello\u2019 as standard input, and echoes the string 'Hello' back to the terminal as standard output. standard input, abbreviated as stdin , is information inputted into the terminal through the keyboard or input device. standard output, abbreviated as stdout , is the information outputted after a process is run. standard error, abbreviated as stderr , is an error message outputted by a failed process. Redirection ( > ) reroutes standard input, standard output, and standard error to or from a different location. Type : 1 $ echo 'Hello' > hello.txt Type : 1 $ cat hello.txt The > command redirects the standard output to a file. The standard output 'Hello' is redirected by > to the file hello.txt , and entered as the standard input. The cat command outputs the contents of a file to the terminal. When you type cat hello.txt , the contents of hello.txt are displayed. Type : 1 $ ls -l This is the filesystem we\u2019ll work with. Create a file. Type : 1 $ touch ocean.txt Fill the file with values (ocean names). Type : 1 $ cat oceans.txt > continents.txt Use cat to view the contents of continents.txt . Notice that we only see oceans as output: 1 $ cat continents.txt > takes the standard output of the command on the left, and redirects it to the file on the right. Here the standard output of cat oceans.txt is redirected to continents.txt . Note that > OVERWRITES all original content in continents.txt . When you view the output data by typing cat on continents.txt , you will see only the contents of oceans.txt . Type : 1 $ cat glaciers.txt >> rivers.txt Use cat to view the contents of rivers.txt : 1 $ cat rivers.txt Notice that we see both rivers and glaciers as output. Type: 1 $ cat glaciers.txt >> rivers.txt >> takes the standard output of the command on the left and APPENDS it to the file on the right. You can view the output data of the file with cat and the filename. Here, the output data of rivers.txt will contain the original contents of rivers.txt with the content of glaciers.txt appended to it. Type : 1 $ touch lakes.txt Fill the files with values (lake names). Type : 1 $ cat < lakes.txt < takes the standard input from the file on the right and inputs it into the program on the left. Here, lakes.txt is the standard input for the cat command. The standard output appears in the terminal. Let\u2019s try some more redirection commands. Type : 1 $ touch volcanoes.txt Fill the files with values (volcano names). Type : 1 $ cat volcanoes.txt | wc You get the count for: lines, words, bytes. Type : 1 $ cat volcanoes.txt | wc | cat > islands.txt Use cat to output the contents in islands.txt . The next command should now equals $ cat volcanoes.txt | wc . 1 $ cat volcanoes.txt | wc | is a \u2018pipe\u2019 or \u2018pipeline\u2019 The | takes the standard output of the command on the left, and PIPES it as standard input to the command on the right. You can think of this as \u2018command to command\u2019 redirection. Here, again, the output of cat volcanoes.txt is the standard input of wc . In turn, the wc command outputs the number of lines, words, and characters in volcanoes.txt , respectively: 1 2 $ cat volcanoes.txt | wc | cat > islands.txt $ less islands.txt Multiple | s can be chained together. Here the standard output of cat volcanoes.txt is \u2018piped\u2019 to the wc command. The standard output of wc is then \u2018piped\u2019 to cat. Finally, the standard output of cat is redirected to islands.txt . You can view the output data of this chain by typing cat islands.txt . A few commands are particularly powerful when combined with redirection. Let\u2019s try them out. First, use cat to output the contents of lakes.txt . Then, sort it. Type : 1 2 $ cat lakes.txt $ sort lakes.txt The lakes in lakes.txt are listed in alphabetical order. Type: 1 $ cat lakes.txt | sort sort takes the standard input and orders it alphabetically for the standard output. The lakes in lakes.txt are listed in alphabetical order. Use cat to output the contents of sorted-lakes.txt . 1 $ cat lakes.txt | sort > sorted-lakes.txt The command takes the standard output from cat lakes.txt and \u2018pipes\u2019 it to sort in ascending order. The standard output of sort is redirected to sorted-lakes.txt . You can view the output data by typing: 1 $ cat sorted-lakes.txt Type : 1 $ touch deserts.txt Fill the file with values. Type : 1 2 $ cat deserts.txt $ uniq deserts.txt You get to see all entries and unique entries. Type : 1 $ sort deserts.txt | uniq You get to see unique entries sorted. Type : 1 $ sort deserts.txt | uniq > uniq-deserts.txt You save the result in uniq-deserts.txt . Use cat to output the contents of uniq-deserts.txt . uniq stands for \u2018unique\u2019 and filters out ADJACENT, duplicate lines in a file. Here uniq deserts.txt filters out duplicates of \u2018Sahara Desert\u2019, because the duplicate of \u2018Sahara Desert\u2019 directly follows the previous instance. The \u2018Kalahari Desert\u2019 duplicates are not adjacent, and thus remain. Type : 1 $ grep Mount mountains.txt grep stands for \u2018global regular expression print\u2019. It searches files for lines that match a pattern and returns the results. It is also case sensitive. Here, grep searches for \u2018Mount\u2019 in mountains.txt . Type: 1 $ grep -i Mount mountains.txt grep -i enables the command to be case insensitive. Here, grep searches for capital or lowercase strings that match \u2018Mount\u2019 in mountains.txt . The above commands are a great way to get started with grep . If you are familiar with regular expressions, you can use regular expressions to search for patterns in files. grep can also be used to search within a directory. Type : 1 $ grep -R Arctic /home/ccuser/workspace/geography ype : 1 $ grep -R Arctic /home/ccuser/workspace/geography grep -R searches files in a directory and outputs filenames and lines containing matched results. -R stands for \u2018recursive\u2019. Here grep -R searches the /home/ccuser/workspace/geography directory for the string \u2018Arctic\u2019 and outputs filenames and lines with matched results. Type: 1 $ grep -R Gambino . grep -R searches ALL files recursively and outputs filenames and lines containing matched results. 1 $ grep -Rl Arctic /home/ccuser/workspace/geography grep -Rl searches all files in a directory and outputs only filenames with matched results. -R stands for \u2018recursive\u2019 and l stands for \u2018files with matches\u2019. Here grep -Rl searches the /home/ccuser/workspace/geography directory for the string \u2018Arctic\u2019 and outputs filenames with matched results. Use cat to display the contents of forests.txt . grep this file.txt ; search for this in file.txt, print the output on screen. grep this < file.txt ; feed file.txt to process grep to look for this , print on screen grep this file.txt > file_this.txt ; write, overwrite the output in file_this.txt . grep this file.txt >> file_this.txt ; write, append the output in file_this.txt . grep \"is\" file.txt ; search pattern is , precisely, in words or alone. grep line file.txt ; search pattern line . grep -n line file.txt ; show the line number where it finds pattern line . grep -i line file.txt ; case insensitive grep -v line file.txt ; inverse or when it does not have the pattern line If you search for one \u201cfile\u201d in the current directory, type: 1 find . _name \"file.txt\" find / _name \"file.txt\" searches \u201cfile\u201d in multiple directories. find dirA dirB dirC _name \"file.txt\" search \u201cfile\u201d in one or more directories. Type : 1 $ sed 's/snow/rain/' forests.txt sed stands for \u2018stream editor\u2019. It accepts standard input and modifies it based on an expression, before displaying it as output data. It is similar to \u2018find and replace\u2019. Let\u2019s look at the expression 's/snow/rain/' : s : stands for \u2018substitution\u2019. it is always used when using sed for substitution. snow : the search string, the text to find. rain : the replacement string, the text to add in place. In this case, sed searches forests.txt for the word 'snow' and replaces it with 'rain' . Importantly, the above command will only replace the FIRST instance of 'snow' on a line. 1 $ sed 's/snow/rain/g' forests.txt The above command uses the g expression, meaning \u2018global\u2019. Here sed searches forests.txt for the word 'snow' and replaces it with 'rain' , globally. ALL instances of 'snow' on a line will be turned to 'rain' . Let\u2019s summarize what we\u2019ve done so far. The common redirection commands are: > ; redirects standard output of a command to a file, overwriting previous content. >> ; redirects standard output of a command to a file, appending new content to old content. < ; redirects standard input to a command. | ; redirects standard output of a command to another command. A number of other commands are powerful when combined with redirection commands: echo 'a' ; display \u2018a\u2019 in the terminal. touch ; create a file. cat ; display the content of a file in the terminal. less ; edit the content of a file in the terminal; prefer nano , it\u2019s more user-friendly. sort ; sorts lines in a file in alphabetical order. uniq ; filters duplicates in a file. grep ; searches for a text pattern in files and outputs it. find ; searches for files, not the content. sed ; searches for a text pattern in files, modifies it, and outputs it. head / tail ; ls , but only the top/bottom results. Examples echo 'thisthat' ; show thisthat . echo $USER ; show user variable. echo \\* , d\\* , s\\* , [[:upper]]\\* , /usr/\\*/share ; show the content of the current directory according to the specified string (wildcards and characters). echo ~ ; show /home/user . echo .\\* ; show hidden files. echo 2 + 2 ; show 2 + 2. echo $((2 + 2)) ; show 4. echo $(($((5 \\*\\* 2)) \\* 3)) ; show 75. echo Front-{A,B,C}-Back ; show Front-A-Back Front-B-Back Front-C-Back. echo Number-{1...5} ; show Number-1 Number-2 Number-3 Number-4 Number-5. echo {Z...A} ; show Z Y X W\u2026 echo a{A{1,2}, B{3,4}}b ; show aS1b aA2b aB3b aB4b. mkdir {2007...2009}-0{1...9} {2007...2009}-{10...12} ; show 2007-01 2007-02 2007-03 2007-04 2007-05 2007-06 2007-07 2007-08 2007-09 2007-10 2007-11 2007-12 2008-01\u2026 echo $(ls) ; show, not a list, but a paragraph of files and directories. echo $USER $((2 + 2)) ; show user 4. echo $(cal) or echo ' $(cal) ' ; show a calendar. With . , special characters become ordinary characters except for $ , \\ or ' ; and with . , it doesn\u2019t suppress commands, variables and aliases as with . . Try : echo text ~/\\*.txt {a,b} $(echo foo) $((2 + 2)) . echo 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))' . echo 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))' . print env | less ; show a list of available variables. cat test.txt ; showthe content of file text.txt . ls > file.txt ; send command and results into a file, if the file exists, it overwrite it, use cat file.txt to edit it. ls >> file.txt ; send command and results into a file, if the file exists, it appends the new content to the existing one, use cat file.txt to edit it. cat test.txt > bbb.txt ; write (overwrite) the (existing) content of test.txt into bbb.txt . cat test.txt >> bbb.txt ; write the content of test.txt into bbb.txt or append it to bbb.txt . wc test.txt ; count lines, words and bytes of test.txt . cat test.txt | wc ; show the wc of test.txt . cat test.txt | wc > bbb.txt ; write the wc of test.txt into bbb.txt . sort < file.txt ; push the content into command sort. cat < aaa.txt ; open file aaa.txt to edit. sort file.txt ; sort the content. cat aaa.txt | sort > sorted_aaa.txt ; in addition, send the results in a new files. uniq file.txt ; extract unique values of the content. ls -l | less ; the list goes into the reader. ls -l | head ; the list shows the 10 top lines only. ls -l | tail ; the list shows the 10 bottom lines only. Environment \u00b6 Each time we launch the terminal application, it creates a new session. The session immediately loads settings and preferences that make up the command line environment. We can configure this environment to support the commands and programs, customize greetings and command aliases, and create variables to share across commands and programs. A simple, command line text editor: nano . It is more powerful than less . Type : 1 $ nano hello.txt In nano , at the top of the window, type : 1 $ 'Hello, I am nano.' Using the menu at the bottom of the terminal for reference, type Ctrl + O (the letter, not the number) to save the file. Press Enter , when prompted about the filename to write. Then type Ctrl + X to exit nano . Finally, type clear to clear the terminal window. The command prompt should now be at the top of the window. You just edited a file in the nano text editor. Type: 1 $ nano hello.txt nano is a command line text editor. It works just like a desktop text editor, except that it is accessible from the command line and only accepts keyboard input. The command nano hello.txt opens a new text file named hello.txt in the nano text editor. \u2018Hello, I am nano\u2019 is a text string entered in nano through the cursor. The menu of keyboard commands at the bottom of the window allow us to save changes to hello.txt and exit nano . The ^ stands for the Ctrl key. Ctrl + O saves a file. o stands for output. Ctrl + X exits the nano program. x stands for exit. Ctrl + G opens a help menu. Clear clears the terminal window, moving the command prompt to the top of the screen. nano editor Now that you are familiar with editing text in nano , let\u2019s create a file to store environment settings. Type : 1 $ nano ~/.bash_profile This opens up a new file in nano . In ~/.bash_profile , at the top of the file, type : 1 $ echo 'Welcome, Jane Doe' You can use your name in place of \u2018Jane Doe\u2019. Type Ctrl + O to save the file. Press Enter to write the filename. Type Ctrl + X to exit. Finally, type clear to clear the terminal window. Type : 1 $ source ~/.bash_profile You should see the greeting you entered. You created a file in nano called ~/.bash_profile and added a greeting. 1 $ nano ~/.bash_profile ~/.bash_profile is the name of file used to store environment settings. It is commonly called the \u2018bash profile\u2019. When a session starts, it will load the contents of the bash profile before executing commands. The ~ represents the user\u2019s home directory. The . indicates a hidden file. The name ~/.bash_profile is important, since this is how the command line recognizes the bash profile. The command nano ~/.bash_profile opens up ~/.bash_profile in nano . The text echoes \u2018Welcome, Jane Doe\u2019 and creates a greeting in the bash profile, which is saved. It tells the command line to echo the string \u2018Welcome, Jane Doe\u2019 when a terminal session begins. The command source ~/.bash_profile ACTIVATES the changes in ~/.bash_profile for the current session. Now that we know what bash profile is, let\u2019s continue configuring the environment by adding command aliases. Open ~/.bash_profile in nano . In ~/.bash_profile , beneath the greeting you created, type : 1 $ alias pd = 'pwd' Save the file. Press Enter to write the filename. Exit nano . Clear the terminal window. In the command line, use the source command to activate the changes in the current session. 1 $ source ~/.bash_profile Let\u2019s try out the alias. Type : 1 $ pd You should see the same output as you would by typing the pwd command. What happens when you store this alias in ~/.bash_profile ? 1 $ alias pd = 'pwd' The alias command allows you to create keyboard shortcuts, or aliases, for commonly used commands. Here alias pd='pwd' creates the alias pd for the pwd command, which is then saved in the bash profile. Each time you enter pd , the output will be the same as the pwd command. The command source ~/.bash_profile makes the alias pd available in the current session. Each time we open up the terminal, we can use the pd alias. Let\u2019s practice aliases some more. Open ~/.bash_profile in nano . In the bash profile, beneath the previous alias, add : 1 $ alias hy = 'history' Save the file. Press Enter to write the filename. Add another alias: 1 $ alias ll = 'ls -la' Save the file. Press Enter to write the filename. Exit nano . Clear the terminal window. In the command line, use source to activate the changes to the bash profile for the current session. Let\u2019s try out the aliases. Type: 1 $ hy What happens when you store the following aliases in ~/.bash_profile ? 1 $ alias hy = 'history' hy is set as alias for the history command in the bash profile. The alias is then made available in the current session through source. By typing hy , the command line outputs a history of commands that were entered in the current session. Type: 1 $ alias ll = 'ls -la' ll is set as an alias for ls -la and made available in the current session through source. By typing ll , the command line now outputs all contents and directories in long format, including all hidden files. Now that you are familiar with configuring greetings and aliases, let\u2019s move on to setting environment variables. Open ~/.bash_profile in nano . In the bash profile, beneath the aliases, on a new line, type: 1 $ export USER = 'Jane Doe' Feel free to use your own name. Save the file. Press Enter to write the filename. Exit nano . Finally, clear the terminal. In the command line, use source to activate the changes in the bash profile for the current session. Type : 1 $ echo $USER This should return the value of the variable that you set. What happens when you store this in ~/.bash_profile ? 1 $ export USER = 'Jane Doe' Environment variables are variables that can be used across commands and programs and hold information about the environment. The line USER='Jane Doe' sets the environment variable USER to a name 'Jane Doe' . Usually the USER variable is set to the name of the computer\u2019s owner. The line export makes the variable to be available to all child sessions initiated from the session you are in. This is a way to make the variable persist across programs. At the command line, the command echo $USER returns the value of the variable. Note that $ is always used when returning a variable\u2019s value. Here, the command echo $USER returns the name set for the variable. Let\u2019s learn a few more environment variables, starting with the variable for the command prompt. Open ~/.bash_profile in nano . On a new line, beneath the last entry, type 1 $ export PS1 = '>> ' Save the file. Press Enter to write the filename. Exit nano . Finally, clear the terminal window. In the command line, use source to activate the changes in the bash profile for the current shell session. Let\u2019s try out the new command prompt. Type : 1 $ echo 'hello' Type : 1 $ ls -alt Did you notice that the prompt has changed? What happens when this is stored in ~/.bash_profile ? 1 $ export PS1 = '>> ' PS1 is a variable that defines the makeup and style of the command prompt. echo $PS1 prints the variable. PS1=\"value\" changes the variable value. PS1='>> ' sets the command prompt variable and exports the variable. It makes the variable available in all sub programs of the current shell. Here we change the default command prompt from $ to >> . After using the source command, the command line displays the new command prompt. Let\u2019s learn about two more environment variables. Type : 1 $ echo $HOME This returns the value of the HOME variable. What happens when you type this command? 1 $ echo $HOME The HOME variable is an environment variable that displays the path of the home directory. Here by typing echo $HOME , the terminal displays the path /home/ccuser as output. cd $HOME goes to the home directory. You can customize the HOME variable if needed, but in most cases this is not necessary. In the command line, type : 1 $ echo $PATH Type : 1 /bin/pwd Type : 1 /bin/ls What happens when you type this command? 1 2 $ echo $PATH /home/ccuser/.gem/ruby/2.0.0/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/sbin:/sbin:/bin PATH is an environment variable that stores a list of directories separated by a colon. Looking carefully, echo $PATH lists the following directories: /home/ccuser/.gem/ruby/2.0.0/bin . /usr/local/sbin . /usr/local/bin . /usr/bin . /usr/sbin . /sbin . /bin . Each directory contains scripts for the command line to execute. The PATH variable simply lists which directories contain scripts. For example, many COMMANDS we\u2019ve learned are scripts stored in the /bin directory. 1 $ /bin/pwd This is the script that is executed when you type the pwd command. 1 $ /bin/ls This is the script that is executed when you type the ls command. In advanced cases, you can customize the PATH variable when adding scripts of your own. Type : 1 $ env Type : 1 $ env | grep PATH What happens when you type this command? 1 $ env The env command stands for \u2018environment\u2019, and returns a list of the environment variables for the current user. Here, the env command returns a number of variables, including PATH , PWD , PS1 , and HOME . 1 2 $ env | grep PATH $ env | grep aliasname env | grep PATH is a command that displays the value of a single environment variable. Here the standard output of env is \u2018piped\u2019 to the grep command. grep searches for the value of the variable PATH and outputs it to the terminal. You learned to use the bash profile to configure the environment. What can we generalize so far? The environment refers to the preferences and settings of the current user. Let\u2019s summarize what we\u2019ve done so far. The nano editor is a command line text editor used to configure the environment. ~/.bash_profile is where environment settings are stored. You can edit this file with nano . Environment variables are variables that can be used across commands and programs and hold information about the environment. export VARIABLE='Value' sets and exports an environment variable. USER is the name of the current user. PS1 is the command prompt. HOME is the home directory. It is usually not customized. PATH returns a colon separated list of file paths. It is customized in advanced cases. echo $PATH prints the path. PATH=\"value\" changes the path.. export PATH=/home/dir/bin:$PATH appends the new path to environment variable PATH. env returns a list of environment variables. Multi-Users \u00b6 Linux is multi-user : multiple users at the same time as opposed to OS X or Windows; just like the UNIX mainframe computers with terminals, users and superuser concepts. Access rights Type : 1 $ ls -l Read, from left to right : \u2013 or d : file or dir. File or dir name. Owner access ( r w x\u2026- ). Group access ( r w x\u2026- ). All access ( r w x\u2026- ). Owner group, size, date. How do we change the acces rights? With chmod . First, there are access right for: u ser. g roup. o thers. Second, there levels. Each level has a numeric value.: r ead: 4. w rite: 2. x ecute: 1. For example, using levels: o+w : others can write the file (create). u+x : users can execute the file. g-x : group can no longer execute the file. etc. chmod o+w file1.txt for example. Why numeric value? It an alternative way for chmod to assign access rights. Levels are ranked with values: Level Binary Decimal r w x 111 7 r w - 110 6 r - - 100 4 - - - 000 0 r - x 101 5 A 7 grants full rights vs a 0 that grants no rights. For example: chmod 600 file ; change the file access rights to rw- --- --- . chmod 600 dir ; change the directory access rights. How are the values calculated? 1 2 3 4 5 6 r+w+x = 4 +2+1 = 7 r = 4 = 4 x = 1 = 1 r+w = 4 +2 = 6 ... ... Once you have the values, you can set the access rights: 1 2 3 4 u g o --- --- --- rwx rw- --x 7 6 1 Therefore, chmod 761 set the access rights rwx rw- --x to a file, files, a directory or directories. Ownership chown thou file ; assign a new owner, thou , to \u2018file\u2019. chown thou dir ; \u2026 to dir. chgrp newgr file assign a new group owner, newgr , to \u2018file\u2019. chgrp newgr dir ; \u2026 to dir. Superuser su ; superuser login. su file ; unlock the file with the superuser password. sudo ; do it with the privilege of a superuser. sudo apt-get update ; updates the database. sudo apt-get upgrade ; upgrades all packages (update before upgrading). sudo apt-get install build-essential ; install a useful package (or any other package). sudo apt-get install git ; install \u2018git\u2019. which git ; find where \u2018git\u2019 is located (if it\u2019s instaled). sudo apt-get remove git ; remove \u2018git\u2019. sudo apt-get purge git ; remove \u2018git\u2019 and purge any remainings. sudo chnow user file ; change the owner. Multi-Tasks \u00b6 Linux is multi-task; multiple tasks can run at the same time like OS X or Windows or UNIX; Linux kernel runs processes; they take turns at the processor(s). All programs and processes can be launched/killed from the GUI and the CLI. Show top ; show process dashboard by PID number; ? for help, q for quit. ps ; show the process list. ps aux ; show all process. ps aux | grep 'top' ; filter top processes. ps aux | grep bash ; filter processes related to the bash. ps aux | grep bash | sort ; \u2026 and sort them. Manage Ctrl + Z ; pause a process, put it in the background. fg ; foreground, bring back the process. jobs ; list paused processes. fg #PID ; bring back process #PID (if there is more than one process on pause, you must identify the process). Ctrl + C ; terminate the active process. xload ; display the system load in a new windows, but jam the current terminal (open another or several terminals then). xload & ; runs in the background Ctrl + X `; suspend the process and unjam the terminal. by ; resume the process. ps ; show processes and their #PID. ps x | grep bad_program ; find the bad processes. kill can send a signal from the OS to a process. For example : when you log off, you send a signal to terminate a word processing program and save the file before closing. kill #PID ; kill the process. kill -STOP #PID ; pause the process. kill -TERM #PID ; terminate the process. kill -SIGTERM #PID ; terminate the process. kill -SIGKILL #PID ; kill the process. kill -KILL ; force closing of the current process. kill -9 #PID ; force closing the process. Killing sequence: kill #PID ; doesn\u2019t work\u2026 kill -9 #PID , or..","title":"Codecademy, Learn the Command Line Notes"},{"location":"Codecademy Learn the Command Line Notes/#navigation","text":"Bash means \u2018Bourne again shell\u2019. There exists other bash programs : ksh, tcsh, zsh, etc. Common terminals: Ubuntu gnome, KDE konsole, eterm, txvt, kvt, nxterm, eterm. A CLI has its advantages over a GUI. Terminal bash ; open another \u2018bash\u2019 in the same bash window. exit ; close one instance of \u2018bash\u2019. export ; export variable to the next bash. export MESSAGE=\"Hi\" . Variables are associated with one bash. One change in one bash does not affect the other bash. man <cmd> ; call the manual, help about a command. man cp ; open the manual about command cp (or any other command). cp --help ; command cp arguments (or any other command). command cp ; get info on command cp (or any other command). type cp ; find where command cp is located and what kind of command (an alias or else ) (or any other command). which cp ; locate command cp (or any other command). which make ; check out where make is. pwd ; print working directory. dir ; list of directory. up / down arrow; resume a past command. tab ; autocomplete. Change dir cd/aaa ; change to directory aaa . cd ~ , cd ~user ; go back home to user directory. cd .. ; go up one level. cd ../.. ; go up two levels. cd ../../.. ; go up three levels. cd temp ; go to directory temp . cd stuff ; if we are in dir temp , switch to directory stuff . cd temp/stuff ; switch directly to directory stuff . cd 'en co re' ; switch to directory en co re . Make dir mkdir aaa ; make directory aaa . mkdir -p aaa/bbb ; make directory aaa with sub-directory bbb . mkdir 'en co re' ; make directory en co re . Remove dir rmdir aaa ; remove directory aaa . rmdir 'en co re' ; remove directory en co re . rmdir aaa/bbb ; remove both directory and sub-directory (if both empty). rmdir -p aaa/bbb ; remove all (you cannot remove a directory if it is non-empty (filled with sub-directories and files)) . rmdir -p aaa/* ; remove all. Switch dir pushd ; save current location; then, change location (another directory). popd ; return to pushd. (for example, you are in aaa , type pushd , go to bbb , type pushb , go to ccc , type popd , return to bbb , type popd , return to aaa ) . Make file touch iamcoo.txt ; create an empty text file. touch temp/ismvoo.txt ; create an empty text file in the temp directory. Edit file less test.txt ; open the file and display its content. Within less : q ; quit. arrows, pgUp , pgDn ; move. h ; help. For other files extensions like tar and GNU archive, use tvf .","title":"Navigation"},{"location":"Codecademy Learn the Command Line Notes/#manipulation","text":"List file & dir ls ; list of directories and files. ls /bin/bash ; list of a remote directory and files. ls -a ; list all. ls -t ; list in alphanumeric order, when they were last modified. ls -l ; list in long format. ls -la ; list all in long format. ls -alt ; list all in long format and ordered. ls /aaa /bbb ; list both directories. ls -r ; list in reverse. ls ../paint/ ; list an upper directory. Copy file & dir cp aaa.txt bbb.txt ; copy aaa.txt file, paste it or create a copy named bbb.txt . cp aaa.txt dir/ ; copy aaa.txt into directory dir . cp aaa.txt bbb.txt dir/ ; copy aaa.txt and bbb.txt into directory dir . cp \\*.txt dir/ ; copy all .txt files into directory dir . cp aaa.txt dir/dir2 ; copy aaa.txt into sub-directory dir2 . cp -r aaa bbb ; copy dir1 , create a copy names dir2 with the exact same content. cp -i ; interactive, to prompt the user. cp f\\* ../paint/ ; copy all files beginning with f to an upper directory. Wildcards \\* ; wildcard for any string. ? ; wildcard for any character. \\*.txt ; all files finishing with .txt . r\\* ; all files beginning with r . ??a.txt ; all files beginning with two characters + a.txt . backup[[:digit:]] ; all file beginning with backup + any digit. [abc]\\* ; all files beginning with either a , b or c [[:upper:]]\\* ; all files beginning with an upper case. \\*[![:lower:]] ; all file not finishing with a lower case. Move file & dir mv aaa.txt bbb.txt ; move file or cut aaa.txt and paste bbb.txt . mv aaa.txt dir/ ; move file aaa.txt into directory dir . mv aaa.txt bbb.txt dir/ ; move files aaa.txt and bbb.txt into directory dir . mv \\*.txt dir/ ; move all .txt files into directory dir . mv aaa.txt dir/dir2 ; move file aaa.txt into sub-directory dir2 . mv -i ; interactive, prompt the user. Remove file & dir rm aaa.txt ; remove file aaa.txt . rm test1.txt test2.txt ; remove both files. rm aaa/\\* ; remove all files in the directory aaa . rm -r aaa ; remove directory aaa , must be empty. rm -rf aaa ; remove directory aaa and its files. rm -i ; interactive, prompt the user.","title":"Manipulation"},{"location":"Codecademy Learn the Command Line Notes/#redirection","text":"Let\u2019s begin by taking a closer look at input and output. In the terminal, after the shell prompt, type : 1 $ echo 'Hello' The echo command accepts the string \u2018Hello\u2019 as standard input, and echoes the string 'Hello' back to the terminal as standard output. standard input, abbreviated as stdin , is information inputted into the terminal through the keyboard or input device. standard output, abbreviated as stdout , is the information outputted after a process is run. standard error, abbreviated as stderr , is an error message outputted by a failed process. Redirection ( > ) reroutes standard input, standard output, and standard error to or from a different location. Type : 1 $ echo 'Hello' > hello.txt Type : 1 $ cat hello.txt The > command redirects the standard output to a file. The standard output 'Hello' is redirected by > to the file hello.txt , and entered as the standard input. The cat command outputs the contents of a file to the terminal. When you type cat hello.txt , the contents of hello.txt are displayed. Type : 1 $ ls -l This is the filesystem we\u2019ll work with. Create a file. Type : 1 $ touch ocean.txt Fill the file with values (ocean names). Type : 1 $ cat oceans.txt > continents.txt Use cat to view the contents of continents.txt . Notice that we only see oceans as output: 1 $ cat continents.txt > takes the standard output of the command on the left, and redirects it to the file on the right. Here the standard output of cat oceans.txt is redirected to continents.txt . Note that > OVERWRITES all original content in continents.txt . When you view the output data by typing cat on continents.txt , you will see only the contents of oceans.txt . Type : 1 $ cat glaciers.txt >> rivers.txt Use cat to view the contents of rivers.txt : 1 $ cat rivers.txt Notice that we see both rivers and glaciers as output. Type: 1 $ cat glaciers.txt >> rivers.txt >> takes the standard output of the command on the left and APPENDS it to the file on the right. You can view the output data of the file with cat and the filename. Here, the output data of rivers.txt will contain the original contents of rivers.txt with the content of glaciers.txt appended to it. Type : 1 $ touch lakes.txt Fill the files with values (lake names). Type : 1 $ cat < lakes.txt < takes the standard input from the file on the right and inputs it into the program on the left. Here, lakes.txt is the standard input for the cat command. The standard output appears in the terminal. Let\u2019s try some more redirection commands. Type : 1 $ touch volcanoes.txt Fill the files with values (volcano names). Type : 1 $ cat volcanoes.txt | wc You get the count for: lines, words, bytes. Type : 1 $ cat volcanoes.txt | wc | cat > islands.txt Use cat to output the contents in islands.txt . The next command should now equals $ cat volcanoes.txt | wc . 1 $ cat volcanoes.txt | wc | is a \u2018pipe\u2019 or \u2018pipeline\u2019 The | takes the standard output of the command on the left, and PIPES it as standard input to the command on the right. You can think of this as \u2018command to command\u2019 redirection. Here, again, the output of cat volcanoes.txt is the standard input of wc . In turn, the wc command outputs the number of lines, words, and characters in volcanoes.txt , respectively: 1 2 $ cat volcanoes.txt | wc | cat > islands.txt $ less islands.txt Multiple | s can be chained together. Here the standard output of cat volcanoes.txt is \u2018piped\u2019 to the wc command. The standard output of wc is then \u2018piped\u2019 to cat. Finally, the standard output of cat is redirected to islands.txt . You can view the output data of this chain by typing cat islands.txt . A few commands are particularly powerful when combined with redirection. Let\u2019s try them out. First, use cat to output the contents of lakes.txt . Then, sort it. Type : 1 2 $ cat lakes.txt $ sort lakes.txt The lakes in lakes.txt are listed in alphabetical order. Type: 1 $ cat lakes.txt | sort sort takes the standard input and orders it alphabetically for the standard output. The lakes in lakes.txt are listed in alphabetical order. Use cat to output the contents of sorted-lakes.txt . 1 $ cat lakes.txt | sort > sorted-lakes.txt The command takes the standard output from cat lakes.txt and \u2018pipes\u2019 it to sort in ascending order. The standard output of sort is redirected to sorted-lakes.txt . You can view the output data by typing: 1 $ cat sorted-lakes.txt Type : 1 $ touch deserts.txt Fill the file with values. Type : 1 2 $ cat deserts.txt $ uniq deserts.txt You get to see all entries and unique entries. Type : 1 $ sort deserts.txt | uniq You get to see unique entries sorted. Type : 1 $ sort deserts.txt | uniq > uniq-deserts.txt You save the result in uniq-deserts.txt . Use cat to output the contents of uniq-deserts.txt . uniq stands for \u2018unique\u2019 and filters out ADJACENT, duplicate lines in a file. Here uniq deserts.txt filters out duplicates of \u2018Sahara Desert\u2019, because the duplicate of \u2018Sahara Desert\u2019 directly follows the previous instance. The \u2018Kalahari Desert\u2019 duplicates are not adjacent, and thus remain. Type : 1 $ grep Mount mountains.txt grep stands for \u2018global regular expression print\u2019. It searches files for lines that match a pattern and returns the results. It is also case sensitive. Here, grep searches for \u2018Mount\u2019 in mountains.txt . Type: 1 $ grep -i Mount mountains.txt grep -i enables the command to be case insensitive. Here, grep searches for capital or lowercase strings that match \u2018Mount\u2019 in mountains.txt . The above commands are a great way to get started with grep . If you are familiar with regular expressions, you can use regular expressions to search for patterns in files. grep can also be used to search within a directory. Type : 1 $ grep -R Arctic /home/ccuser/workspace/geography ype : 1 $ grep -R Arctic /home/ccuser/workspace/geography grep -R searches files in a directory and outputs filenames and lines containing matched results. -R stands for \u2018recursive\u2019. Here grep -R searches the /home/ccuser/workspace/geography directory for the string \u2018Arctic\u2019 and outputs filenames and lines with matched results. Type: 1 $ grep -R Gambino . grep -R searches ALL files recursively and outputs filenames and lines containing matched results. 1 $ grep -Rl Arctic /home/ccuser/workspace/geography grep -Rl searches all files in a directory and outputs only filenames with matched results. -R stands for \u2018recursive\u2019 and l stands for \u2018files with matches\u2019. Here grep -Rl searches the /home/ccuser/workspace/geography directory for the string \u2018Arctic\u2019 and outputs filenames with matched results. Use cat to display the contents of forests.txt . grep this file.txt ; search for this in file.txt, print the output on screen. grep this < file.txt ; feed file.txt to process grep to look for this , print on screen grep this file.txt > file_this.txt ; write, overwrite the output in file_this.txt . grep this file.txt >> file_this.txt ; write, append the output in file_this.txt . grep \"is\" file.txt ; search pattern is , precisely, in words or alone. grep line file.txt ; search pattern line . grep -n line file.txt ; show the line number where it finds pattern line . grep -i line file.txt ; case insensitive grep -v line file.txt ; inverse or when it does not have the pattern line If you search for one \u201cfile\u201d in the current directory, type: 1 find . _name \"file.txt\" find / _name \"file.txt\" searches \u201cfile\u201d in multiple directories. find dirA dirB dirC _name \"file.txt\" search \u201cfile\u201d in one or more directories. Type : 1 $ sed 's/snow/rain/' forests.txt sed stands for \u2018stream editor\u2019. It accepts standard input and modifies it based on an expression, before displaying it as output data. It is similar to \u2018find and replace\u2019. Let\u2019s look at the expression 's/snow/rain/' : s : stands for \u2018substitution\u2019. it is always used when using sed for substitution. snow : the search string, the text to find. rain : the replacement string, the text to add in place. In this case, sed searches forests.txt for the word 'snow' and replaces it with 'rain' . Importantly, the above command will only replace the FIRST instance of 'snow' on a line. 1 $ sed 's/snow/rain/g' forests.txt The above command uses the g expression, meaning \u2018global\u2019. Here sed searches forests.txt for the word 'snow' and replaces it with 'rain' , globally. ALL instances of 'snow' on a line will be turned to 'rain' . Let\u2019s summarize what we\u2019ve done so far. The common redirection commands are: > ; redirects standard output of a command to a file, overwriting previous content. >> ; redirects standard output of a command to a file, appending new content to old content. < ; redirects standard input to a command. | ; redirects standard output of a command to another command. A number of other commands are powerful when combined with redirection commands: echo 'a' ; display \u2018a\u2019 in the terminal. touch ; create a file. cat ; display the content of a file in the terminal. less ; edit the content of a file in the terminal; prefer nano , it\u2019s more user-friendly. sort ; sorts lines in a file in alphabetical order. uniq ; filters duplicates in a file. grep ; searches for a text pattern in files and outputs it. find ; searches for files, not the content. sed ; searches for a text pattern in files, modifies it, and outputs it. head / tail ; ls , but only the top/bottom results. Examples echo 'thisthat' ; show thisthat . echo $USER ; show user variable. echo \\* , d\\* , s\\* , [[:upper]]\\* , /usr/\\*/share ; show the content of the current directory according to the specified string (wildcards and characters). echo ~ ; show /home/user . echo .\\* ; show hidden files. echo 2 + 2 ; show 2 + 2. echo $((2 + 2)) ; show 4. echo $(($((5 \\*\\* 2)) \\* 3)) ; show 75. echo Front-{A,B,C}-Back ; show Front-A-Back Front-B-Back Front-C-Back. echo Number-{1...5} ; show Number-1 Number-2 Number-3 Number-4 Number-5. echo {Z...A} ; show Z Y X W\u2026 echo a{A{1,2}, B{3,4}}b ; show aS1b aA2b aB3b aB4b. mkdir {2007...2009}-0{1...9} {2007...2009}-{10...12} ; show 2007-01 2007-02 2007-03 2007-04 2007-05 2007-06 2007-07 2007-08 2007-09 2007-10 2007-11 2007-12 2008-01\u2026 echo $(ls) ; show, not a list, but a paragraph of files and directories. echo $USER $((2 + 2)) ; show user 4. echo $(cal) or echo ' $(cal) ' ; show a calendar. With . , special characters become ordinary characters except for $ , \\ or ' ; and with . , it doesn\u2019t suppress commands, variables and aliases as with . . Try : echo text ~/\\*.txt {a,b} $(echo foo) $((2 + 2)) . echo 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))' . echo 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))' . print env | less ; show a list of available variables. cat test.txt ; showthe content of file text.txt . ls > file.txt ; send command and results into a file, if the file exists, it overwrite it, use cat file.txt to edit it. ls >> file.txt ; send command and results into a file, if the file exists, it appends the new content to the existing one, use cat file.txt to edit it. cat test.txt > bbb.txt ; write (overwrite) the (existing) content of test.txt into bbb.txt . cat test.txt >> bbb.txt ; write the content of test.txt into bbb.txt or append it to bbb.txt . wc test.txt ; count lines, words and bytes of test.txt . cat test.txt | wc ; show the wc of test.txt . cat test.txt | wc > bbb.txt ; write the wc of test.txt into bbb.txt . sort < file.txt ; push the content into command sort. cat < aaa.txt ; open file aaa.txt to edit. sort file.txt ; sort the content. cat aaa.txt | sort > sorted_aaa.txt ; in addition, send the results in a new files. uniq file.txt ; extract unique values of the content. ls -l | less ; the list goes into the reader. ls -l | head ; the list shows the 10 top lines only. ls -l | tail ; the list shows the 10 bottom lines only.","title":"Redirection"},{"location":"Codecademy Learn the Command Line Notes/#environment","text":"Each time we launch the terminal application, it creates a new session. The session immediately loads settings and preferences that make up the command line environment. We can configure this environment to support the commands and programs, customize greetings and command aliases, and create variables to share across commands and programs. A simple, command line text editor: nano . It is more powerful than less . Type : 1 $ nano hello.txt In nano , at the top of the window, type : 1 $ 'Hello, I am nano.' Using the menu at the bottom of the terminal for reference, type Ctrl + O (the letter, not the number) to save the file. Press Enter , when prompted about the filename to write. Then type Ctrl + X to exit nano . Finally, type clear to clear the terminal window. The command prompt should now be at the top of the window. You just edited a file in the nano text editor. Type: 1 $ nano hello.txt nano is a command line text editor. It works just like a desktop text editor, except that it is accessible from the command line and only accepts keyboard input. The command nano hello.txt opens a new text file named hello.txt in the nano text editor. \u2018Hello, I am nano\u2019 is a text string entered in nano through the cursor. The menu of keyboard commands at the bottom of the window allow us to save changes to hello.txt and exit nano . The ^ stands for the Ctrl key. Ctrl + O saves a file. o stands for output. Ctrl + X exits the nano program. x stands for exit. Ctrl + G opens a help menu. Clear clears the terminal window, moving the command prompt to the top of the screen. nano editor Now that you are familiar with editing text in nano , let\u2019s create a file to store environment settings. Type : 1 $ nano ~/.bash_profile This opens up a new file in nano . In ~/.bash_profile , at the top of the file, type : 1 $ echo 'Welcome, Jane Doe' You can use your name in place of \u2018Jane Doe\u2019. Type Ctrl + O to save the file. Press Enter to write the filename. Type Ctrl + X to exit. Finally, type clear to clear the terminal window. Type : 1 $ source ~/.bash_profile You should see the greeting you entered. You created a file in nano called ~/.bash_profile and added a greeting. 1 $ nano ~/.bash_profile ~/.bash_profile is the name of file used to store environment settings. It is commonly called the \u2018bash profile\u2019. When a session starts, it will load the contents of the bash profile before executing commands. The ~ represents the user\u2019s home directory. The . indicates a hidden file. The name ~/.bash_profile is important, since this is how the command line recognizes the bash profile. The command nano ~/.bash_profile opens up ~/.bash_profile in nano . The text echoes \u2018Welcome, Jane Doe\u2019 and creates a greeting in the bash profile, which is saved. It tells the command line to echo the string \u2018Welcome, Jane Doe\u2019 when a terminal session begins. The command source ~/.bash_profile ACTIVATES the changes in ~/.bash_profile for the current session. Now that we know what bash profile is, let\u2019s continue configuring the environment by adding command aliases. Open ~/.bash_profile in nano . In ~/.bash_profile , beneath the greeting you created, type : 1 $ alias pd = 'pwd' Save the file. Press Enter to write the filename. Exit nano . Clear the terminal window. In the command line, use the source command to activate the changes in the current session. 1 $ source ~/.bash_profile Let\u2019s try out the alias. Type : 1 $ pd You should see the same output as you would by typing the pwd command. What happens when you store this alias in ~/.bash_profile ? 1 $ alias pd = 'pwd' The alias command allows you to create keyboard shortcuts, or aliases, for commonly used commands. Here alias pd='pwd' creates the alias pd for the pwd command, which is then saved in the bash profile. Each time you enter pd , the output will be the same as the pwd command. The command source ~/.bash_profile makes the alias pd available in the current session. Each time we open up the terminal, we can use the pd alias. Let\u2019s practice aliases some more. Open ~/.bash_profile in nano . In the bash profile, beneath the previous alias, add : 1 $ alias hy = 'history' Save the file. Press Enter to write the filename. Add another alias: 1 $ alias ll = 'ls -la' Save the file. Press Enter to write the filename. Exit nano . Clear the terminal window. In the command line, use source to activate the changes to the bash profile for the current session. Let\u2019s try out the aliases. Type: 1 $ hy What happens when you store the following aliases in ~/.bash_profile ? 1 $ alias hy = 'history' hy is set as alias for the history command in the bash profile. The alias is then made available in the current session through source. By typing hy , the command line outputs a history of commands that were entered in the current session. Type: 1 $ alias ll = 'ls -la' ll is set as an alias for ls -la and made available in the current session through source. By typing ll , the command line now outputs all contents and directories in long format, including all hidden files. Now that you are familiar with configuring greetings and aliases, let\u2019s move on to setting environment variables. Open ~/.bash_profile in nano . In the bash profile, beneath the aliases, on a new line, type: 1 $ export USER = 'Jane Doe' Feel free to use your own name. Save the file. Press Enter to write the filename. Exit nano . Finally, clear the terminal. In the command line, use source to activate the changes in the bash profile for the current session. Type : 1 $ echo $USER This should return the value of the variable that you set. What happens when you store this in ~/.bash_profile ? 1 $ export USER = 'Jane Doe' Environment variables are variables that can be used across commands and programs and hold information about the environment. The line USER='Jane Doe' sets the environment variable USER to a name 'Jane Doe' . Usually the USER variable is set to the name of the computer\u2019s owner. The line export makes the variable to be available to all child sessions initiated from the session you are in. This is a way to make the variable persist across programs. At the command line, the command echo $USER returns the value of the variable. Note that $ is always used when returning a variable\u2019s value. Here, the command echo $USER returns the name set for the variable. Let\u2019s learn a few more environment variables, starting with the variable for the command prompt. Open ~/.bash_profile in nano . On a new line, beneath the last entry, type 1 $ export PS1 = '>> ' Save the file. Press Enter to write the filename. Exit nano . Finally, clear the terminal window. In the command line, use source to activate the changes in the bash profile for the current shell session. Let\u2019s try out the new command prompt. Type : 1 $ echo 'hello' Type : 1 $ ls -alt Did you notice that the prompt has changed? What happens when this is stored in ~/.bash_profile ? 1 $ export PS1 = '>> ' PS1 is a variable that defines the makeup and style of the command prompt. echo $PS1 prints the variable. PS1=\"value\" changes the variable value. PS1='>> ' sets the command prompt variable and exports the variable. It makes the variable available in all sub programs of the current shell. Here we change the default command prompt from $ to >> . After using the source command, the command line displays the new command prompt. Let\u2019s learn about two more environment variables. Type : 1 $ echo $HOME This returns the value of the HOME variable. What happens when you type this command? 1 $ echo $HOME The HOME variable is an environment variable that displays the path of the home directory. Here by typing echo $HOME , the terminal displays the path /home/ccuser as output. cd $HOME goes to the home directory. You can customize the HOME variable if needed, but in most cases this is not necessary. In the command line, type : 1 $ echo $PATH Type : 1 /bin/pwd Type : 1 /bin/ls What happens when you type this command? 1 2 $ echo $PATH /home/ccuser/.gem/ruby/2.0.0/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/sbin:/sbin:/bin PATH is an environment variable that stores a list of directories separated by a colon. Looking carefully, echo $PATH lists the following directories: /home/ccuser/.gem/ruby/2.0.0/bin . /usr/local/sbin . /usr/local/bin . /usr/bin . /usr/sbin . /sbin . /bin . Each directory contains scripts for the command line to execute. The PATH variable simply lists which directories contain scripts. For example, many COMMANDS we\u2019ve learned are scripts stored in the /bin directory. 1 $ /bin/pwd This is the script that is executed when you type the pwd command. 1 $ /bin/ls This is the script that is executed when you type the ls command. In advanced cases, you can customize the PATH variable when adding scripts of your own. Type : 1 $ env Type : 1 $ env | grep PATH What happens when you type this command? 1 $ env The env command stands for \u2018environment\u2019, and returns a list of the environment variables for the current user. Here, the env command returns a number of variables, including PATH , PWD , PS1 , and HOME . 1 2 $ env | grep PATH $ env | grep aliasname env | grep PATH is a command that displays the value of a single environment variable. Here the standard output of env is \u2018piped\u2019 to the grep command. grep searches for the value of the variable PATH and outputs it to the terminal. You learned to use the bash profile to configure the environment. What can we generalize so far? The environment refers to the preferences and settings of the current user. Let\u2019s summarize what we\u2019ve done so far. The nano editor is a command line text editor used to configure the environment. ~/.bash_profile is where environment settings are stored. You can edit this file with nano . Environment variables are variables that can be used across commands and programs and hold information about the environment. export VARIABLE='Value' sets and exports an environment variable. USER is the name of the current user. PS1 is the command prompt. HOME is the home directory. It is usually not customized. PATH returns a colon separated list of file paths. It is customized in advanced cases. echo $PATH prints the path. PATH=\"value\" changes the path.. export PATH=/home/dir/bin:$PATH appends the new path to environment variable PATH. env returns a list of environment variables.","title":"Environment"},{"location":"Codecademy Learn the Command Line Notes/#multi-users","text":"Linux is multi-user : multiple users at the same time as opposed to OS X or Windows; just like the UNIX mainframe computers with terminals, users and superuser concepts. Access rights Type : 1 $ ls -l Read, from left to right : \u2013 or d : file or dir. File or dir name. Owner access ( r w x\u2026- ). Group access ( r w x\u2026- ). All access ( r w x\u2026- ). Owner group, size, date. How do we change the acces rights? With chmod . First, there are access right for: u ser. g roup. o thers. Second, there levels. Each level has a numeric value.: r ead: 4. w rite: 2. x ecute: 1. For example, using levels: o+w : others can write the file (create). u+x : users can execute the file. g-x : group can no longer execute the file. etc. chmod o+w file1.txt for example. Why numeric value? It an alternative way for chmod to assign access rights. Levels are ranked with values: Level Binary Decimal r w x 111 7 r w - 110 6 r - - 100 4 - - - 000 0 r - x 101 5 A 7 grants full rights vs a 0 that grants no rights. For example: chmod 600 file ; change the file access rights to rw- --- --- . chmod 600 dir ; change the directory access rights. How are the values calculated? 1 2 3 4 5 6 r+w+x = 4 +2+1 = 7 r = 4 = 4 x = 1 = 1 r+w = 4 +2 = 6 ... ... Once you have the values, you can set the access rights: 1 2 3 4 u g o --- --- --- rwx rw- --x 7 6 1 Therefore, chmod 761 set the access rights rwx rw- --x to a file, files, a directory or directories. Ownership chown thou file ; assign a new owner, thou , to \u2018file\u2019. chown thou dir ; \u2026 to dir. chgrp newgr file assign a new group owner, newgr , to \u2018file\u2019. chgrp newgr dir ; \u2026 to dir. Superuser su ; superuser login. su file ; unlock the file with the superuser password. sudo ; do it with the privilege of a superuser. sudo apt-get update ; updates the database. sudo apt-get upgrade ; upgrades all packages (update before upgrading). sudo apt-get install build-essential ; install a useful package (or any other package). sudo apt-get install git ; install \u2018git\u2019. which git ; find where \u2018git\u2019 is located (if it\u2019s instaled). sudo apt-get remove git ; remove \u2018git\u2019. sudo apt-get purge git ; remove \u2018git\u2019 and purge any remainings. sudo chnow user file ; change the owner.","title":"Multi-Users"},{"location":"Codecademy Learn the Command Line Notes/#multi-tasks","text":"Linux is multi-task; multiple tasks can run at the same time like OS X or Windows or UNIX; Linux kernel runs processes; they take turns at the processor(s). All programs and processes can be launched/killed from the GUI and the CLI. Show top ; show process dashboard by PID number; ? for help, q for quit. ps ; show the process list. ps aux ; show all process. ps aux | grep 'top' ; filter top processes. ps aux | grep bash ; filter processes related to the bash. ps aux | grep bash | sort ; \u2026 and sort them. Manage Ctrl + Z ; pause a process, put it in the background. fg ; foreground, bring back the process. jobs ; list paused processes. fg #PID ; bring back process #PID (if there is more than one process on pause, you must identify the process). Ctrl + C ; terminate the active process. xload ; display the system load in a new windows, but jam the current terminal (open another or several terminals then). xload & ; runs in the background Ctrl + X `; suspend the process and unjam the terminal. by ; resume the process. ps ; show processes and their #PID. ps x | grep bad_program ; find the bad processes. kill can send a signal from the OS to a process. For example : when you log off, you send a signal to terminate a word processing program and save the file before closing. kill #PID ; kill the process. kill -STOP #PID ; pause the process. kill -TERM #PID ; terminate the process. kill -SIGTERM #PID ; terminate the process. kill -SIGKILL #PID ; kill the process. kill -KILL ; force closing of the current process. kill -9 #PID ; force closing the process. Killing sequence: kill #PID ; doesn\u2019t work\u2026 kill -9 #PID , or..","title":"Multi-Tasks"},{"location":"Codecademy SQL, Analyzing Business Metrics/","text":"Foreword Code snippets. With SQLite. From Codecademy in collaboration with Periscope Data . Specifying Comments \u00b6 Line comment. This is indicated by two negative signs. The remainder of the text on the line is the comment. Block comment. The start of the block comment is indicated by /* , the end of the comment by the same string. A block comment can cover text in part of a line, or can span multiple lines. Rem or @ . For Oracle, a line starting with either REM or @ is a comment line. Avanced Aggregates \u00b6 At the heart of every great business decision is data. Since most businesses store critical data in SQL databases, a deep understanding of SQL is a necessary skill for every data analyst. Chief among data analysis tasks is data aggregation, the grouping of data to express in summary form. We\u2019ll be working with SpeedySpoon, a meal delivery app. The folks at SpeedySpoon have asked us to look into their deliveries and help them optimize their process. This course was developed in partnership with our good friends at Periscope Data. If you\u2019re new to SQL, we recommend you do this course first. Complete each query by replacing the comments /**/ with SQL code. We\u2019ll start by looking at SpeedySpoon\u2019s data. The orders table has a row for each order of a SpeedySpoon delivery. It says when the order took place, and who ordered it. Add code to the select statement to select all columns in the orders table. 1 2 3 4 select * from /**/ order by id limit 100 ; Note that the order and limit clauses keep the data organized. 1 SELECT * FROM orders ORDER BY id LIMIT 100 ; The order_items table lists the individual foods and their prices in each order. Complete the query to select all columns in the order_items table. 1 SELECT * FROM order_items ORDER BY id LIMIT 100 ; Now that we have a good handle on our data, let\u2019s dive into some common business queries. We\u2019ll begin with the Daily Count of orders placed. To make our Daily Count metric, we\u2019ll focus on the date function and the ordered_at field. To get the Daily Metrics we need the date. Most dates in databases are timestamps, which have hours and minutes, as well as the year, month, and day. Timestamps look like 2015-01-05 14:43:31 , while dates are the first part: 2015-01-05 . We can easily select the date an item was ordered at with the date function and the ordered_at field: 1 2 SELECT date ( ordered_at ) FROM orders ; Let\u2019s get a Daily Count of orders from the orders table. Complete the query using the date function to cast the timestamps in ordered_at to dates. 1 2 3 4 \u200b SELECT date ( ordered_at ) FROM orders ORDER BY 1 LIMIT 100 ; The order by 1 statement is a shortcut for order by date(ordered_at). The 1 refers to the first column. Now that we can convert timestamps to dates, we can count the Orders Per Date. Generally, when we count all records in a table we run a query with the count function, as follows: 1 2 SELECT count ( 1 ) FROM users ; This will treat all rows as a single group, and return one row in the result set - the total count. To count orders by their dates, we\u2019ll use the date and count functions and pair them with the group by clause. Together this will count the records in the table, grouped by date. For example, to see the user records counted by the date created, we use the date and count functions and group by clause as follows: 1 2 3 SELECT date ( created_at ), count ( 1 ) FROM users GROUP BY date ( created_at ) Use the date and count functions and group by clause to count and group the orders by the dates they were ordered_at. 1 2 3 4 SELECT date ( ordered_at ), count ( 1 ) FROM orders GROUP BY 1 ORDER BY 1 ; Group before ordering We have the Daily Count of orders, but what we really want to know is revenue. How much money has SpeedySpoon made from orders each day? We can make a few changes to our Daily Count query to get the revenue. First, instead of using count(1) to count the rows per date, we\u2019ll use round(sum(amount_paid), 2) to add up the revenue per date. Complete the query by adding revenue per date. Second, we need to join in the order_items table because that table has an amount_paid column representing revenue. Complete the query by adding a join clause where orders.id = order_items.order_id. Note that the round function rounds decimals to digits, based on the number passed in. Here round(\u2026, 2) rounds the sum paid to two digits. 1 2 3 4 5 SELECT date ( ordered_at ), round ( sum ( amount_paid ), 2 ) FROM orders JOIN order_items ON orders . id = order_items . order_id GROUP BY 1 ORDER BY 1 ; Now with a small change, we can find out how much we\u2019re making per day for any single dish. What\u2019s the daily revenue from customers ordering kale smoothies? Complete the query by using a where clause to filter the daily sums down to orders where the name = \u2018kale-smoothie\u2019. 1 2 3 4 5 6 SELECT date ( ordered_at ), round ( sum ( amount_paid ), 2 ) FROM orders JOIN order_items ON orders . id = order_items . order_id WHERE name = 'kale-smoothie' GROUP BY 1 ORDER BY 1 ; It looks like the smoothies might not be performing well, but to be sure we need to see how they\u2019re doing in the context of the other order items. We\u2019ll look at the data several different ways, the first of which is determining what percent of revenue these smoothies represent. To get the percent of revenue that each item represents, we need to know the total revenue of each item. We will later divide the per-item total with the overall total revenue. The following query groups and sum the products by price to get the total revenue for each item. Complete the query by passing in sum(amount_paid) into the round function and rounding to two decimal places. 1 2 3 4 SELECT name , round ( sum ( amount_paid ), 2 ) FROM order_items GROUP BY name ORDER BY 2 DESC ; We have the sum of the the products by revenue, but we still need the percent. For that, we\u2019ll need to get the total using a subquery. A subquery can perform complicated calculations and create filtered or aggregate tables on the fly. Subqueries are useful when you want to perform an aggregation outside the context of the current query. This will let us calculate the overall total and per-item total at the same time. Complete the denominator in the subquery, which is the total revenue from order_items. Use the sum function to query the amount_paid from the order_items table. We now have the percent or revenue each product represents! 1 2 3 4 5 SELECT name , round ( sum ( amount_paid ) / ( SELECT sum ( amount_paid ) FROM order_items ) * 100 . 0 , 2 ) FROM order_items GROUP BY 1 ORDER BY 2 DESC ; Here order by 2 desc sorts by the second column (the percent) to show the products in order of their contribution to revenue. As we suspected, kale smoothies are not bringing in the money. And thanks to this analysis, we found what might be a trend - several of the other low performing products are also smoothies. Let\u2019s keep digging to find out what\u2019s going on with these smoothies. To see if our smoothie suspicion has merit, let\u2019s look at purchases by category. We can group the order items by what type of food they are, and go from there. Since our order_items table does not include categories already, we\u2019ll need to make some! Previously we\u2019ve been using group by with a column (like order_items.name) or a function (like date(orders.ordered_at)). We can also use group by with expressions. In this case a case statement is just what we need to build our own categories. case statements are similar to if/else in other languages. Here\u2019s the basic structure of a case statement: 1 2 3 4 5 CASE { condition } WHEN { value1 } THEN { result1 } WHEN { value2 } THEN { result2 } ELSE { result3 } END We\u2019ll build our own categories using a case statement. Complete the query below with a case condition of name that lists out each product, and decides its group. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SELECT * , CASE name WHEN 'kale-smoothie' THEN 'smoothie' WHEN 'banana-smoothie' THEN 'smoothie' WHEN 'orange-juice' THEN 'drink' WHEN 'soda' THEN 'drink' WHEN 'blt' THEN 'sandwich' WHEN 'grilled-cheese' THEN 'sandwich' WHEN 'tikka-masala' THEN 'dinner' WHEN 'chicken-parm' THEN 'dinner' ELSE 'other' END AS category FROM order_items ORDER BY id LIMIT 100 ; Complete the query by using the category column created by the case statement in our previous revenue percent calculation. Add the denominator that will sum the amount_paid. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SELECT CASE name WHEN 'kale-smoothie' THEN 'smoothie' WHEN 'banana-smoothie' THEN 'smoothie' WHEN 'orange-juice' THEN 'drink' WHEN 'soda' THEN 'drink' WHEN 'blt' THEN 'sandwich' WHEN 'grilled-cheese' THEN 'sandwich' WHEN 'tikka-masala' THEN 'dinner' WHEN 'chicken-parm' THEN 'dinner' ELSE 'other' END AS category , round ( 1 . 0 * sum ( amount_paid ) / ( SELECT sum ( amount_paid ) FROM order_items ) * 100 , 2 ) AS pct FROM order_items GROUP BY 1 ORDER BY 2 DESC ; Here 1.0 * is a shortcut to ensure the database represents the percent as a decimal. It\u2019s true that the whole smoothie category is performing poorly compared to the others. We\u2019ll certainly take this discovery to SpeedySpoon. Before we do, let\u2019s go one level deeper and figure out why. While we do know that kale smoothies (and drinks overall) are not driving a lot of revenue, we don\u2019t know why. A big part of data analysis is implementing your own metrics to get information out of the piles of data in your database. In our case, the reason could be that no one likes kale, but it could be something else entirely. To find out, we\u2019ll create a metric called reorder rate and see how that compares to the other products at SpeedySpoon. We\u2019ll define reorder rate as the ratio of the total number of orders to the number of people making those orders. A lower ratio means most of the orders are reorders. A higher ratio means more of the orders are first purchases. Let\u2019s calculate the reorder ratio for all of SpeedySpoon\u2019s products and take a look at the results. Counting the total orders per product is straightforward. We count the distinct order_ids (different) in the order_items table. Complete the query by passing in the distinct keyword and the order_id column name into the count function Here\u2019s a hint on how to use the count function to count distinct columns in a table. 1 2 3 4 5 6 7 SELECT DISTINCT column_name FROM table_name ; SELECT column_name FROM table_name GROUP BY column_name ; SELECT name , count ( DISTINCT order_id ) FROM order_items GROUP BY 1 ORDER BY 1 ; Now we need the number of people making these orders. To get that information, we need to join in the orders table and count unique values in the delivered_to field, and sort by the reorder_rate. Complete the query below. The numerator should count the distinct order_ids. The denominator should count the distinct values of the orders table\u2019s delivered_to field (orders.delivered_to). 1 2 3 4 5 6 7 8 SELECT name , round ( 1 . 0 * count ( DISTINCT order_id ) / count ( DISTINCT delivered_to ), 2 ) AS reorder_rate FROM order_items JOIN orders ON orders . id = order_items . order_id GROUP BY 1 ORDER BY 2 DESC ; That\u2019s unexpected. While smoothies aren\u2019t making a lot of money for SpeedySpoon, they have a very high reorder rate. That means these smoothie customers are strong repeat customers. Instead of recommending smoothies be taken off the menu, we should talk to the smoothie customers and see what they like so much about these smoothies. There could be an opportunity here to expand the product line, or get other customers as excited as these kale fanatics. Nice work! Let\u2019s generalize what we\u2019ve learned so far: Data aggregation is the grouping of data in summary form. Daily Count is the count of orders in a day. Daily Revenue Count is the revenue on orders per day. Product Sum is the total revenue of a product. Subqueries can be used to perform complicated calculations and create filtered or aggregate tables on the fly. Reorder Rate is the ratio of the total number of orders to the number of people making orders. Common Metrics \u00b6 As a data scientist, when you\u2019re not investigating spikes or dips in your data, you might be building dashboards of KPIs, or key performance indicators for a company. KPIs are often displayed on TVs on the walls of the office, and serve as high level health metrics for the business. While every company\u2019s metrics are defined slightly differently, the basics are usually very similar. In this lesson we\u2019ll take a look at basic KPIs like Daily Revenue, Daily Active Users, ARPU, and Retention for a video game, Mineblocks. This company has two tables, gameplays and purchases. The purchases table lists all purchases made by players while they\u2019re playing Mineblocks. Complete the query to select from purchases. 1 SELECT * FROM purchases ORDER BY id LIMIT 10 ; The gameplays table lists the date and platform for each session a user plays. Select from gameplays. 1 SELECT * FROM gameplays ORDER BY id LIMIT 10 ; At the heart of every company is revenue, and Mineblocks is no exception. For our first KPI we\u2019ll calculate daily revenue. Daily Revenue is simply the sum of money made per day. To get close to Daily Revenue, we calculate the daily sum of the prices in the purchases table. Complete the query by using the sum function and passing the price column from the purchases table. 1 2 3 4 5 6 SELECT date ( created_at ), round ( sum ( price ), 2 ) FROM purchases GROUP BY 1 ORDER BY 1 ; Update our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null. Update our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null. Mineblocks is a game, and one of the core metrics to any game is the number people who play each day. That KPI is called Daily Active Users, or DAU. DAU is defined as the number of unique players seen in-game each day. It\u2019s important not to double count users who played multiple times, so we\u2019ll use distinct in our count function. Likewise, Weekly Active Users (WAU) and Monthly Active Users (MAU) are in the same family. For Mineblocks, we\u2019ll use the gameplays table to calculate DAU. Each time a user plays the game, their session is recorded in gameplays. Thus, a distinct count of users per day from gameplays will give us DAU. Calculate Daily Active Users for Mineblocks. Complete the query\u2019s count function by passing in the distinct keyword and the user_id column name. 1 2 3 4 5 6 SELECT date ( created_at ), COUNT ( DISTINCT user_id ) AS dau FROM gameplays GROUP BY 1 ORDER BY 1 ; Since Mineblocks is on multiple platforms, we can calculate DAU per-platform. Previously we calculated DAU only per day, so the output we wanted was [date, dau_count]. Now we want DAU per platform, making the desired output [date, platform, dau_count]. Calculate DAU for Mineblocks per-platform. Complete the query below. You will need to select the platform column and add a count function by passing in the distinct keyword and the user_id column name. 1 2 3 4 5 6 7 SELECT date ( created_at ), platform , COUNT ( DISTINCT user_id ) AS dau FROM gameplays GROUP BY 1 , 2 ORDER BY 1 , 2 ; group by 1 (date), 2 (platform) order by 1 (date), 2 (platform) We\u2019ve looked at DAU and Daily Revenue in Mineblocks. Now we must understand the purchasing habits of our users. Mineblocks, like every freemium game, has two types of users: 1 2 purchasers : users who have bought things in the game players : users who play the game but have not yet purchased The next KPI we\u2019ll look at Daily ARPPU - Average Revenue Per Purchasing User. This metric shows if the average amount of money spent by purchasers is going up over time. Daily ARPPU is defined as the sum of revenue divided by the number of purchasers per day. To get Daily ARPPU, modify the daily revenue query from earlier to divide by the number of purchasers. Complete the query by adding a numerator and a denominator. The numerator will display daily revenue, or sum the price columns. The denominator will display the number of purchasers by passing the distinct keyword and the user_id column name into the count function. 1 2 3 4 5 6 7 SELECT date ( created_at ), round ( sum ( price ) / COUNT ( DISTINCT user_id ), 2 ) AS arppu FROM purchases WHERE refunded_at IS NULL GROUP BY 1 ORDER BY 1 ; The more popular (and difficult) cousin to Daily ARPPU is Daily ARPU, Average Revenue Per User. ARPU measures the average amount of money we\u2019re getting across all players, whether or not they\u2019ve purchased. ARPPU increases if purchasers are spending more money. ARPU increases if more players are choosing to purchase, even if the purchase size stays consistent. No one metric can tell the whole story. That\u2019s why it\u2019s so helpful to have many KPIs on the same dashboard. Daily ARPU is defined as revenue divided by the number of players, per-day. To get that, we\u2019ll need to calculate the daily revenue and daily active users separately, and then join them on their dates. One way to easily create and organize temporary results in a query is with CTEs, Common Table Expressions, also known as with clauses. The with clauses make it easy to define and use results in a more organized way than subqueries. These clauses usually look like this: 1 2 3 4 5 6 WITH { subquery_name } AS ( { subquery_body } ) SELECT ... FROM { subquery_name } WHERE ... Use a with clause to define daily_revenue and then select from it. 1 2 3 4 5 6 7 8 9 daily_revenue AS ( SELECT date ( created_at ) AS dt , round ( sum ( price ), 2 ) AS rev FROM purchases WHERE refunded_at IS NULL GROUP BY 1 ) SELECT * FROM daily_revenue ORDER BY dt ; Use a with clause to define daily_revenue and then select from it. 1 2 3 4 5 6 7 8 9 WITH daily_revenue AS ( SELECT date ( created_at ) AS dt , round ( sum ( price ), 2 ) AS rev FROM purchases WHERE refunded_at IS NULL GROUP BY 1 ) SELECT * FROM daily_revenue ORDER BY dt ; Now you\u2019re familiar with using the with clause to create temporary result sets. You just built the first part of ARPU, daily_revenue. From here we can build the second half of ARPU in our with clause, daily_players, and use both together to create ARPU. Building on this CTE, we can add in DAU from earlier. Complete the query by calling the DAU query we created earlier, now aliased as daily_players: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 WITH daily_revenue AS ( SELECT date ( created_at ) AS dt , round ( sum ( price ), 2 ) AS rev FROM purchases WHERE refunded_at IS NULL GROUP BY 1 ), daily_players AS ( SELECT date ( created_at ) AS dt , COUNT ( DISTINCT user_id ) AS players FROM gameplays GROUP BY 1 ) SELECT * FROM daily_players ORDER BY dt ; Now that we have the revenue and DAU, join them on their dates and calculate daily ARPU. Complete the query by adding the keyword using in the join clause. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 WITH daily_revenue AS ( SELECT date ( created_at ) AS dt , round ( sum ( price ), 2 ) AS rev FROM purchases WHERE refunded_at IS NULL GROUP BY 1 ), daily_players AS ( SELECT date ( created_at ) AS dt , COUNT ( DISTINCT user_id ) AS players FROM gameplays GROUP BY 1 ) SELECT daily_revenue . dt , daily_revenue . rev / daily_players . players FROM daily_players JOIN daily_players USING ( dt ); In the final select statement, daily_revenue.dt represents the date, while daily_revenue.rev / daily_players.players is the daily revenue divided by the number of players that day. In full, it represents how much the company is making per player, per day. In our ARPU query, we used using instead of on in the join clause. This is a special case join. 1 FROM daily_revenue JOIN daily_players USING ( dt ); When the columns to join have the same name in both tables you can use USING instead of on. Our use of the using keyword is in this case equivalent to this clause: 1 2 3 4 FROM daily_revenue JOIN daily_players ON daily_revenue . dt = daily_players . dt ; JOIN daily_players USING ( dt ); JOIN daily_players ON daily_revenue . dt = daily_players . dt ; Now let\u2019s find out what percent of Mineblock players are returning to play the next day. This KPI is called 1 Day Retention. Retention can be defined many different ways, but we\u2019ll stick to the most basic definition. For all players on Day N, we\u2019ll consider them retained if they came back to play again on Day N+1. This will let us track whether or not Mineblocks is getting \u201cstickier\u201d over time. The stickier our game, the more days players will spend in-game. And more time in-game means more opportunities to monetize and grow our business. Before we can calculate retention we need to get our data formatted in a way where we can determine if a user returned. Currently the gameplays table is a list of when the user played, and it\u2019s not easy to see if any user came back. By using a self-join, we can make multiple gameplays available on the same row of results. This will enable us to calculate retention. The power of self-join comes from joining every row to every other row. This makes it possible to compare values from two different rows in the new result set. In our case, we\u2019ll compare rows that are one date apart from each user. To calculate retention, start from a query that selects the date(created_at) as dt and user_id columns from the gameplays table. 1 2 3 4 5 6 SELECT date ( created_at ) AS dt , user_id FROM gameplays AS g1 ORDER BY dt LIMIT 100 ; Now we\u2019ll join gameplays on itself so that we can have access to all gameplays for each player, for each of their gameplays. This is known as a self-join and will let us connect the players on Day N to the players on Day N+1. In order to join a table to itself, it must be aliased so we can access each copy separately. We aliased gameplays in the query above because in the next step, we need to join gameplays to itself so we can get a result selecting [date, user_id, user_id_if_retained]. Complete the query by using a join statement to join gameplays to itself on user_id using the aliases g1 and g2. 1 2 3 4 5 6 7 8 SELECT date ( g1 . created_at ) as dt , g1 . user_id FROM gameplays AS g1 JOIN gameplays AS g2 ON g1 . user_id = g2 . user_id ORDER BY 1 LIMIT 100 ; We don\u2019t use the using clause here because the join is about to get more complicated. Now that we have our gameplays table joined to itself, we can start to calculate retention. 1 Day Retention is defined as the number of players who returned the next day divided by the number of original players, per day. Suppose 10 players played Mineblocks on Dec 10 th . If 4 of them play on Dec 11 th , the 1 day retention for Dec 10 th is 40%. The previous query joined all rows in gameplays against all other rows for each user, making a massive result set that we don\u2019t need. We\u2019ll need to modify this query. 1 2 3 4 5 6 7 8 9 10 SELECT date ( g1 . created_at ) AS dt , g1 . user_id , g2 . user_id FROM gameplays AS g1 JOIN gameplays AS g2 ON g1 . user_id = g2 . user_id AND /**/ ORDER BY 1 LIMIT 100 ; Complete the query above such that the join clause includes a date join: 1 date ( g1 . created_at ) = date ( datetime ( g2 . created_at , '-1 day' )) This means \u201conly join rows where the date in g1 is one less than the date in g2\u201d, which makes it possible to see if users have returned! 1 2 3 4 5 6 7 8 9 10 SELECT date ( g1 . created_at ) as dt , g1 . user_id , g2 . user_id FROM gameplays AS g1 JOIN gameplays AS g2 ON g1 . user_id = g2 . user_id AND date ( g1 . created_at ) = date ( datetime ( g2 . created_at , '-1 day' )) ORDER BY 1 LIMIT 100 ; The query above won\u2019t return meaningful results because we\u2019re using an inner join. This type of join requires that the condition be met for all rows, effectively limiting our selection to only the users that have returned. Instead, we want to use a left join, this way all rows in g1 are preserved, leaving nulls in the rows from g2 where users did not return to play the next day. Change the join clause to use left join and count the distinct number of users from g1 and g2 per date. 1 2 3 4 5 6 7 8 9 10 11 SELECT date ( g1 . created_at ) as dt , count ( DISTINCT g1 . user_id ) as total_users , count ( DISTINCT g2 . user_id ) as retained_users FROM gameplays AS g1 LEFT JOIN gameplays AS g2 ON g1 . user_id = g2 . user_id AND date ( g1 . created_at ) = date ( datetime ( g2 . created_at , '-1 day' )) GROUP BY 1 ORDER BY 1 LIMIT 100 ; Now that we have retained users as count(distinct g2.user_id) and total users as count(distinct g1.user_id), divide retained users by total users to calculate 1 day retention! 1 2 3 4 5 6 7 8 9 10 SELECT date ( g1 . created_at ) as dt , round ( 100 * count ( DISTINCT g2 . user_id ) / count ( DISTINCT g1 . user_id )) AS retention FROM gameplays AS g1 LEFT JOIN gameplays AS g2 ON g1 . user_id = g2 . user_id AND date ( g1 . created_at ) = date ( datetime ( g2 . created_at , '-1 day' )) GROUP BY 1 ORDER BY 1 LIMIT 100 ; While every business has different metrics to track their success, most are based on revenue and usage. The metrics in this lesson are merely a starting point, and from here you\u2019ll be able to create and customize metrics to track whatever is most important to your company. And remember, data science is exploratory! The current set of metrics can always be improved and there\u2019s usually more to any spike or dip than what immediately meets the eye. Let\u2019s generalize what we\u2019ve learned so far: Key Performance Indicators are high level health metrics for a business. Daily Revenue is the sum of money made per day. Daily Active Users are the number of unique users seen each day. Daily Average Revenue Per Purchasing User (ARPPU) is the average amount of money spent by purchasers each day. Daily Average Revenue Per User (ARPU) is the average amount of money across all users. 1 Day Retention is defined as the number of players from Day N who came back to play again on Day N+1.","title":"Codecademy, SQL, Analyzing Business Metrics"},{"location":"Codecademy SQL, Analyzing Business Metrics/#specifying-comments","text":"Line comment. This is indicated by two negative signs. The remainder of the text on the line is the comment. Block comment. The start of the block comment is indicated by /* , the end of the comment by the same string. A block comment can cover text in part of a line, or can span multiple lines. Rem or @ . For Oracle, a line starting with either REM or @ is a comment line.","title":"Specifying Comments"},{"location":"Codecademy SQL, Analyzing Business Metrics/#avanced-aggregates","text":"At the heart of every great business decision is data. Since most businesses store critical data in SQL databases, a deep understanding of SQL is a necessary skill for every data analyst. Chief among data analysis tasks is data aggregation, the grouping of data to express in summary form. We\u2019ll be working with SpeedySpoon, a meal delivery app. The folks at SpeedySpoon have asked us to look into their deliveries and help them optimize their process. This course was developed in partnership with our good friends at Periscope Data. If you\u2019re new to SQL, we recommend you do this course first. Complete each query by replacing the comments /**/ with SQL code. We\u2019ll start by looking at SpeedySpoon\u2019s data. The orders table has a row for each order of a SpeedySpoon delivery. It says when the order took place, and who ordered it. Add code to the select statement to select all columns in the orders table. 1 2 3 4 select * from /**/ order by id limit 100 ; Note that the order and limit clauses keep the data organized. 1 SELECT * FROM orders ORDER BY id LIMIT 100 ; The order_items table lists the individual foods and their prices in each order. Complete the query to select all columns in the order_items table. 1 SELECT * FROM order_items ORDER BY id LIMIT 100 ; Now that we have a good handle on our data, let\u2019s dive into some common business queries. We\u2019ll begin with the Daily Count of orders placed. To make our Daily Count metric, we\u2019ll focus on the date function and the ordered_at field. To get the Daily Metrics we need the date. Most dates in databases are timestamps, which have hours and minutes, as well as the year, month, and day. Timestamps look like 2015-01-05 14:43:31 , while dates are the first part: 2015-01-05 . We can easily select the date an item was ordered at with the date function and the ordered_at field: 1 2 SELECT date ( ordered_at ) FROM orders ; Let\u2019s get a Daily Count of orders from the orders table. Complete the query using the date function to cast the timestamps in ordered_at to dates. 1 2 3 4 \u200b SELECT date ( ordered_at ) FROM orders ORDER BY 1 LIMIT 100 ; The order by 1 statement is a shortcut for order by date(ordered_at). The 1 refers to the first column. Now that we can convert timestamps to dates, we can count the Orders Per Date. Generally, when we count all records in a table we run a query with the count function, as follows: 1 2 SELECT count ( 1 ) FROM users ; This will treat all rows as a single group, and return one row in the result set - the total count. To count orders by their dates, we\u2019ll use the date and count functions and pair them with the group by clause. Together this will count the records in the table, grouped by date. For example, to see the user records counted by the date created, we use the date and count functions and group by clause as follows: 1 2 3 SELECT date ( created_at ), count ( 1 ) FROM users GROUP BY date ( created_at ) Use the date and count functions and group by clause to count and group the orders by the dates they were ordered_at. 1 2 3 4 SELECT date ( ordered_at ), count ( 1 ) FROM orders GROUP BY 1 ORDER BY 1 ; Group before ordering We have the Daily Count of orders, but what we really want to know is revenue. How much money has SpeedySpoon made from orders each day? We can make a few changes to our Daily Count query to get the revenue. First, instead of using count(1) to count the rows per date, we\u2019ll use round(sum(amount_paid), 2) to add up the revenue per date. Complete the query by adding revenue per date. Second, we need to join in the order_items table because that table has an amount_paid column representing revenue. Complete the query by adding a join clause where orders.id = order_items.order_id. Note that the round function rounds decimals to digits, based on the number passed in. Here round(\u2026, 2) rounds the sum paid to two digits. 1 2 3 4 5 SELECT date ( ordered_at ), round ( sum ( amount_paid ), 2 ) FROM orders JOIN order_items ON orders . id = order_items . order_id GROUP BY 1 ORDER BY 1 ; Now with a small change, we can find out how much we\u2019re making per day for any single dish. What\u2019s the daily revenue from customers ordering kale smoothies? Complete the query by using a where clause to filter the daily sums down to orders where the name = \u2018kale-smoothie\u2019. 1 2 3 4 5 6 SELECT date ( ordered_at ), round ( sum ( amount_paid ), 2 ) FROM orders JOIN order_items ON orders . id = order_items . order_id WHERE name = 'kale-smoothie' GROUP BY 1 ORDER BY 1 ; It looks like the smoothies might not be performing well, but to be sure we need to see how they\u2019re doing in the context of the other order items. We\u2019ll look at the data several different ways, the first of which is determining what percent of revenue these smoothies represent. To get the percent of revenue that each item represents, we need to know the total revenue of each item. We will later divide the per-item total with the overall total revenue. The following query groups and sum the products by price to get the total revenue for each item. Complete the query by passing in sum(amount_paid) into the round function and rounding to two decimal places. 1 2 3 4 SELECT name , round ( sum ( amount_paid ), 2 ) FROM order_items GROUP BY name ORDER BY 2 DESC ; We have the sum of the the products by revenue, but we still need the percent. For that, we\u2019ll need to get the total using a subquery. A subquery can perform complicated calculations and create filtered or aggregate tables on the fly. Subqueries are useful when you want to perform an aggregation outside the context of the current query. This will let us calculate the overall total and per-item total at the same time. Complete the denominator in the subquery, which is the total revenue from order_items. Use the sum function to query the amount_paid from the order_items table. We now have the percent or revenue each product represents! 1 2 3 4 5 SELECT name , round ( sum ( amount_paid ) / ( SELECT sum ( amount_paid ) FROM order_items ) * 100 . 0 , 2 ) FROM order_items GROUP BY 1 ORDER BY 2 DESC ; Here order by 2 desc sorts by the second column (the percent) to show the products in order of their contribution to revenue. As we suspected, kale smoothies are not bringing in the money. And thanks to this analysis, we found what might be a trend - several of the other low performing products are also smoothies. Let\u2019s keep digging to find out what\u2019s going on with these smoothies. To see if our smoothie suspicion has merit, let\u2019s look at purchases by category. We can group the order items by what type of food they are, and go from there. Since our order_items table does not include categories already, we\u2019ll need to make some! Previously we\u2019ve been using group by with a column (like order_items.name) or a function (like date(orders.ordered_at)). We can also use group by with expressions. In this case a case statement is just what we need to build our own categories. case statements are similar to if/else in other languages. Here\u2019s the basic structure of a case statement: 1 2 3 4 5 CASE { condition } WHEN { value1 } THEN { result1 } WHEN { value2 } THEN { result2 } ELSE { result3 } END We\u2019ll build our own categories using a case statement. Complete the query below with a case condition of name that lists out each product, and decides its group. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SELECT * , CASE name WHEN 'kale-smoothie' THEN 'smoothie' WHEN 'banana-smoothie' THEN 'smoothie' WHEN 'orange-juice' THEN 'drink' WHEN 'soda' THEN 'drink' WHEN 'blt' THEN 'sandwich' WHEN 'grilled-cheese' THEN 'sandwich' WHEN 'tikka-masala' THEN 'dinner' WHEN 'chicken-parm' THEN 'dinner' ELSE 'other' END AS category FROM order_items ORDER BY id LIMIT 100 ; Complete the query by using the category column created by the case statement in our previous revenue percent calculation. Add the denominator that will sum the amount_paid. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SELECT CASE name WHEN 'kale-smoothie' THEN 'smoothie' WHEN 'banana-smoothie' THEN 'smoothie' WHEN 'orange-juice' THEN 'drink' WHEN 'soda' THEN 'drink' WHEN 'blt' THEN 'sandwich' WHEN 'grilled-cheese' THEN 'sandwich' WHEN 'tikka-masala' THEN 'dinner' WHEN 'chicken-parm' THEN 'dinner' ELSE 'other' END AS category , round ( 1 . 0 * sum ( amount_paid ) / ( SELECT sum ( amount_paid ) FROM order_items ) * 100 , 2 ) AS pct FROM order_items GROUP BY 1 ORDER BY 2 DESC ; Here 1.0 * is a shortcut to ensure the database represents the percent as a decimal. It\u2019s true that the whole smoothie category is performing poorly compared to the others. We\u2019ll certainly take this discovery to SpeedySpoon. Before we do, let\u2019s go one level deeper and figure out why. While we do know that kale smoothies (and drinks overall) are not driving a lot of revenue, we don\u2019t know why. A big part of data analysis is implementing your own metrics to get information out of the piles of data in your database. In our case, the reason could be that no one likes kale, but it could be something else entirely. To find out, we\u2019ll create a metric called reorder rate and see how that compares to the other products at SpeedySpoon. We\u2019ll define reorder rate as the ratio of the total number of orders to the number of people making those orders. A lower ratio means most of the orders are reorders. A higher ratio means more of the orders are first purchases. Let\u2019s calculate the reorder ratio for all of SpeedySpoon\u2019s products and take a look at the results. Counting the total orders per product is straightforward. We count the distinct order_ids (different) in the order_items table. Complete the query by passing in the distinct keyword and the order_id column name into the count function Here\u2019s a hint on how to use the count function to count distinct columns in a table. 1 2 3 4 5 6 7 SELECT DISTINCT column_name FROM table_name ; SELECT column_name FROM table_name GROUP BY column_name ; SELECT name , count ( DISTINCT order_id ) FROM order_items GROUP BY 1 ORDER BY 1 ; Now we need the number of people making these orders. To get that information, we need to join in the orders table and count unique values in the delivered_to field, and sort by the reorder_rate. Complete the query below. The numerator should count the distinct order_ids. The denominator should count the distinct values of the orders table\u2019s delivered_to field (orders.delivered_to). 1 2 3 4 5 6 7 8 SELECT name , round ( 1 . 0 * count ( DISTINCT order_id ) / count ( DISTINCT delivered_to ), 2 ) AS reorder_rate FROM order_items JOIN orders ON orders . id = order_items . order_id GROUP BY 1 ORDER BY 2 DESC ; That\u2019s unexpected. While smoothies aren\u2019t making a lot of money for SpeedySpoon, they have a very high reorder rate. That means these smoothie customers are strong repeat customers. Instead of recommending smoothies be taken off the menu, we should talk to the smoothie customers and see what they like so much about these smoothies. There could be an opportunity here to expand the product line, or get other customers as excited as these kale fanatics. Nice work! Let\u2019s generalize what we\u2019ve learned so far: Data aggregation is the grouping of data in summary form. Daily Count is the count of orders in a day. Daily Revenue Count is the revenue on orders per day. Product Sum is the total revenue of a product. Subqueries can be used to perform complicated calculations and create filtered or aggregate tables on the fly. Reorder Rate is the ratio of the total number of orders to the number of people making orders.","title":"Avanced Aggregates"},{"location":"Codecademy SQL, Analyzing Business Metrics/#common-metrics","text":"As a data scientist, when you\u2019re not investigating spikes or dips in your data, you might be building dashboards of KPIs, or key performance indicators for a company. KPIs are often displayed on TVs on the walls of the office, and serve as high level health metrics for the business. While every company\u2019s metrics are defined slightly differently, the basics are usually very similar. In this lesson we\u2019ll take a look at basic KPIs like Daily Revenue, Daily Active Users, ARPU, and Retention for a video game, Mineblocks. This company has two tables, gameplays and purchases. The purchases table lists all purchases made by players while they\u2019re playing Mineblocks. Complete the query to select from purchases. 1 SELECT * FROM purchases ORDER BY id LIMIT 10 ; The gameplays table lists the date and platform for each session a user plays. Select from gameplays. 1 SELECT * FROM gameplays ORDER BY id LIMIT 10 ; At the heart of every company is revenue, and Mineblocks is no exception. For our first KPI we\u2019ll calculate daily revenue. Daily Revenue is simply the sum of money made per day. To get close to Daily Revenue, we calculate the daily sum of the prices in the purchases table. Complete the query by using the sum function and passing the price column from the purchases table. 1 2 3 4 5 6 SELECT date ( created_at ), round ( sum ( price ), 2 ) FROM purchases GROUP BY 1 ORDER BY 1 ; Update our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null. Update our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null. Mineblocks is a game, and one of the core metrics to any game is the number people who play each day. That KPI is called Daily Active Users, or DAU. DAU is defined as the number of unique players seen in-game each day. It\u2019s important not to double count users who played multiple times, so we\u2019ll use distinct in our count function. Likewise, Weekly Active Users (WAU) and Monthly Active Users (MAU) are in the same family. For Mineblocks, we\u2019ll use the gameplays table to calculate DAU. Each time a user plays the game, their session is recorded in gameplays. Thus, a distinct count of users per day from gameplays will give us DAU. Calculate Daily Active Users for Mineblocks. Complete the query\u2019s count function by passing in the distinct keyword and the user_id column name. 1 2 3 4 5 6 SELECT date ( created_at ), COUNT ( DISTINCT user_id ) AS dau FROM gameplays GROUP BY 1 ORDER BY 1 ; Since Mineblocks is on multiple platforms, we can calculate DAU per-platform. Previously we calculated DAU only per day, so the output we wanted was [date, dau_count]. Now we want DAU per platform, making the desired output [date, platform, dau_count]. Calculate DAU for Mineblocks per-platform. Complete the query below. You will need to select the platform column and add a count function by passing in the distinct keyword and the user_id column name. 1 2 3 4 5 6 7 SELECT date ( created_at ), platform , COUNT ( DISTINCT user_id ) AS dau FROM gameplays GROUP BY 1 , 2 ORDER BY 1 , 2 ; group by 1 (date), 2 (platform) order by 1 (date), 2 (platform) We\u2019ve looked at DAU and Daily Revenue in Mineblocks. Now we must understand the purchasing habits of our users. Mineblocks, like every freemium game, has two types of users: 1 2 purchasers : users who have bought things in the game players : users who play the game but have not yet purchased The next KPI we\u2019ll look at Daily ARPPU - Average Revenue Per Purchasing User. This metric shows if the average amount of money spent by purchasers is going up over time. Daily ARPPU is defined as the sum of revenue divided by the number of purchasers per day. To get Daily ARPPU, modify the daily revenue query from earlier to divide by the number of purchasers. Complete the query by adding a numerator and a denominator. The numerator will display daily revenue, or sum the price columns. The denominator will display the number of purchasers by passing the distinct keyword and the user_id column name into the count function. 1 2 3 4 5 6 7 SELECT date ( created_at ), round ( sum ( price ) / COUNT ( DISTINCT user_id ), 2 ) AS arppu FROM purchases WHERE refunded_at IS NULL GROUP BY 1 ORDER BY 1 ; The more popular (and difficult) cousin to Daily ARPPU is Daily ARPU, Average Revenue Per User. ARPU measures the average amount of money we\u2019re getting across all players, whether or not they\u2019ve purchased. ARPPU increases if purchasers are spending more money. ARPU increases if more players are choosing to purchase, even if the purchase size stays consistent. No one metric can tell the whole story. That\u2019s why it\u2019s so helpful to have many KPIs on the same dashboard. Daily ARPU is defined as revenue divided by the number of players, per-day. To get that, we\u2019ll need to calculate the daily revenue and daily active users separately, and then join them on their dates. One way to easily create and organize temporary results in a query is with CTEs, Common Table Expressions, also known as with clauses. The with clauses make it easy to define and use results in a more organized way than subqueries. These clauses usually look like this: 1 2 3 4 5 6 WITH { subquery_name } AS ( { subquery_body } ) SELECT ... FROM { subquery_name } WHERE ... Use a with clause to define daily_revenue and then select from it. 1 2 3 4 5 6 7 8 9 daily_revenue AS ( SELECT date ( created_at ) AS dt , round ( sum ( price ), 2 ) AS rev FROM purchases WHERE refunded_at IS NULL GROUP BY 1 ) SELECT * FROM daily_revenue ORDER BY dt ; Use a with clause to define daily_revenue and then select from it. 1 2 3 4 5 6 7 8 9 WITH daily_revenue AS ( SELECT date ( created_at ) AS dt , round ( sum ( price ), 2 ) AS rev FROM purchases WHERE refunded_at IS NULL GROUP BY 1 ) SELECT * FROM daily_revenue ORDER BY dt ; Now you\u2019re familiar with using the with clause to create temporary result sets. You just built the first part of ARPU, daily_revenue. From here we can build the second half of ARPU in our with clause, daily_players, and use both together to create ARPU. Building on this CTE, we can add in DAU from earlier. Complete the query by calling the DAU query we created earlier, now aliased as daily_players: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 WITH daily_revenue AS ( SELECT date ( created_at ) AS dt , round ( sum ( price ), 2 ) AS rev FROM purchases WHERE refunded_at IS NULL GROUP BY 1 ), daily_players AS ( SELECT date ( created_at ) AS dt , COUNT ( DISTINCT user_id ) AS players FROM gameplays GROUP BY 1 ) SELECT * FROM daily_players ORDER BY dt ; Now that we have the revenue and DAU, join them on their dates and calculate daily ARPU. Complete the query by adding the keyword using in the join clause. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 WITH daily_revenue AS ( SELECT date ( created_at ) AS dt , round ( sum ( price ), 2 ) AS rev FROM purchases WHERE refunded_at IS NULL GROUP BY 1 ), daily_players AS ( SELECT date ( created_at ) AS dt , COUNT ( DISTINCT user_id ) AS players FROM gameplays GROUP BY 1 ) SELECT daily_revenue . dt , daily_revenue . rev / daily_players . players FROM daily_players JOIN daily_players USING ( dt ); In the final select statement, daily_revenue.dt represents the date, while daily_revenue.rev / daily_players.players is the daily revenue divided by the number of players that day. In full, it represents how much the company is making per player, per day. In our ARPU query, we used using instead of on in the join clause. This is a special case join. 1 FROM daily_revenue JOIN daily_players USING ( dt ); When the columns to join have the same name in both tables you can use USING instead of on. Our use of the using keyword is in this case equivalent to this clause: 1 2 3 4 FROM daily_revenue JOIN daily_players ON daily_revenue . dt = daily_players . dt ; JOIN daily_players USING ( dt ); JOIN daily_players ON daily_revenue . dt = daily_players . dt ; Now let\u2019s find out what percent of Mineblock players are returning to play the next day. This KPI is called 1 Day Retention. Retention can be defined many different ways, but we\u2019ll stick to the most basic definition. For all players on Day N, we\u2019ll consider them retained if they came back to play again on Day N+1. This will let us track whether or not Mineblocks is getting \u201cstickier\u201d over time. The stickier our game, the more days players will spend in-game. And more time in-game means more opportunities to monetize and grow our business. Before we can calculate retention we need to get our data formatted in a way where we can determine if a user returned. Currently the gameplays table is a list of when the user played, and it\u2019s not easy to see if any user came back. By using a self-join, we can make multiple gameplays available on the same row of results. This will enable us to calculate retention. The power of self-join comes from joining every row to every other row. This makes it possible to compare values from two different rows in the new result set. In our case, we\u2019ll compare rows that are one date apart from each user. To calculate retention, start from a query that selects the date(created_at) as dt and user_id columns from the gameplays table. 1 2 3 4 5 6 SELECT date ( created_at ) AS dt , user_id FROM gameplays AS g1 ORDER BY dt LIMIT 100 ; Now we\u2019ll join gameplays on itself so that we can have access to all gameplays for each player, for each of their gameplays. This is known as a self-join and will let us connect the players on Day N to the players on Day N+1. In order to join a table to itself, it must be aliased so we can access each copy separately. We aliased gameplays in the query above because in the next step, we need to join gameplays to itself so we can get a result selecting [date, user_id, user_id_if_retained]. Complete the query by using a join statement to join gameplays to itself on user_id using the aliases g1 and g2. 1 2 3 4 5 6 7 8 SELECT date ( g1 . created_at ) as dt , g1 . user_id FROM gameplays AS g1 JOIN gameplays AS g2 ON g1 . user_id = g2 . user_id ORDER BY 1 LIMIT 100 ; We don\u2019t use the using clause here because the join is about to get more complicated. Now that we have our gameplays table joined to itself, we can start to calculate retention. 1 Day Retention is defined as the number of players who returned the next day divided by the number of original players, per day. Suppose 10 players played Mineblocks on Dec 10 th . If 4 of them play on Dec 11 th , the 1 day retention for Dec 10 th is 40%. The previous query joined all rows in gameplays against all other rows for each user, making a massive result set that we don\u2019t need. We\u2019ll need to modify this query. 1 2 3 4 5 6 7 8 9 10 SELECT date ( g1 . created_at ) AS dt , g1 . user_id , g2 . user_id FROM gameplays AS g1 JOIN gameplays AS g2 ON g1 . user_id = g2 . user_id AND /**/ ORDER BY 1 LIMIT 100 ; Complete the query above such that the join clause includes a date join: 1 date ( g1 . created_at ) = date ( datetime ( g2 . created_at , '-1 day' )) This means \u201conly join rows where the date in g1 is one less than the date in g2\u201d, which makes it possible to see if users have returned! 1 2 3 4 5 6 7 8 9 10 SELECT date ( g1 . created_at ) as dt , g1 . user_id , g2 . user_id FROM gameplays AS g1 JOIN gameplays AS g2 ON g1 . user_id = g2 . user_id AND date ( g1 . created_at ) = date ( datetime ( g2 . created_at , '-1 day' )) ORDER BY 1 LIMIT 100 ; The query above won\u2019t return meaningful results because we\u2019re using an inner join. This type of join requires that the condition be met for all rows, effectively limiting our selection to only the users that have returned. Instead, we want to use a left join, this way all rows in g1 are preserved, leaving nulls in the rows from g2 where users did not return to play the next day. Change the join clause to use left join and count the distinct number of users from g1 and g2 per date. 1 2 3 4 5 6 7 8 9 10 11 SELECT date ( g1 . created_at ) as dt , count ( DISTINCT g1 . user_id ) as total_users , count ( DISTINCT g2 . user_id ) as retained_users FROM gameplays AS g1 LEFT JOIN gameplays AS g2 ON g1 . user_id = g2 . user_id AND date ( g1 . created_at ) = date ( datetime ( g2 . created_at , '-1 day' )) GROUP BY 1 ORDER BY 1 LIMIT 100 ; Now that we have retained users as count(distinct g2.user_id) and total users as count(distinct g1.user_id), divide retained users by total users to calculate 1 day retention! 1 2 3 4 5 6 7 8 9 10 SELECT date ( g1 . created_at ) as dt , round ( 100 * count ( DISTINCT g2 . user_id ) / count ( DISTINCT g1 . user_id )) AS retention FROM gameplays AS g1 LEFT JOIN gameplays AS g2 ON g1 . user_id = g2 . user_id AND date ( g1 . created_at ) = date ( datetime ( g2 . created_at , '-1 day' )) GROUP BY 1 ORDER BY 1 LIMIT 100 ; While every business has different metrics to track their success, most are based on revenue and usage. The metrics in this lesson are merely a starting point, and from here you\u2019ll be able to create and customize metrics to track whatever is most important to your company. And remember, data science is exploratory! The current set of metrics can always be improved and there\u2019s usually more to any spike or dip than what immediately meets the eye. Let\u2019s generalize what we\u2019ve learned so far: Key Performance Indicators are high level health metrics for a business. Daily Revenue is the sum of money made per day. Daily Active Users are the number of unique users seen each day. Daily Average Revenue Per Purchasing User (ARPPU) is the average amount of money spent by purchasers each day. Daily Average Revenue Per User (ARPU) is the average amount of money across all users. 1 Day Retention is defined as the number of players from Day N who came back to play again on Day N+1.","title":"Common Metrics"},{"location":"Codecademy SQL, Table Transformation/","text":"Foreword Code snippets. From Codecademy. While working with databases, we often need to transform data from one format to achieve a desired result. In SQL, this is often called data transformation or table transformation. Subqueries \u00b6 Subqueries, sometimes referred to as inner queries or nested queries, are used to transform table data by nesting one query within another query. Two tables: airports and flights . Select ten rows from the flights table. 1 2 3 4 5 6 7 SELECT * FROM flights LIMIT 10 ; ``` sql We first create an inner query , or subquery , that finds the airports with elevation greater than 2000 from the ` airports ` table . ``` sql SELECT code FROM airports WHERE elevation > 2000 ; Next, we take the result set of the inner query and use it to filter on the flights table, to find the flight detail that meets the elevation criteria. 1 2 SELECT * FROM flights WHERE origin in ( SELECT code FROM airports WHERE elevation > 2000 ); Find flight information about flights where the origin elevation is less than 2000 feet. 1 2 SELECT * FROM flights WHERE origin in ( SELECT code FROM airports WHERE elevation < 2000 ); Non-Correlated Subqueries \u00b6 A non-correlated subquery is a subquery that can be run independently of the outer query and as we saw, can be used to complete a multi-step transformation. Perhaps we\u2019d like to look at a selection of flights whose origin airport is a seaplane base, designated by 'SEAPLANE_BASE' . The facility type of an airport is located in the fac_type field of the airports table. 1 2 SELECT * FROM flights WHERE origin in ( SELECT code FROM airports WHERE fac_type = 'SEAPLANE_BASE' ); Using the same pattern, find flight information about flights where the Federal Aviation Administration region ( faa_region ) is the Southern region ( 'ASO' ). 1 2 SELECT * FROM flights WHERE origin in ( SELECT code FROM airports WHERE faa_region = 'ASO' ); Perform transformations on a single table. For instance, sometimes we need to aggregate in multiple steps \u2013 like taking an average of a count. Imagine you\u2019d like to know how many flights there are on average, for all Fridays in a given month from the flights table. First, we\u2019d need to calculate the number of flights per day, and then we\u2019d need to calculate the average based on the daily flight count for each day of the week. We can do this all in one step using a subquery: 1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT a . dep_month , a . dep_day_of_week , AVG ( a . flight_count ) AS average_ ` flights ` FROM ( SELECT dep_month , dep_day_of_week , dep_date , COUNT ( * ) AS flight_count FROM ` flights ` GROUP BY 1 , 2 , 3 ) a GROUP BY 1 , 2 ORDER BY 1 , 2 ; The inner query provides the count of flights by day, and the outer query uses the inner query\u2019s result set to compute the average by day of week of a given month. Using a subquery, find the average total distance flown by day of week and month. Be sure to alias the outer query as average_distance and the inner query as flight_distance . 1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT a . dep_month , a . dep_day_of_week , AVG ( a . flight_distance ) AS average_distance FROM ( SELECT dep_month , dep_day_of_week , dep_date , sum ( distance ) AS flight_distance FROM ` flights ` GROUP BY 1 , 2 , 3 ) a GROUP BY 1 , 2 ORDER BY 1 , 2 ; Correlated Subqueries \u00b6 In a correlated subquery, the subquery can not be run independently of the outer query. The order of operations is important in a correlated subquery: A row is processed in the outer query. Then, for that particular row in the outer query, the subquery is executed. This means that for each row processed by the outer query, the subquery will also be processed for that row. In this example, we will find the list of all flights whose distance is above average for their carrier (query on a query, same table). 1 2 3 4 5 6 SELECT id FROM ` flights ` AS f WHERE distance > ( SELECT AVG ( distance ) FROM ` flights ` WHERE carrier = f . carrier ); In the above query the inner query has to be re-executed for each flight. Correlated subqueries may appear elsewhere besides the WHERE clause, they can also appear in the SELECT . Find the id of the flights whose distance is below average for their carrier. 1 2 3 4 5 6 SELECT id FROM flights AS f WHERE distance < ( SELECT AVG ( distance ) FROM flights WHERE carrier = f . carrier ); It would also be interesting to order flights by giving them a sequence number based on time, by carrier. For instance, assuming flight_id increments with each additional flight, we could use the following query to view flights by carrier, flight id, and sequence number: 1 2 3 4 5 6 7 SELECT carrier , id , ( SELECT COUNT ( * ) FROM flights f WHERE f . id < flights . id AND f . carrier = flights . carrier ) + 1 AS flight_sequence_number FROM flights ; Using the same pattern, write a query to view flights by origin, flight id, and sequence number. Alias the sequence number column as flight_sequence_number . 1 2 3 4 5 6 7 SELECT origin , id , ( SELECT COUNT ( * ) FROM flights f WHERE f . id < flights . id AND f . origin = flights . origin ) + 1 AS flight_sequence_number FROM flights ; What can we generalize so far? \u00b6 Subqueries are used to complete an SQL transformation by nesting one query within another query. A non-correlated subquery is a subquery that can be run independently of the outer query and can be used to complete a multi-step transformation. A correlated subquery is a subquery that cannot be run independently of the outer query. The order of operations in a correlated subquery is as follows: A row is processed in the outer query. Then, for that particular row in the outer query, the subquery is executed. Set Operations. Unions allow us to utilize information from multiple tables in our queries. In this lesson, we\u2019ll utilize data from an e-commerce store. Let\u2019s explore the available data we\u2019ll be using. Four tables: new_products , legacy_products , order_items and order_items_historic . In our database, we have products tables that contain metadata about each product in the store. Select ten rows from the new_products table. 1 SELECT * FROM new_products LIMIT 10 ; Merging Tables Together \u00b6 Sometimes, in order to answer certain questions based on data, we need to merge two tables together and then query the merged result. Perhaps we have two tables that contain information about products in an ecommerce store that we would like to combine. There are two ways of doing this: Merge the rows, called a JOIN . Merge the columns, called a UNION . UNION \u00b6 We\u2019ll focus on unions here. Union combines the result of two or more SELECT statements, using the following syntax: 1 2 3 SELECT column_name ( s ) FROM table1 UNION SELECT column_name ( s ) FROM table2 ; Each SELECT statement within the UNION must have the same number of columns with similar data types. The columns in each SELECT statement must be in the same order. By default, the UNION operator selects only distinct values. Suppose we are a growing ecommerce store and recently acquired another store to diversify our offering. The product data still exists in two separate tables: a legacy_products table and a new_products table. To get the complete list of product names from both tables, we can perform the following union. 1 2 3 SELECT item_name FROM legacy_products UNION SELECT item_name FROM new_products ; Select a complete list of brand names from the legacy_products and new_products tables. 1 2 3 SELECT brand FROM legacy_products UNION SELECT brand FROM new_products ; What if we wanted to allow duplicate values? We can do this by using the ALL keyword with UNION , with the following syntax: 1 2 3 SELECT column_name ( s ) FROM table1 UNION ALL SELECT column_name ( s ) FROM table2 ; In our ecommerce store, if we learned that we had records from historic order items in an additional table, we could use the following query to combine the tables for a complete analysis of sale price: 1 2 3 SELECT id , sale_price FROM order_items UNION ALL SELECT id , sale_price FROM order_items_historic ; Then we can perform an analysis on top of the combined result set, like finding the total count of order items. 1 2 3 4 SELECT count ( * ) FROM ( SELECT id , sale_price FROM order_items UNION ALL SELECT id , sale_price FROM order_items_historic ) as a ; Using the same pattern, utilize a subquery to find the average sale price over both order_items and order_items_historic tables. 1 2 3 4 5 SELECT id , avg ( a . sale_price ) FROM ( SELECT id , sale_price FROM order_items UNION ALL SELECT id , sale_price FROM order_items_historic ) AS a GROUP BY 1 ; Before running the top analysis, create an alias a with the preliminary results, run the avg(a.sale_price) , and group by 1 to view separate records and not a unique aggregate record!!! INTERSECT \u00b6 \u2026is used to combine two SELECT statements, but returns rows only from the first SELECT statement that are identical to a row in the second SELECT statement. This means that it returns only common rows returned by the two SELECT statements. 1 2 3 SELECT column_name ( s ) FROM table1 INTERSECT SELECT column_name ( s ) FROM table2 ; For instance, we might want to know what brands in our newly acquired store are also in our legacy store. We can do so using the following query: 1 2 3 SELECT brand FROM new_products INTERSECT SELECT brand FROM legacy_products ; Select the items in the category column that are both in the newly acquired new_products table and the legacy_products table. 1 2 3 SELECT category FROM new_products INTERSECT SELECT category FROM legacy_products ; EXCEPT \u00b6 \u2026is constructed in the same way, but returns distinct rows from the first SELECT statement that aren\u2019t output by the second SELECT statement. 1 2 3 SELECT column_name ( s ) FROM table1 EXCEPT SELECT column_name ( s ) FROM table2 ; Suppose we want to see if there are any categories that are in the new_products table that aren\u2019t in the legacy_products table. We can use an EXCEPT query to perform this analysis: 1 2 3 SELECT category FROM new_products EXCEPT SELECT category FROM legacy_products ; Conversely, select the items in the category column that are in the legacy_products table and not in the new_products table. 1 2 3 SELECT category FROM legacy_products EXCEPT SELECT category FROM new_products ; What can we generalize so far? \u00b6 The UNION clause allows us to utilize information from multiple tables in our queries. The UNION ALL clause allows us to utilize information from multiple tables in our queries, including duplicate values. INTERSECT is used to combine two SELECT statements, but returns rows only from the first SELECT statement that are identical to a row in the second SELECT statement. EXCEPT returns distinct rows from the first SELECT statement that aren\u2019t output by the second SELECT statement Conditional Aggregates \u00b6 Aggregate functions compute a single result from a set of multiple input values. You can think of aggregate data as data collected from multiple rows at a time. In this lesson, we\u2019ll continue learning about aggregate functions by focusing on conditionals, sums, and combining aggregate functions. Conditional Aggregates are aggregate functions that compute a result set based on a given set of conditions. The count function is an aggregate function, since it aggregates data from multiple rows. Count the number of rows in the flights table, representing the total number of flights contained in the table. 1 SELECT COUNT ( * ) FROM flights ; While working with databases, it\u2019s common to have empty or unknown \u201ccells\u201d in data tables. What do we do when we need to test whether a value is or is not null? We use the special keywords IS NULL or IS NOT NULL in the WHERE clause ( = NULL does not work). Count the number of rows from the flights table, where arr_time is not null and the destination is 'ATL' . 1 SELECT COUNT ( * ) FROM flights WHERE arr_time IS NOT NULL AND destination = 'ATL' ; Almost every programming language has a way to represent \u201cif, then, else\u201d, or conditional logic. In SQL, we represent this logic with the CASE statement, as follows: 1 2 3 4 5 6 7 8 9 10 SELECT CASE WHEN elevation < 500 THEN 'Low' WHEN elevation BETWEEN 500 AND 1999 THEN 'Medium' WHEN elevation >= 2000 THEN 'High' ELSE 'Unknown' END AS elevation_tier , COUNT ( * ) FROM airports GROUP BY 1 ; In the above statement, END is required to terminate the statement, but ELSE is optional. If ELSE is not included, the result will be NULL . Also notice the shorthand method of referencing columns to use in GROUP BY , so we don\u2019t have to rewrite the entire CASE Statement. Modify the case statement\u2019s such that when the elevation is less than 250, the elevation_tier column returns 'Low' , when between 250 and 1749 it returns 'Medium' , and when greater than or equal to 1750 it returns 'High' . Be sure to alias the conditional statement as elevation_tier , in your query. 1 2 3 4 5 6 7 8 9 10 SELECT CASE WHEN elevation < 250 THEN 'Low' WHEN elevation BETWEEN 250 AND 1749 THEN 'Medium' WHEN elevation >= 1750 THEN 'High' ELSE 'Unknown' END AS elevation_tier , COUNT ( * ) FROM airports GROUP BY 1 ; Sometimes you want to look at an entire result set, but want to implement conditions on certain aggregates. For instance, maybe you want to identify the total amount of airports as well as the total amount of airports with high elevation in the same result set. We can accomplish this by putting a CASE WHEN statement in the aggregate. 1 2 3 4 SELECT state , COUNT ( CASE WHEN elevation >= 2000 THEN 1 ELSE NULL END ) as count_high_elevation_aiports FROM airports GROUP BY state ; Using the same pattern, write a query to count the number of low elevation airports by state where low elevation is defined as less than 1000 ft. Be sure to alias the counted airports as count_low_elevation_airports . 1 2 3 4 SELECT state , COUNT ( CASE WHEN elevation < 1000 THEN 1 ELSE NULL END ) as count_low_elevation_aiports FROM airports GROUP BY state ; We can do that same thing for other aggregates like SUM() . For instance, if we wanted to sum the total flight distance and compare that to the sum of flight distance from a particular airline (in this case, United Airlines) by origin airport, we could run the following query: sum(distance) for all carriers ( total_flight_distance ) sum(distance) for 'UA' only (others are turned to 0) ( total_united_flight_distance ). 1 2 3 SELECT origin , sum ( distance ) as total_flight_distance , sum ( CASE WHEN carrier = 'UA' THEN distance ELSE 0 END ) as total_united_flight_distance FROM flights GROUP BY origin ; Using the same pattern, find both the total flight distance as and flight distance by origin for Delta ( carrier = 'DL' ). Alias the flight distance as total_flight_distance and the and flight distance by origin as total_delta_flight_distance . 1 2 3 SELECT origin , sum ( distance ) as total_flight_distance , sum ( CASE WHEN carrier = 'DL' THEN distance ELSE 0 END ) as total_delta_flight_distance FROM flights GROUP BY origin ; Using the same pattern, find the percentage of flights from Delta by origin ( carrier = 'DL' ): 1 2 SELECT origin , 100 . 0 * ( sum ( CASE WHEN carrier = 'DL' THEN distance ELSE 0 END ) / sum ( distance )) as percentage_flight_distance_from_delta FROM flights GROUP BY origin ; Find the percentage of high elevation airports ( elevation >= 2000 ) by state from the airports table. In the query, alias the percentage column as percentage_high_elevation_airports . (sum of '1') / count(*) and not (count of '1') / count(*) : 1 SELECT state , 100 . 0 * sum ( CASE WHEN elevation >= 2000 THEN 1 ELSE 0 END ) / count ( * ) as percentage_high_elevation_airports FROM airports GROUP BY state ; What can we generalize so far? \u00b6 Conditional Aggregates are aggregate functions the compute a result set based on a given set of conditions. NULL can be used to denote an empty field value CASE statements allow for custom classification of data CASE statements can be used inside aggregates (like SUM() and COUNT() ) to provide filtered measures Date, Number, and String Functions \u00b6 Oftentimes, data in columns of tables is not in the exact format we need to complete our desired analysis. We may need to extract a date from a full timestamp, manipulate a number, or combine first and last name columns to create a full name. In this lesson, we\u2019ll be learning about some of SQL\u2019s built-in functions for transforming dates, numbers and strings. We\u2019ll be using database of bakeries in this lesson. It is important to note that date, number, and string functions are highly database dependent. Here, we focus on built-in functions in the SQLite database management system. Select ten rows from the bakeries table: 1 SELECT * FROM bakeries LIMIT 10 ; We\u2019ll begin with dates. Dates are often written in the following format Date: \u201cYYYY-MM-DD\u201d Datetime or Timestamp: \u201cYYYY-MM-DD hh: mm:ss\u201d We can use SQL\u2019s date functions to transform data into a desired format. Since date functions can be database specific, verify the functions that exist on your relational database management system. For example, this statement: 1 SELECT DATETIME ( manufacture_time ) FROM baked_goods ; Using the datetime function, select the date and time of all deliveries in the baked_goods table using the column delivery_time . 1 SELECT DATETIME ( delivery_time ) FROM baked_goods ; Now let\u2019s assume that we have a column in our baked_goods table named manufacture_time in the format \u201cYYYY-MM-DD hh: mm:ss\u201d. We\u2019d like to know the number of baked_goods manufactured by day, and not by second. We can use the DATE() function to easily convert timestamps to dates and complete the following query: 1 2 3 SELECT DATE ( manufacture_time ), count ( * ) as count_baked_goods FROM baked_goods GROUP BY DATE ( manufacture_time ); Similarly, we can query the time with: 1 2 3 SELECT TIME ( manufacture_time ), count ( * ) as count_baked_goods FROM baked_goods GROUP BY TIME ( manufacture_time ); Find the number of baked goods by date of delivery. Be sure to alias the total count of baked goods as count_baked_goods . 1 SELECT DATE ( delivery_time ), count ( * ) as count_baked_goods FROM baked_goods GROUP BY DATE ( delivery_time ); Given a datepart and a column of date or timestamp data type, we can increment date or timestamp values by a specified interval. For example, in SQLite, the statement: 1 DATETIME ( time1 , '+3 hours' , '40 minutes' , '2 days' ); Would return a time 3 hours, 20 minutes, and 2 days after time1. Imagine that each dessert in our baked_goods table is inspected 2 hours, 30 minutes, and 1 day after the manufacture time. To derive the inspection date for each baked good, we can use the following query: 1 2 SELECT DATETIME ( manufacture_time , '+2 hours' , '30 minutes' , '1 day' ) as inspection_time FROM baked_goods ; Each of the baked goods is packaged by Baker\u2019s Market exactly five hours, twenty minutes, and two days after the delivery (designated by delivery_time). Create a query returning all the packaging times for the goods in the baked_goods table. Be sure to alias the package time column as package_time . 1 2 SELECT DATETIME ( delivery_time , '+5 hours' , '20 minutes' , '2 days' ) as package_time FROM baked_goods ; Numeric functions can be used to transform numbers. Some common SQLite mathematical functions are included below that take numeric data types as inputs: SELECT (number1 + number2); : returns the sum of two numbers. Similar, SQL can be used for subtraction, multiplication, and division. SELECT CAST(number1 AS REAL) / number3; : returns the result as a real number by casting one of the values as a real number, rather than an integer. SELECT ROUND(number, precision); : returns the numeric value rounded off to the next value specified. In our baked_goods table, we have information about cost designated by ingredients_cost . For accounting purposes, we\u2019d like to make sure that each ingredient cost is rounded to four decimal places rather than two, to account for currency fluctuations. 1 2 SELECT ROUND ( ingredients_cost , 4 ) as rounded_cost FROM baked_goods ; Find the bakery\u2019s distance from the market rounded to two decimal places. Be sure to alias the column as distance_from_market . 1 2 SELECT ROUND ( distance , 2 ) as distance_from_market FROM bakeries ; A couple more useful numeric SQL functions are included below: MAX and MIN . MAX(n1,n2,n3,...) : returns the greatest value in the set of the input numeric expressions MIN(n1,n2,n3,...) : returns the least value in the set of the input numeric expressions In our baked_goods table , in addition to the numeric ingredients_cost we have information about the packaging cost located in the packaging_cost column. We can use the MAX function to determine the overall greatest value of cost for each item using the following query: 1 2 SELECT id , MAX ( ingredients_cost , packaging_cost ) FROM baked_goods ; We also have information about cook time designated as cook_time and cool down time designated as cool_down_time in the baked_goods table. Find the greatest time value for each item in the table. 1 2 SELECT id , MAX ( cook_time , cool_down_time ) FROM baked_goods ; Find the least time value for each item in the table. 1 2 SELECT id , MIN ( cook_time , cool_down_time ) FROM baked_goods ; String manipulation can be useful to derive information from columns. We\u2019ll cover a couple of the common string functions here. A common use case for string manipulation in SQL is concatenation of strings. In SQLite, this is written as: 1 SELECT string1 || ' ' || string2 ; For example, the bakeries table contains both city and state columns. In order to create a route for these columns, we use the || function to concatenate them as in the following query: 1 2 SELECT city || ' ' || state as location FROM bakeries ; String functions are again, very database specific, and it is best practice to consult documentation before proceeding. Combine the first_name and last_name columns from the bakeries table as the full_name to identify the owners of the bakeries. Be sure to add a space between the names in the full_name as shown in the example. 1 SELECT first_name || ' ' || last_name as full_name FROM bakeries ; Another useful string function in SQL is REPLACE() : 1 REPLACE ( string , from_string , to_string ) The function returns the string string with all occurrences of the string from_string replaced by the string to_string. For example in baked_goods , there is a column named ingredients. The ingredients strings are formatted with underscores, such as baking_soda and vanilla_extract . To make these values more readable, we might like to replace the underscores with spaces. We can do so by using the following query: 1 2 SELECT id , REPLACE ( ingredients , '_' , ' ' ) as item_ingredients from baked_goods ; Any time enriched_flour appears in the ingredients list, we\u2019d like to replace it with just flour. Apply this transformation and also be sure to remove the underscore in enriched_flour . 1 2 SELECT REPLACE ( ingredients , 'enriched_' , ' ' ) as item_ingredients FROM baked_goods ; What can we generalize so far? \u00b6 Date Functions: DATETIME ; Returns the date and time of the column specified. This can be modified to return only the date or only the time. DATETIME(time1, +X hours, Y minutes, Z days) : Increments the specificed column by a given number of hours, minutes, or days. Numeric Functions: (number1 + number2); : Returns the sum of two numbers, or other mathematical operations, accordingly. CAST(number1 AS REAL) / number2; : Returns the result as a real number by casting one of numeric inputs as a real number ROUND(number, precision); : Returns the numeric value rounded off to the next value specified. String Functions: 'string1' || ' ' || 'string2'; : Concatenates string1 and string 2, with a space between. REPLACE(string,from_string,to_string) : Returns the string with all occurrences of the string from_string replaced by the string to_string .","title":"Codecademy, SQL, Table Transformation"},{"location":"Codecademy SQL, Table Transformation/#subqueries","text":"Subqueries, sometimes referred to as inner queries or nested queries, are used to transform table data by nesting one query within another query. Two tables: airports and flights . Select ten rows from the flights table. 1 2 3 4 5 6 7 SELECT * FROM flights LIMIT 10 ; ``` sql We first create an inner query , or subquery , that finds the airports with elevation greater than 2000 from the ` airports ` table . ``` sql SELECT code FROM airports WHERE elevation > 2000 ; Next, we take the result set of the inner query and use it to filter on the flights table, to find the flight detail that meets the elevation criteria. 1 2 SELECT * FROM flights WHERE origin in ( SELECT code FROM airports WHERE elevation > 2000 ); Find flight information about flights where the origin elevation is less than 2000 feet. 1 2 SELECT * FROM flights WHERE origin in ( SELECT code FROM airports WHERE elevation < 2000 );","title":"Subqueries"},{"location":"Codecademy SQL, Table Transformation/#non-correlated-subqueries","text":"A non-correlated subquery is a subquery that can be run independently of the outer query and as we saw, can be used to complete a multi-step transformation. Perhaps we\u2019d like to look at a selection of flights whose origin airport is a seaplane base, designated by 'SEAPLANE_BASE' . The facility type of an airport is located in the fac_type field of the airports table. 1 2 SELECT * FROM flights WHERE origin in ( SELECT code FROM airports WHERE fac_type = 'SEAPLANE_BASE' ); Using the same pattern, find flight information about flights where the Federal Aviation Administration region ( faa_region ) is the Southern region ( 'ASO' ). 1 2 SELECT * FROM flights WHERE origin in ( SELECT code FROM airports WHERE faa_region = 'ASO' ); Perform transformations on a single table. For instance, sometimes we need to aggregate in multiple steps \u2013 like taking an average of a count. Imagine you\u2019d like to know how many flights there are on average, for all Fridays in a given month from the flights table. First, we\u2019d need to calculate the number of flights per day, and then we\u2019d need to calculate the average based on the daily flight count for each day of the week. We can do this all in one step using a subquery: 1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT a . dep_month , a . dep_day_of_week , AVG ( a . flight_count ) AS average_ ` flights ` FROM ( SELECT dep_month , dep_day_of_week , dep_date , COUNT ( * ) AS flight_count FROM ` flights ` GROUP BY 1 , 2 , 3 ) a GROUP BY 1 , 2 ORDER BY 1 , 2 ; The inner query provides the count of flights by day, and the outer query uses the inner query\u2019s result set to compute the average by day of week of a given month. Using a subquery, find the average total distance flown by day of week and month. Be sure to alias the outer query as average_distance and the inner query as flight_distance . 1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT a . dep_month , a . dep_day_of_week , AVG ( a . flight_distance ) AS average_distance FROM ( SELECT dep_month , dep_day_of_week , dep_date , sum ( distance ) AS flight_distance FROM ` flights ` GROUP BY 1 , 2 , 3 ) a GROUP BY 1 , 2 ORDER BY 1 , 2 ;","title":"Non-Correlated Subqueries"},{"location":"Codecademy SQL, Table Transformation/#correlated-subqueries","text":"In a correlated subquery, the subquery can not be run independently of the outer query. The order of operations is important in a correlated subquery: A row is processed in the outer query. Then, for that particular row in the outer query, the subquery is executed. This means that for each row processed by the outer query, the subquery will also be processed for that row. In this example, we will find the list of all flights whose distance is above average for their carrier (query on a query, same table). 1 2 3 4 5 6 SELECT id FROM ` flights ` AS f WHERE distance > ( SELECT AVG ( distance ) FROM ` flights ` WHERE carrier = f . carrier ); In the above query the inner query has to be re-executed for each flight. Correlated subqueries may appear elsewhere besides the WHERE clause, they can also appear in the SELECT . Find the id of the flights whose distance is below average for their carrier. 1 2 3 4 5 6 SELECT id FROM flights AS f WHERE distance < ( SELECT AVG ( distance ) FROM flights WHERE carrier = f . carrier ); It would also be interesting to order flights by giving them a sequence number based on time, by carrier. For instance, assuming flight_id increments with each additional flight, we could use the following query to view flights by carrier, flight id, and sequence number: 1 2 3 4 5 6 7 SELECT carrier , id , ( SELECT COUNT ( * ) FROM flights f WHERE f . id < flights . id AND f . carrier = flights . carrier ) + 1 AS flight_sequence_number FROM flights ; Using the same pattern, write a query to view flights by origin, flight id, and sequence number. Alias the sequence number column as flight_sequence_number . 1 2 3 4 5 6 7 SELECT origin , id , ( SELECT COUNT ( * ) FROM flights f WHERE f . id < flights . id AND f . origin = flights . origin ) + 1 AS flight_sequence_number FROM flights ;","title":"Correlated Subqueries"},{"location":"Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far","text":"Subqueries are used to complete an SQL transformation by nesting one query within another query. A non-correlated subquery is a subquery that can be run independently of the outer query and can be used to complete a multi-step transformation. A correlated subquery is a subquery that cannot be run independently of the outer query. The order of operations in a correlated subquery is as follows: A row is processed in the outer query. Then, for that particular row in the outer query, the subquery is executed. Set Operations. Unions allow us to utilize information from multiple tables in our queries. In this lesson, we\u2019ll utilize data from an e-commerce store. Let\u2019s explore the available data we\u2019ll be using. Four tables: new_products , legacy_products , order_items and order_items_historic . In our database, we have products tables that contain metadata about each product in the store. Select ten rows from the new_products table. 1 SELECT * FROM new_products LIMIT 10 ;","title":"What can we generalize so far?"},{"location":"Codecademy SQL, Table Transformation/#merging-tables-together","text":"Sometimes, in order to answer certain questions based on data, we need to merge two tables together and then query the merged result. Perhaps we have two tables that contain information about products in an ecommerce store that we would like to combine. There are two ways of doing this: Merge the rows, called a JOIN . Merge the columns, called a UNION .","title":"Merging Tables Together"},{"location":"Codecademy SQL, Table Transformation/#union","text":"We\u2019ll focus on unions here. Union combines the result of two or more SELECT statements, using the following syntax: 1 2 3 SELECT column_name ( s ) FROM table1 UNION SELECT column_name ( s ) FROM table2 ; Each SELECT statement within the UNION must have the same number of columns with similar data types. The columns in each SELECT statement must be in the same order. By default, the UNION operator selects only distinct values. Suppose we are a growing ecommerce store and recently acquired another store to diversify our offering. The product data still exists in two separate tables: a legacy_products table and a new_products table. To get the complete list of product names from both tables, we can perform the following union. 1 2 3 SELECT item_name FROM legacy_products UNION SELECT item_name FROM new_products ; Select a complete list of brand names from the legacy_products and new_products tables. 1 2 3 SELECT brand FROM legacy_products UNION SELECT brand FROM new_products ; What if we wanted to allow duplicate values? We can do this by using the ALL keyword with UNION , with the following syntax: 1 2 3 SELECT column_name ( s ) FROM table1 UNION ALL SELECT column_name ( s ) FROM table2 ; In our ecommerce store, if we learned that we had records from historic order items in an additional table, we could use the following query to combine the tables for a complete analysis of sale price: 1 2 3 SELECT id , sale_price FROM order_items UNION ALL SELECT id , sale_price FROM order_items_historic ; Then we can perform an analysis on top of the combined result set, like finding the total count of order items. 1 2 3 4 SELECT count ( * ) FROM ( SELECT id , sale_price FROM order_items UNION ALL SELECT id , sale_price FROM order_items_historic ) as a ; Using the same pattern, utilize a subquery to find the average sale price over both order_items and order_items_historic tables. 1 2 3 4 5 SELECT id , avg ( a . sale_price ) FROM ( SELECT id , sale_price FROM order_items UNION ALL SELECT id , sale_price FROM order_items_historic ) AS a GROUP BY 1 ; Before running the top analysis, create an alias a with the preliminary results, run the avg(a.sale_price) , and group by 1 to view separate records and not a unique aggregate record!!!","title":"UNION"},{"location":"Codecademy SQL, Table Transformation/#intersect","text":"\u2026is used to combine two SELECT statements, but returns rows only from the first SELECT statement that are identical to a row in the second SELECT statement. This means that it returns only common rows returned by the two SELECT statements. 1 2 3 SELECT column_name ( s ) FROM table1 INTERSECT SELECT column_name ( s ) FROM table2 ; For instance, we might want to know what brands in our newly acquired store are also in our legacy store. We can do so using the following query: 1 2 3 SELECT brand FROM new_products INTERSECT SELECT brand FROM legacy_products ; Select the items in the category column that are both in the newly acquired new_products table and the legacy_products table. 1 2 3 SELECT category FROM new_products INTERSECT SELECT category FROM legacy_products ;","title":"INTERSECT"},{"location":"Codecademy SQL, Table Transformation/#except","text":"\u2026is constructed in the same way, but returns distinct rows from the first SELECT statement that aren\u2019t output by the second SELECT statement. 1 2 3 SELECT column_name ( s ) FROM table1 EXCEPT SELECT column_name ( s ) FROM table2 ; Suppose we want to see if there are any categories that are in the new_products table that aren\u2019t in the legacy_products table. We can use an EXCEPT query to perform this analysis: 1 2 3 SELECT category FROM new_products EXCEPT SELECT category FROM legacy_products ; Conversely, select the items in the category column that are in the legacy_products table and not in the new_products table. 1 2 3 SELECT category FROM legacy_products EXCEPT SELECT category FROM new_products ;","title":"EXCEPT"},{"location":"Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far_1","text":"The UNION clause allows us to utilize information from multiple tables in our queries. The UNION ALL clause allows us to utilize information from multiple tables in our queries, including duplicate values. INTERSECT is used to combine two SELECT statements, but returns rows only from the first SELECT statement that are identical to a row in the second SELECT statement. EXCEPT returns distinct rows from the first SELECT statement that aren\u2019t output by the second SELECT statement","title":"What can we generalize so far?"},{"location":"Codecademy SQL, Table Transformation/#conditional-aggregates","text":"Aggregate functions compute a single result from a set of multiple input values. You can think of aggregate data as data collected from multiple rows at a time. In this lesson, we\u2019ll continue learning about aggregate functions by focusing on conditionals, sums, and combining aggregate functions. Conditional Aggregates are aggregate functions that compute a result set based on a given set of conditions. The count function is an aggregate function, since it aggregates data from multiple rows. Count the number of rows in the flights table, representing the total number of flights contained in the table. 1 SELECT COUNT ( * ) FROM flights ; While working with databases, it\u2019s common to have empty or unknown \u201ccells\u201d in data tables. What do we do when we need to test whether a value is or is not null? We use the special keywords IS NULL or IS NOT NULL in the WHERE clause ( = NULL does not work). Count the number of rows from the flights table, where arr_time is not null and the destination is 'ATL' . 1 SELECT COUNT ( * ) FROM flights WHERE arr_time IS NOT NULL AND destination = 'ATL' ; Almost every programming language has a way to represent \u201cif, then, else\u201d, or conditional logic. In SQL, we represent this logic with the CASE statement, as follows: 1 2 3 4 5 6 7 8 9 10 SELECT CASE WHEN elevation < 500 THEN 'Low' WHEN elevation BETWEEN 500 AND 1999 THEN 'Medium' WHEN elevation >= 2000 THEN 'High' ELSE 'Unknown' END AS elevation_tier , COUNT ( * ) FROM airports GROUP BY 1 ; In the above statement, END is required to terminate the statement, but ELSE is optional. If ELSE is not included, the result will be NULL . Also notice the shorthand method of referencing columns to use in GROUP BY , so we don\u2019t have to rewrite the entire CASE Statement. Modify the case statement\u2019s such that when the elevation is less than 250, the elevation_tier column returns 'Low' , when between 250 and 1749 it returns 'Medium' , and when greater than or equal to 1750 it returns 'High' . Be sure to alias the conditional statement as elevation_tier , in your query. 1 2 3 4 5 6 7 8 9 10 SELECT CASE WHEN elevation < 250 THEN 'Low' WHEN elevation BETWEEN 250 AND 1749 THEN 'Medium' WHEN elevation >= 1750 THEN 'High' ELSE 'Unknown' END AS elevation_tier , COUNT ( * ) FROM airports GROUP BY 1 ; Sometimes you want to look at an entire result set, but want to implement conditions on certain aggregates. For instance, maybe you want to identify the total amount of airports as well as the total amount of airports with high elevation in the same result set. We can accomplish this by putting a CASE WHEN statement in the aggregate. 1 2 3 4 SELECT state , COUNT ( CASE WHEN elevation >= 2000 THEN 1 ELSE NULL END ) as count_high_elevation_aiports FROM airports GROUP BY state ; Using the same pattern, write a query to count the number of low elevation airports by state where low elevation is defined as less than 1000 ft. Be sure to alias the counted airports as count_low_elevation_airports . 1 2 3 4 SELECT state , COUNT ( CASE WHEN elevation < 1000 THEN 1 ELSE NULL END ) as count_low_elevation_aiports FROM airports GROUP BY state ; We can do that same thing for other aggregates like SUM() . For instance, if we wanted to sum the total flight distance and compare that to the sum of flight distance from a particular airline (in this case, United Airlines) by origin airport, we could run the following query: sum(distance) for all carriers ( total_flight_distance ) sum(distance) for 'UA' only (others are turned to 0) ( total_united_flight_distance ). 1 2 3 SELECT origin , sum ( distance ) as total_flight_distance , sum ( CASE WHEN carrier = 'UA' THEN distance ELSE 0 END ) as total_united_flight_distance FROM flights GROUP BY origin ; Using the same pattern, find both the total flight distance as and flight distance by origin for Delta ( carrier = 'DL' ). Alias the flight distance as total_flight_distance and the and flight distance by origin as total_delta_flight_distance . 1 2 3 SELECT origin , sum ( distance ) as total_flight_distance , sum ( CASE WHEN carrier = 'DL' THEN distance ELSE 0 END ) as total_delta_flight_distance FROM flights GROUP BY origin ; Using the same pattern, find the percentage of flights from Delta by origin ( carrier = 'DL' ): 1 2 SELECT origin , 100 . 0 * ( sum ( CASE WHEN carrier = 'DL' THEN distance ELSE 0 END ) / sum ( distance )) as percentage_flight_distance_from_delta FROM flights GROUP BY origin ; Find the percentage of high elevation airports ( elevation >= 2000 ) by state from the airports table. In the query, alias the percentage column as percentage_high_elevation_airports . (sum of '1') / count(*) and not (count of '1') / count(*) : 1 SELECT state , 100 . 0 * sum ( CASE WHEN elevation >= 2000 THEN 1 ELSE 0 END ) / count ( * ) as percentage_high_elevation_airports FROM airports GROUP BY state ;","title":"Conditional Aggregates"},{"location":"Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far_2","text":"Conditional Aggregates are aggregate functions the compute a result set based on a given set of conditions. NULL can be used to denote an empty field value CASE statements allow for custom classification of data CASE statements can be used inside aggregates (like SUM() and COUNT() ) to provide filtered measures","title":"What can we generalize so far?"},{"location":"Codecademy SQL, Table Transformation/#date-number-and-string-functions","text":"Oftentimes, data in columns of tables is not in the exact format we need to complete our desired analysis. We may need to extract a date from a full timestamp, manipulate a number, or combine first and last name columns to create a full name. In this lesson, we\u2019ll be learning about some of SQL\u2019s built-in functions for transforming dates, numbers and strings. We\u2019ll be using database of bakeries in this lesson. It is important to note that date, number, and string functions are highly database dependent. Here, we focus on built-in functions in the SQLite database management system. Select ten rows from the bakeries table: 1 SELECT * FROM bakeries LIMIT 10 ; We\u2019ll begin with dates. Dates are often written in the following format Date: \u201cYYYY-MM-DD\u201d Datetime or Timestamp: \u201cYYYY-MM-DD hh: mm:ss\u201d We can use SQL\u2019s date functions to transform data into a desired format. Since date functions can be database specific, verify the functions that exist on your relational database management system. For example, this statement: 1 SELECT DATETIME ( manufacture_time ) FROM baked_goods ; Using the datetime function, select the date and time of all deliveries in the baked_goods table using the column delivery_time . 1 SELECT DATETIME ( delivery_time ) FROM baked_goods ; Now let\u2019s assume that we have a column in our baked_goods table named manufacture_time in the format \u201cYYYY-MM-DD hh: mm:ss\u201d. We\u2019d like to know the number of baked_goods manufactured by day, and not by second. We can use the DATE() function to easily convert timestamps to dates and complete the following query: 1 2 3 SELECT DATE ( manufacture_time ), count ( * ) as count_baked_goods FROM baked_goods GROUP BY DATE ( manufacture_time ); Similarly, we can query the time with: 1 2 3 SELECT TIME ( manufacture_time ), count ( * ) as count_baked_goods FROM baked_goods GROUP BY TIME ( manufacture_time ); Find the number of baked goods by date of delivery. Be sure to alias the total count of baked goods as count_baked_goods . 1 SELECT DATE ( delivery_time ), count ( * ) as count_baked_goods FROM baked_goods GROUP BY DATE ( delivery_time ); Given a datepart and a column of date or timestamp data type, we can increment date or timestamp values by a specified interval. For example, in SQLite, the statement: 1 DATETIME ( time1 , '+3 hours' , '40 minutes' , '2 days' ); Would return a time 3 hours, 20 minutes, and 2 days after time1. Imagine that each dessert in our baked_goods table is inspected 2 hours, 30 minutes, and 1 day after the manufacture time. To derive the inspection date for each baked good, we can use the following query: 1 2 SELECT DATETIME ( manufacture_time , '+2 hours' , '30 minutes' , '1 day' ) as inspection_time FROM baked_goods ; Each of the baked goods is packaged by Baker\u2019s Market exactly five hours, twenty minutes, and two days after the delivery (designated by delivery_time). Create a query returning all the packaging times for the goods in the baked_goods table. Be sure to alias the package time column as package_time . 1 2 SELECT DATETIME ( delivery_time , '+5 hours' , '20 minutes' , '2 days' ) as package_time FROM baked_goods ; Numeric functions can be used to transform numbers. Some common SQLite mathematical functions are included below that take numeric data types as inputs: SELECT (number1 + number2); : returns the sum of two numbers. Similar, SQL can be used for subtraction, multiplication, and division. SELECT CAST(number1 AS REAL) / number3; : returns the result as a real number by casting one of the values as a real number, rather than an integer. SELECT ROUND(number, precision); : returns the numeric value rounded off to the next value specified. In our baked_goods table, we have information about cost designated by ingredients_cost . For accounting purposes, we\u2019d like to make sure that each ingredient cost is rounded to four decimal places rather than two, to account for currency fluctuations. 1 2 SELECT ROUND ( ingredients_cost , 4 ) as rounded_cost FROM baked_goods ; Find the bakery\u2019s distance from the market rounded to two decimal places. Be sure to alias the column as distance_from_market . 1 2 SELECT ROUND ( distance , 2 ) as distance_from_market FROM bakeries ; A couple more useful numeric SQL functions are included below: MAX and MIN . MAX(n1,n2,n3,...) : returns the greatest value in the set of the input numeric expressions MIN(n1,n2,n3,...) : returns the least value in the set of the input numeric expressions In our baked_goods table , in addition to the numeric ingredients_cost we have information about the packaging cost located in the packaging_cost column. We can use the MAX function to determine the overall greatest value of cost for each item using the following query: 1 2 SELECT id , MAX ( ingredients_cost , packaging_cost ) FROM baked_goods ; We also have information about cook time designated as cook_time and cool down time designated as cool_down_time in the baked_goods table. Find the greatest time value for each item in the table. 1 2 SELECT id , MAX ( cook_time , cool_down_time ) FROM baked_goods ; Find the least time value for each item in the table. 1 2 SELECT id , MIN ( cook_time , cool_down_time ) FROM baked_goods ; String manipulation can be useful to derive information from columns. We\u2019ll cover a couple of the common string functions here. A common use case for string manipulation in SQL is concatenation of strings. In SQLite, this is written as: 1 SELECT string1 || ' ' || string2 ; For example, the bakeries table contains both city and state columns. In order to create a route for these columns, we use the || function to concatenate them as in the following query: 1 2 SELECT city || ' ' || state as location FROM bakeries ; String functions are again, very database specific, and it is best practice to consult documentation before proceeding. Combine the first_name and last_name columns from the bakeries table as the full_name to identify the owners of the bakeries. Be sure to add a space between the names in the full_name as shown in the example. 1 SELECT first_name || ' ' || last_name as full_name FROM bakeries ; Another useful string function in SQL is REPLACE() : 1 REPLACE ( string , from_string , to_string ) The function returns the string string with all occurrences of the string from_string replaced by the string to_string. For example in baked_goods , there is a column named ingredients. The ingredients strings are formatted with underscores, such as baking_soda and vanilla_extract . To make these values more readable, we might like to replace the underscores with spaces. We can do so by using the following query: 1 2 SELECT id , REPLACE ( ingredients , '_' , ' ' ) as item_ingredients from baked_goods ; Any time enriched_flour appears in the ingredients list, we\u2019d like to replace it with just flour. Apply this transformation and also be sure to remove the underscore in enriched_flour . 1 2 SELECT REPLACE ( ingredients , 'enriched_' , ' ' ) as item_ingredients FROM baked_goods ;","title":"Date, Number, and String Functions"},{"location":"Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far_3","text":"Date Functions: DATETIME ; Returns the date and time of the column specified. This can be modified to return only the date or only the time. DATETIME(time1, +X hours, Y minutes, Z days) : Increments the specificed column by a given number of hours, minutes, or days. Numeric Functions: (number1 + number2); : Returns the sum of two numbers, or other mathematical operations, accordingly. CAST(number1 AS REAL) / number2; : Returns the result as a real number by casting one of numeric inputs as a real number ROUND(number, precision); : Returns the numeric value rounded off to the next value specified. String Functions: 'string1' || ' ' || 'string2'; : Concatenates string1 and string 2, with a space between. REPLACE(string,from_string,to_string) : Returns the string with all occurrences of the string from_string replaced by the string to_string .","title":"What can we generalize so far?"},{"location":"Command Line Crash Course/","text":"Foreword Commands and snippets. The Setup \u00b6 Linux \u00b6 Look through the menu for your window manager for anything named \u2018shell or \u2018terminal\u2019. Mac OS X \u00b6 Hold down the Cmd key and hit the spacebar. In the top right the blue \u2018search bar\u2019 will pop up. Type: terminal Click on the terminal application that looks kind of like a black box. This will open Terminal. You can now go to your dock and Ctrl -click to pull up the menu, then select Options->Keep In dock. Windows \u00b6 On Windows we\u2019re going to use PowerShell. People used to work with a program called cmd, but it\u2019s not nearly as usable as PowerShell. If you have Windows 7 or later, do this: Click Start. In Search programs and files type: powershell Hit Enter . Linux/Mac OS X \u00b6 List of commands: pwd ; print working directory. hostname ; my computer\u2019s network name. mkdir ; make directory. cd ; change directory. ls ; list directory. rmdir ; remove directory. pushd ; push directory. popd ; pop directory. cp ; copy a file or directory. mv ; move a file or directory. less ; page through a file. :q to quit. cat ; print the whole file. xargs ; execute arguments. find ; find files. grep ; find things inside files. man ; read a manual page. apropos ; find what man page is appropriate. env ; look at your environment. echo ; print some arguments. export ; export/set a new environment variable. exit ; exit the shell. sudo ; become super user root. Windows \u00b6 List of commands: pwd ; print working directory. hostname ; my computer\u2019s network name. mkdir ; make directory. cd ; change directory. ls ; list directory. rmdir ; remove directory. pushd ; push directory. popd ; pop directory. cp ; copy a file or directory. robocopy ; robust copy. mv ; move a file or directory. more ; page through a file. type ; print the whole file. forfiles ; run a command on lots of files. dir -r ; find files. select-string ; find things inside files. help ; read a manual page. helpctr ; find what man page is appropriate. echo ; print some arguments. set ; export/set a new environment variable. exit ; exit the shell. runas ; become super user root. Paths, Folders, Directories ( pwd ) \u00b6 $ (Unix) or > (Windows). You type in the stuff after $ or > , then hit Enter . You can then see what I have for output followed by another $ or > prompt. That content is the output and you should see the same output. Let\u2019s do a simple first command so you can get the hang of this: Linux/Mac OS X \u00b6 1 2 3 $ pwd /Users/zedshaw $ Windows \u00b6 1 2 3 4 5 PS C: \\> pwd C: \\U sers \\z ed PS C: \\> View a File ( less, more ) \u00b6 To do this exercise you\u2019re going to do some work using the commands you know so far. You\u2019ll also need a text editor that can make plain text (.txt) files. Here\u2019s what you do: Open your text editor and type some stuff into a new file. On OS X this could be TextWrangler. On Windows this might be Notepad++. On Linux this could be Gedit. Any editor will work. Save that file to your desktop and name it test.txt. In your shell use the commands you know to copy this file to your temp directory that you\u2019ve been working with. Once you\u2019ve done that, complete this exercise: Linux/Mac OS X \u00b6 1 2 3 $ less test.txt [ displays file here ] $ That\u2019s it. To get out of less just type :q (as in quit). Windows \u00b6 1 2 3 > more test.txt [ displays file here ] > Stream a File ( cat ) \u00b6 You\u2019re going to do some more setup for this one so you get used to making files in one program and then accessing them from the command line. With the same text editor from the last exercise, create another file named test2.txt but this time save it directly to your temp directory: Linux/Mac OS X \u00b6 1 2 3 4 5 6 7 8 9 10 $ less test2.txt [ displays file here ] $ cat test2.txt I am a fun guy. Dont you know why? Because I make poems, that make babies cry. $ cat test.txt Hi there this is cool. $ Windows \u00b6 1 2 3 4 5 6 7 8 9 10 > more test2.txt [ displays file here ] > cat test2.txt I am a fun guy. Dont you know why? Because I make poems, that make babies cry. > cat test.txt Hi there this is cool. > Edit a file ( cat, nano, pico, vim ) \u00b6 Unix: try cat test.txt test2.txt to concatenate the files on screen. Windows: try cat test.txt,test2.txt . cat test.txt will print on screen. cat file1.txt > file2.txt to copy. cat file1.txt >> file2.txt to append. Also nano test.txt , pico test.txt , and vim test.txt . Exiting Your Terminal ( exit ) \u00b6 Linux/Mac OS X \u00b6 1 $ exit Windows \u00b6 1 > exit Unix Bash References \u00b6 Reference Manual PowerShell References \u00b6 Owner\u2019s Manual . Master PowerShell .","title":"Command Line Crash Course"},{"location":"Command Line Crash Course/#the-setup","text":"","title":"The Setup"},{"location":"Command Line Crash Course/#linux","text":"Look through the menu for your window manager for anything named \u2018shell or \u2018terminal\u2019.","title":"Linux"},{"location":"Command Line Crash Course/#mac-os-x","text":"Hold down the Cmd key and hit the spacebar. In the top right the blue \u2018search bar\u2019 will pop up. Type: terminal Click on the terminal application that looks kind of like a black box. This will open Terminal. You can now go to your dock and Ctrl -click to pull up the menu, then select Options->Keep In dock.","title":"Mac OS X"},{"location":"Command Line Crash Course/#windows","text":"On Windows we\u2019re going to use PowerShell. People used to work with a program called cmd, but it\u2019s not nearly as usable as PowerShell. If you have Windows 7 or later, do this: Click Start. In Search programs and files type: powershell Hit Enter .","title":"Windows"},{"location":"Command Line Crash Course/#linuxmac-os-x","text":"List of commands: pwd ; print working directory. hostname ; my computer\u2019s network name. mkdir ; make directory. cd ; change directory. ls ; list directory. rmdir ; remove directory. pushd ; push directory. popd ; pop directory. cp ; copy a file or directory. mv ; move a file or directory. less ; page through a file. :q to quit. cat ; print the whole file. xargs ; execute arguments. find ; find files. grep ; find things inside files. man ; read a manual page. apropos ; find what man page is appropriate. env ; look at your environment. echo ; print some arguments. export ; export/set a new environment variable. exit ; exit the shell. sudo ; become super user root.","title":"Linux/Mac OS X"},{"location":"Command Line Crash Course/#windows_1","text":"List of commands: pwd ; print working directory. hostname ; my computer\u2019s network name. mkdir ; make directory. cd ; change directory. ls ; list directory. rmdir ; remove directory. pushd ; push directory. popd ; pop directory. cp ; copy a file or directory. robocopy ; robust copy. mv ; move a file or directory. more ; page through a file. type ; print the whole file. forfiles ; run a command on lots of files. dir -r ; find files. select-string ; find things inside files. help ; read a manual page. helpctr ; find what man page is appropriate. echo ; print some arguments. set ; export/set a new environment variable. exit ; exit the shell. runas ; become super user root.","title":"Windows"},{"location":"Command Line Crash Course/#paths-folders-directories-pwd","text":"$ (Unix) or > (Windows). You type in the stuff after $ or > , then hit Enter . You can then see what I have for output followed by another $ or > prompt. That content is the output and you should see the same output. Let\u2019s do a simple first command so you can get the hang of this:","title":"Paths, Folders, Directories (pwd)"},{"location":"Command Line Crash Course/#linuxmac-os-x_1","text":"1 2 3 $ pwd /Users/zedshaw $","title":"Linux/Mac OS X"},{"location":"Command Line Crash Course/#windows_2","text":"1 2 3 4 5 PS C: \\> pwd C: \\U sers \\z ed PS C: \\>","title":"Windows"},{"location":"Command Line Crash Course/#view-a-file-less-more","text":"To do this exercise you\u2019re going to do some work using the commands you know so far. You\u2019ll also need a text editor that can make plain text (.txt) files. Here\u2019s what you do: Open your text editor and type some stuff into a new file. On OS X this could be TextWrangler. On Windows this might be Notepad++. On Linux this could be Gedit. Any editor will work. Save that file to your desktop and name it test.txt. In your shell use the commands you know to copy this file to your temp directory that you\u2019ve been working with. Once you\u2019ve done that, complete this exercise:","title":"View a File (less, more)"},{"location":"Command Line Crash Course/#linuxmac-os-x_2","text":"1 2 3 $ less test.txt [ displays file here ] $ That\u2019s it. To get out of less just type :q (as in quit).","title":"Linux/Mac OS X"},{"location":"Command Line Crash Course/#windows_3","text":"1 2 3 > more test.txt [ displays file here ] >","title":"Windows"},{"location":"Command Line Crash Course/#stream-a-file-cat","text":"You\u2019re going to do some more setup for this one so you get used to making files in one program and then accessing them from the command line. With the same text editor from the last exercise, create another file named test2.txt but this time save it directly to your temp directory:","title":"Stream a File (cat)"},{"location":"Command Line Crash Course/#linuxmac-os-x_3","text":"1 2 3 4 5 6 7 8 9 10 $ less test2.txt [ displays file here ] $ cat test2.txt I am a fun guy. Dont you know why? Because I make poems, that make babies cry. $ cat test.txt Hi there this is cool. $","title":"Linux/Mac OS X"},{"location":"Command Line Crash Course/#windows_4","text":"1 2 3 4 5 6 7 8 9 10 > more test2.txt [ displays file here ] > cat test2.txt I am a fun guy. Dont you know why? Because I make poems, that make babies cry. > cat test.txt Hi there this is cool. >","title":"Windows"},{"location":"Command Line Crash Course/#edit-a-file-cat-nano-pico-vim","text":"Unix: try cat test.txt test2.txt to concatenate the files on screen. Windows: try cat test.txt,test2.txt . cat test.txt will print on screen. cat file1.txt > file2.txt to copy. cat file1.txt >> file2.txt to append. Also nano test.txt , pico test.txt , and vim test.txt .","title":"Edit a file (cat, nano, pico, vim)"},{"location":"Command Line Crash Course/#exiting-your-terminal-exit","text":"","title":"Exiting Your Terminal (exit)"},{"location":"Command Line Crash Course/#linuxmac-os-x_4","text":"1 $ exit","title":"Linux/Mac OS X"},{"location":"Command Line Crash Course/#windows_5","text":"1 > exit","title":"Windows"},{"location":"Command Line Crash Course/#unix-bash-references","text":"Reference Manual","title":"Unix Bash References"},{"location":"Command Line Crash Course/#powershell-references","text":"Owner\u2019s Manual . Master PowerShell .","title":"PowerShell References"},{"location":"Command Shell Snippets/","text":"Foreword Code snippets. Sort in alphabetical order: 1 $ sort myfile.txt Sort in numerical order: 1 $ sort -n myfile.txt Sort on multiple column (on column 2 in numerical order, then on column 1 in alphabetical order): 1 $ sort myfile.txt -k2n -k1 Sort a comma-separated table: 1 $ sort -k2 -k3 k1 -t ',' myfile.txt Sort in reversed order: 1 $ sort -r myfile.txt Sort a pip-separated file in reverses order by the second column: 1 $ sort myfile.txt -nrk 2 -st '|' Search text files for lines matching regular expressions (regex) with grep <matching string> <source> : 1 $ grep ArticleTitle webpage.html An asterisk ( * ) indicates any character (the matching expression starts with Ar and ends with le ): 1 $ grep Ar*le mytext.txt Search for starting metacharacter > : 1 $ grep ^ '>' mytext.txt List the access rights for all files: 1 $ ls -lag Run commands in background, kill the job running in the foreground, suspend the job running in the foreground, and background the suspended job: 1 2 3 4 5 6 7 $ command & $ ^C $ ^Z $ bg List the current jobs, foreground job number 1, and kill job number 1: 1 2 3 4 5 $ jobs $ fg%1 $ kill%1 List current processes, and kill process number 26152: 1 2 3 $ ps $ kill 26152 Help about commands: 1 2 3 $ man < command name> $ whatis < command name> Find and delete 1 2 3 4 5 6 7 # 1 find find . -name \"*.bak\" -type f find /dir/here... # 2 delete find . -name \"*.bak\" -type f -delete find . -name \"*.bak\" -print0 | xargs -0 rm -rf Rename many files 1 2 3 4 5 # 's/_13/_15/' replaces _13 by _15 in all files starting with 'cc' in the directory rename 's/_13/_15/' cc* # ... finishing by .jpg in the directory rename 's/_13/_15/' *.jpg","title":"Command Shell Snippets"},{"location":"Data_Storytelling/","text":"Foreword Notes. From Harvard Business Review. Time \u00b6 1- Reporting : perform descriptive analytics. Tell what happened. 2- Explanatory survey : analyze what people or objects are up to. Ask people what they think about something. Conceive a statistical model; what factors drive others. 3- Prediction : perform predictive analytics. Use historical data, add a statistical model, probabilities, and assumptions, predict the future. Find out what customers are likely to buy. Assess how likely it is for an event to happen. Forecast economic conditions. Focus \u00b6 4- What : tell what happened with a focus on one issue ( Reporting is not as focused). 5- Why : tell what underlying factors caused the outcome. Focus on the outcome. 6- How to address the issue: explore various ways to improve the situation. Focus on the situation. Depth \u00b6 7- CSI : run a small, ad hoc investigation. Find out why something is happening. Find out why some customer are dropping online transactions when they get to the postal code input form; what if some rural locations don\u2019t have postal codes? 8- Eureka : invest in long, analytically-driven searches for a solution to a complex problem. Discover the right way to refer and price potential buyers to real estate agents on a website. Since the project is core to the company\u2019s business model, it needs a corporate buy-in. The project involve several different analytical methods, false starts, dead-ends, discoveries. Methods \u00b6 9- Correlations : find why the relationships among variables rise and fall at the same time. 10- Causation : argue that one variable caused the other. Controlled experiment. Afterword \u00b6 These 10 approaches are not mutually exclusive. Begin the report with the result and recommended outcome. Follow with the demonstration. For the c-suite, keep technical terms to the minimum.","title":"Ten Kinds of Stories to Tell with Data"},{"location":"Data_Storytelling/#time","text":"1- Reporting : perform descriptive analytics. Tell what happened. 2- Explanatory survey : analyze what people or objects are up to. Ask people what they think about something. Conceive a statistical model; what factors drive others. 3- Prediction : perform predictive analytics. Use historical data, add a statistical model, probabilities, and assumptions, predict the future. Find out what customers are likely to buy. Assess how likely it is for an event to happen. Forecast economic conditions.","title":"Time"},{"location":"Data_Storytelling/#focus","text":"4- What : tell what happened with a focus on one issue ( Reporting is not as focused). 5- Why : tell what underlying factors caused the outcome. Focus on the outcome. 6- How to address the issue: explore various ways to improve the situation. Focus on the situation.","title":"Focus"},{"location":"Data_Storytelling/#depth","text":"7- CSI : run a small, ad hoc investigation. Find out why something is happening. Find out why some customer are dropping online transactions when they get to the postal code input form; what if some rural locations don\u2019t have postal codes? 8- Eureka : invest in long, analytically-driven searches for a solution to a complex problem. Discover the right way to refer and price potential buyers to real estate agents on a website. Since the project is core to the company\u2019s business model, it needs a corporate buy-in. The project involve several different analytical methods, false starts, dead-ends, discoveries.","title":"Depth"},{"location":"Data_Storytelling/#methods","text":"9- Correlations : find why the relationships among variables rise and fall at the same time. 10- Causation : argue that one variable caused the other. Controlled experiment.","title":"Methods"},{"location":"Data_Storytelling/#afterword","text":"These 10 approaches are not mutually exclusive. Begin the report with the result and recommended outcome. Follow with the demonstration. For the c-suite, keep technical terms to the minimum.","title":"Afterword"},{"location":"Digital Humanities/","text":"Foreword Notes. From: The Historian\u2019s Macroscope Book exploring big historical data. Digital Humanities Standford Humanities Center. Blogs \u00b6 Historyonics . Day of Archaeology . The Day of Archaeology is an event where archaeologists write about their activities on a group blog. Currently there are over 1000 posts on the blog; a lot to read in one sitting. Rather than closely read each post, we can do a distant reading to get some insights into the corpus. Distant reading refers to efforts to understand texts through quantitative analysis and visualisation. Global Perspective on Digital History . The Programming Historian Lessons, projects, research, blog. The Programming Historian offers novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate their research. ITHAKA S+R . Provides research and strategic guidance to help the academic community navigate economic and technological change. Cases \u00b6 Big Data + Old History . Video; dig into documents without reading them. Old Bailey . The proceedings of the Old Bailey, 1674-1913. A fully searchable edition of the largest body of texts detailing the lives of non-elite people ever published, containing 197,745 criminal trials held at London\u2019s central criminal court. White paper; Data Mining with Criminal Intent . Courses \u00b6 Python Programming for the Humanities . The Programming Historian Lessons . Data and Models \u00b6 Open Data . Datasets and Projects \u00b6 Le Programme de recherche en d\u00e9mographie historique . Donn\u00e9es g\u00e9n\u00e9alogiques de la Nouvelle France au Qu\u00e9bec contemporain. ORBIS . The Stanford Geospatial Network Model of the Roman World. Could become a boardgame (or an app) . Pelagios . Pelagios Commons provides online resources and a community forum for using open data methods to link and explore historical places. Maps . Blog . LOTR Project . LotrProject is dedicated to bringing J.R.R. Tolkien\u2019s works to life through various creative web projects (genealogy, interactive maps, timelines, and statistics). Computational Folkloristics . Mapping folktales and linking themes. Magazine . Article; Big Folklore: A Special Issue on Computational Folkloristics . Dataverse . Dataverse collects data on social-scientific, health, and environmental data for the world as a whole for the past four or five centuries. CLIWOC . Climatological Database for the World\u2019s Oceans 1750-1850. Wikipedia . Royal Netherlands Meteorological Institute . Networks \u00b6 Access Linked Open Data . RDF databases, graph databases, and how researchers can access these data though the query language called SPARQL. RDF represents information in a series of three-part \u2018statements\u2019 that comprise a subject, predicate, and an object. Network visualizations . Data extraction and network visualization of historical sources. UCINETS . UCINET 6 is a software package for the analysis of social network data. Pajek . Analysis and visualization of large networks. Analyse des r\u00e9seaux : une introduction \u00e0 Pajek . Network Workbench . A Large-Scale Network Analysis, Modeling and Visualization Toolkit for Biomedical, Social Science and Physics Research. Sci2 . The Science of Science (Sci2) Tool is a modular toolset specifically designed for the study of science. It supports the temporal, geospatial, topical, and network analysis and visualization of scholarly datasets at the micro (individual), meso (local), and macro (global) levels. NodeXL . Network overview, discovery and exploration for Excel. Gephi . Visualization and exploration software for all kinds of graphs and networks. Viewer . Exporter . Web Export . Tools, Data Mining and Analyzing \u00b6 Google Ngram . Mine Google Books. Online and an API is available. R and Python . Packages for text mining, text analysis, NLP (natural language processing). Topic Modeling Tool . Unix . Mining text and qualitative data with Unix. Download, trim, dig into dir and subdir, find patterns, count files, lines, words, save into a new file or a subdir, count instances and other stats like concordances, use pipelines, create lists with unix commands, etc. OpenRefine . Clean data, remove duplicate records, separate multiple values contained in the same field, analyse the distribution of values throughout a data set, group together different representations of the same reality, etc. AntConc . Corpus analysis. Create/download a corpus of texts, conduct a keyword-in-context search, identify patterns surrounding a particular word, use more specific search queries, look at statistically significant differences between corpora, make multi-modal comparisons using corpus lingiustic methods. Many other tools on Laurence Anthony\u2019s Website : AntConc : a freeware corpus analysis toolkit for concordancing and text analysis. AntPConc : a freeware parallel corpus analysis toolkit for concordancing and text analysis using UTF-8 encoded text files. AntWordProfiler : a freeware tool for profiling the vocabulary level and complexity of texts. AntFileConverter : a freeware tool to convert PDF and Word (DOCX) files into plain text for use in corpus tools like AntConc. AntMover : a freeware text structure (moves, outline, flow) analysis program. AntCLAWSGUI : a front-end interface to the CLAWS tagger developed at Lancaster University. EncodeAnt : a freeware tool for detecting and converting character encodings. FireAnt : a freeware social media and data analysis toolkit (developed in collaboration with Claire Hardaker of Lancaster University). ProtAnt : a freeware prototypical text analysis tool (developed in collaboration with Paul Baker of Lancaster University). SarAnt : a freeware batch search and replace tool. SegmentAnt : a freeware Japanese and Chinese segmenter (segmentation/tokenizing tool). TagAnt : a freeware Part-Of-Speech (POS) tagger built on TreeTagger (developed by Helmut Schmid). VariAnt : A freeware spelling VariAnt analysis program. Beautiful Soup . Python module. Text parsing. TaPOR . Gateway to the tools for sophisticated text analysis and retrieval. Paper Machines . Visualize thousands of texts. Plugin for the Zotero bibliographic management software. Voyant . Online tool for concordances, wordles, stats, graphics. Can be set on a local server. Documentation Overview . Search, visualize, and review your documents. Up to hundreds of thousands of them, in any format. MALLET . NLP toolkit and machine learning. Topic Analysis, keywords, bags of words. Topic modeling tool takes a single text (or corpus) and looks for patterns in the use of words; it is an attempt to inject semantic meaning into vocabulary. Topic models represent a family of computer programs that extract topics from texts. A topic to the computer is a list of words that occur in statistically meaningful ways. A text can be an email, a blog post, a book chapter, a journal article, a diary entry \u2013 that is, any kind of unstructured text. Documentation . Article; Getting Started with Topic Modeling and MALLET . Stanford Topic Modeling Toolbox . The Stanford Topic Modeling Toolbox (TMT) brings topic modeling tools to social scientists and others who wish to perform analysis on datasets that have a substantial textual component: Import and manipulate text from cells in Excel and other spreadsheets. Train topic models (LDA, Labeled LDA, and PLDA new) to create summaries of the text. Select parameters (such as the number of topics) via a data-driven process. Generate rich Excel-compatible outputs for tracking word usage across topics, time, and other groupings of data. Regexr . Online tool for processing regular expression or regex. Regex can also be done . Tutorial on how to process regular expressions in Notepad++, on TextWrangler or other text/code editors (Vim, Emacs, etc.). Tools, Mining the Web \u00b6 Retrieving Web Archive: Internet Archives, Wayback Machine . Time Travel . Mining the Internet Archive Collection . internetarchive Python package. Wget . Automated downloading with Wget. Pull data from the web. Query . Downloading many records using Python. How to check if a directory exists and create it if necessary . Figshare . Web scraping. Outwit . Find, grab and organize all kinds of data and media from online sources. XPath . Web scraping, screen scraping, data parsing and other related things. About XPath . Importio . Extract web data the easy way. Tabula . Extract data from PDF. Tools, Referencing \u00b6 Zotero . Standalone and add-in to Mozilla Firefox. Add-in to text processors (including LaTeX and Markdown editors). Dig into journals and books primary sources. Collect, organize, cite, and share your research sources. JSOR . Journals, primary sources, and now BOOKS. Tools, Scanning \u00b6 OCR scanner Digitization material documents (from books, letters to maps). The MNIST Database . The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples (used in machine learning to process NLP). It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. Tools, Visualization \u00b6 Sparklines are mini-graphics. Add-in to Excel. GIS: the best open-source database for mapping and GIS is PostgreSQL. Google Maps and Google Earth . Google My Maps and Google Earth provide an easy way to start creating digital maps. With a Google Account you can create and edit personal maps by clicking on My Places. In My Maps you can choose between several different base maps (including the standard satellite, terrain, or standard maps) and add points, lines and polygons. It is also possible to import data from a spreadsheet, if you have columns with geographical information (i.e. longitudes and latitudes or place names). This automates a formerly complex task known as geocoding. Not only is this one of the easiest ways to begin plotting your historical data on a map, but it also has the power of Google\u2019s search engine. As you read about unfamiliar places in historical documents, journal articles or books, you can search for them using Google Maps. It is then possible to mark numerous locations and explore how they relate to each other geographically. Your personal maps are saved by Google (in their cloud), meaning you can access them from any computer with an internet connection. You can keep them private or embed them in your website or blog. Finally, you can export your points, lines, and polygons as KML files and open them in Google Earth or Quantum GIS. QGIS Adding layers to a map. Installing QGIS 2.0 and Adding Layers . Creating New Vector Layers . Georeferencing . A Free and Open Source Geographic Information System . Omeka . Displays items, collections, like in museums, libraries, archives with narratives. Augmented Reality . Overlaying of digital content (images, video, text, sound, etc.) onto physical objects or locations, and it is typically experienced by looking through the camera lens of an electronic device such as a smartphone, tablet, or optical head-mounted display (e.g. Microsoft Hololens). Although AR is a cutting-edge, complex technology, there are a number of user-friendly platforms that allow people with no previous coding experience to create compelling augmented reality experiences. D3.js . JavaScript visualization for the web; server-side. D3 Examples . A freelancer\u2019s gallery . Bokeh . Gallery . Top 7 Free Infographics Tools & Online Makers in 2016 . SHANTI INTERACTIVE . Suite of tools that make it easy to create highly interactive web-based visualizations, videos, and maps. They are freely available from the University of Virginia\u2019s Sciences, Humanities & Arts Network of Technological Initiatives (SHANTI). Qmedia provides new ways to use video for instructional and scholarly purposes. The viewer interacts with the whole screen and sees a wide array of web-based resources and offers an immersive experience that adds context. SHIVA takes a new approach that makes it easy to add graphical and data-driven visualizations to websites. Elements such as data, charts, network graphs, maps, image montages, and timelines are easily created. MapScholar is an online platform for geospatial visualization funded by the NEH. It enables humanities and social science scholars to create digital \u201catlases\u201d featuring high-resolution images of historic maps. VisualEyes is web-based authoring tool for historic visualization funded by the NEH to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations. VisualEyes5 is a HTML5 version of the VisualEyes authoring tool for historic visualization to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.","title":"Digital Humanities"},{"location":"Digital Humanities/#blogs","text":"Historyonics . Day of Archaeology . The Day of Archaeology is an event where archaeologists write about their activities on a group blog. Currently there are over 1000 posts on the blog; a lot to read in one sitting. Rather than closely read each post, we can do a distant reading to get some insights into the corpus. Distant reading refers to efforts to understand texts through quantitative analysis and visualisation. Global Perspective on Digital History . The Programming Historian Lessons, projects, research, blog. The Programming Historian offers novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate their research. ITHAKA S+R . Provides research and strategic guidance to help the academic community navigate economic and technological change.","title":"Blogs"},{"location":"Digital Humanities/#cases","text":"Big Data + Old History . Video; dig into documents without reading them. Old Bailey . The proceedings of the Old Bailey, 1674-1913. A fully searchable edition of the largest body of texts detailing the lives of non-elite people ever published, containing 197,745 criminal trials held at London\u2019s central criminal court. White paper; Data Mining with Criminal Intent .","title":"Cases"},{"location":"Digital Humanities/#courses","text":"Python Programming for the Humanities . The Programming Historian Lessons .","title":"Courses"},{"location":"Digital Humanities/#data-and-models","text":"Open Data .","title":"Data and Models"},{"location":"Digital Humanities/#datasets-and-projects","text":"Le Programme de recherche en d\u00e9mographie historique . Donn\u00e9es g\u00e9n\u00e9alogiques de la Nouvelle France au Qu\u00e9bec contemporain. ORBIS . The Stanford Geospatial Network Model of the Roman World. Could become a boardgame (or an app) . Pelagios . Pelagios Commons provides online resources and a community forum for using open data methods to link and explore historical places. Maps . Blog . LOTR Project . LotrProject is dedicated to bringing J.R.R. Tolkien\u2019s works to life through various creative web projects (genealogy, interactive maps, timelines, and statistics). Computational Folkloristics . Mapping folktales and linking themes. Magazine . Article; Big Folklore: A Special Issue on Computational Folkloristics . Dataverse . Dataverse collects data on social-scientific, health, and environmental data for the world as a whole for the past four or five centuries. CLIWOC . Climatological Database for the World\u2019s Oceans 1750-1850. Wikipedia . Royal Netherlands Meteorological Institute .","title":"Datasets and Projects"},{"location":"Digital Humanities/#networks","text":"Access Linked Open Data . RDF databases, graph databases, and how researchers can access these data though the query language called SPARQL. RDF represents information in a series of three-part \u2018statements\u2019 that comprise a subject, predicate, and an object. Network visualizations . Data extraction and network visualization of historical sources. UCINETS . UCINET 6 is a software package for the analysis of social network data. Pajek . Analysis and visualization of large networks. Analyse des r\u00e9seaux : une introduction \u00e0 Pajek . Network Workbench . A Large-Scale Network Analysis, Modeling and Visualization Toolkit for Biomedical, Social Science and Physics Research. Sci2 . The Science of Science (Sci2) Tool is a modular toolset specifically designed for the study of science. It supports the temporal, geospatial, topical, and network analysis and visualization of scholarly datasets at the micro (individual), meso (local), and macro (global) levels. NodeXL . Network overview, discovery and exploration for Excel. Gephi . Visualization and exploration software for all kinds of graphs and networks. Viewer . Exporter . Web Export .","title":"Networks"},{"location":"Digital Humanities/#tools-data-mining-and-analyzing","text":"Google Ngram . Mine Google Books. Online and an API is available. R and Python . Packages for text mining, text analysis, NLP (natural language processing). Topic Modeling Tool . Unix . Mining text and qualitative data with Unix. Download, trim, dig into dir and subdir, find patterns, count files, lines, words, save into a new file or a subdir, count instances and other stats like concordances, use pipelines, create lists with unix commands, etc. OpenRefine . Clean data, remove duplicate records, separate multiple values contained in the same field, analyse the distribution of values throughout a data set, group together different representations of the same reality, etc. AntConc . Corpus analysis. Create/download a corpus of texts, conduct a keyword-in-context search, identify patterns surrounding a particular word, use more specific search queries, look at statistically significant differences between corpora, make multi-modal comparisons using corpus lingiustic methods. Many other tools on Laurence Anthony\u2019s Website : AntConc : a freeware corpus analysis toolkit for concordancing and text analysis. AntPConc : a freeware parallel corpus analysis toolkit for concordancing and text analysis using UTF-8 encoded text files. AntWordProfiler : a freeware tool for profiling the vocabulary level and complexity of texts. AntFileConverter : a freeware tool to convert PDF and Word (DOCX) files into plain text for use in corpus tools like AntConc. AntMover : a freeware text structure (moves, outline, flow) analysis program. AntCLAWSGUI : a front-end interface to the CLAWS tagger developed at Lancaster University. EncodeAnt : a freeware tool for detecting and converting character encodings. FireAnt : a freeware social media and data analysis toolkit (developed in collaboration with Claire Hardaker of Lancaster University). ProtAnt : a freeware prototypical text analysis tool (developed in collaboration with Paul Baker of Lancaster University). SarAnt : a freeware batch search and replace tool. SegmentAnt : a freeware Japanese and Chinese segmenter (segmentation/tokenizing tool). TagAnt : a freeware Part-Of-Speech (POS) tagger built on TreeTagger (developed by Helmut Schmid). VariAnt : A freeware spelling VariAnt analysis program. Beautiful Soup . Python module. Text parsing. TaPOR . Gateway to the tools for sophisticated text analysis and retrieval. Paper Machines . Visualize thousands of texts. Plugin for the Zotero bibliographic management software. Voyant . Online tool for concordances, wordles, stats, graphics. Can be set on a local server. Documentation Overview . Search, visualize, and review your documents. Up to hundreds of thousands of them, in any format. MALLET . NLP toolkit and machine learning. Topic Analysis, keywords, bags of words. Topic modeling tool takes a single text (or corpus) and looks for patterns in the use of words; it is an attempt to inject semantic meaning into vocabulary. Topic models represent a family of computer programs that extract topics from texts. A topic to the computer is a list of words that occur in statistically meaningful ways. A text can be an email, a blog post, a book chapter, a journal article, a diary entry \u2013 that is, any kind of unstructured text. Documentation . Article; Getting Started with Topic Modeling and MALLET . Stanford Topic Modeling Toolbox . The Stanford Topic Modeling Toolbox (TMT) brings topic modeling tools to social scientists and others who wish to perform analysis on datasets that have a substantial textual component: Import and manipulate text from cells in Excel and other spreadsheets. Train topic models (LDA, Labeled LDA, and PLDA new) to create summaries of the text. Select parameters (such as the number of topics) via a data-driven process. Generate rich Excel-compatible outputs for tracking word usage across topics, time, and other groupings of data. Regexr . Online tool for processing regular expression or regex. Regex can also be done . Tutorial on how to process regular expressions in Notepad++, on TextWrangler or other text/code editors (Vim, Emacs, etc.).","title":"Tools, Data Mining and Analyzing"},{"location":"Digital Humanities/#tools-mining-the-web","text":"Retrieving Web Archive: Internet Archives, Wayback Machine . Time Travel . Mining the Internet Archive Collection . internetarchive Python package. Wget . Automated downloading with Wget. Pull data from the web. Query . Downloading many records using Python. How to check if a directory exists and create it if necessary . Figshare . Web scraping. Outwit . Find, grab and organize all kinds of data and media from online sources. XPath . Web scraping, screen scraping, data parsing and other related things. About XPath . Importio . Extract web data the easy way. Tabula . Extract data from PDF.","title":"Tools, Mining the Web"},{"location":"Digital Humanities/#tools-referencing","text":"Zotero . Standalone and add-in to Mozilla Firefox. Add-in to text processors (including LaTeX and Markdown editors). Dig into journals and books primary sources. Collect, organize, cite, and share your research sources. JSOR . Journals, primary sources, and now BOOKS.","title":"Tools, Referencing"},{"location":"Digital Humanities/#tools-scanning","text":"OCR scanner Digitization material documents (from books, letters to maps). The MNIST Database . The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples (used in machine learning to process NLP). It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.","title":"Tools, Scanning"},{"location":"Digital Humanities/#tools-visualization","text":"Sparklines are mini-graphics. Add-in to Excel. GIS: the best open-source database for mapping and GIS is PostgreSQL. Google Maps and Google Earth . Google My Maps and Google Earth provide an easy way to start creating digital maps. With a Google Account you can create and edit personal maps by clicking on My Places. In My Maps you can choose between several different base maps (including the standard satellite, terrain, or standard maps) and add points, lines and polygons. It is also possible to import data from a spreadsheet, if you have columns with geographical information (i.e. longitudes and latitudes or place names). This automates a formerly complex task known as geocoding. Not only is this one of the easiest ways to begin plotting your historical data on a map, but it also has the power of Google\u2019s search engine. As you read about unfamiliar places in historical documents, journal articles or books, you can search for them using Google Maps. It is then possible to mark numerous locations and explore how they relate to each other geographically. Your personal maps are saved by Google (in their cloud), meaning you can access them from any computer with an internet connection. You can keep them private or embed them in your website or blog. Finally, you can export your points, lines, and polygons as KML files and open them in Google Earth or Quantum GIS. QGIS Adding layers to a map. Installing QGIS 2.0 and Adding Layers . Creating New Vector Layers . Georeferencing . A Free and Open Source Geographic Information System . Omeka . Displays items, collections, like in museums, libraries, archives with narratives. Augmented Reality . Overlaying of digital content (images, video, text, sound, etc.) onto physical objects or locations, and it is typically experienced by looking through the camera lens of an electronic device such as a smartphone, tablet, or optical head-mounted display (e.g. Microsoft Hololens). Although AR is a cutting-edge, complex technology, there are a number of user-friendly platforms that allow people with no previous coding experience to create compelling augmented reality experiences. D3.js . JavaScript visualization for the web; server-side. D3 Examples . A freelancer\u2019s gallery . Bokeh . Gallery . Top 7 Free Infographics Tools & Online Makers in 2016 . SHANTI INTERACTIVE . Suite of tools that make it easy to create highly interactive web-based visualizations, videos, and maps. They are freely available from the University of Virginia\u2019s Sciences, Humanities & Arts Network of Technological Initiatives (SHANTI). Qmedia provides new ways to use video for instructional and scholarly purposes. The viewer interacts with the whole screen and sees a wide array of web-based resources and offers an immersive experience that adds context. SHIVA takes a new approach that makes it easy to add graphical and data-driven visualizations to websites. Elements such as data, charts, network graphs, maps, image montages, and timelines are easily created. MapScholar is an online platform for geospatial visualization funded by the NEH. It enables humanities and social science scholars to create digital \u201catlases\u201d featuring high-resolution images of historic maps. VisualEyes is web-based authoring tool for historic visualization funded by the NEH to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations. VisualEyes5 is a HTML5 version of the VisualEyes authoring tool for historic visualization to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.","title":"Tools, Visualization"},{"location":"Edition_CS/","text":"Foreword Cheat sheets. Markdown \u00b6 GitHub Markdown Syntax . PDF. Git \u00b6 Principal commands \u00b6 git init : creates a new Git repository. git status : inspects the contents of the working directory and staging area. git add <filename_1> <filename_2> : adds files from the working directory to the staging area. git add . , git add --all . git diff <filename_1> <filename_2> : shows the difference between the working directory and the staging area. git commit -m \"<comments>\" : permanently stores file changes from the staging area in the repository. git log : shows a lis t of all previous commits. git show HEAD : show the most recent commit. git checkout HEAD <filename_1> <filename_2> : discards changes in the working directory. git checkout <filename_1> <filename_2> : idem. git reset HEAD <filename_1> <filename_2> : unstages file changes in the staging area, restore the file in your working directory. git reset <SHA> (7 characters) : can be used to reset to a previous commit in your commit history. git branch : lists all a Git project\u2019s branches. git branch <branch_name> : creates a new branch. git checkout <branch_name> : used to switch from one branch to another (the branch_name). git merge <branch_name> : used to join file changes from one branch to another (your in branch A, you merge branch_name with A). git branch -d <branch_name> : deletes the branch specified. git clone <remote_location> <clone_name> : creates a local copy of a remote. git remote -v : lists a Git project\u2019s remotes. git fetch : fetches work from the remote into the local copy (no merging). git merge origin/master : Merges origin/master into your local branch. git push origin <branch_name> : pushes a local branch to the origin remote. Git Cheat Sheet . PDF. Git Cheat Sheet . PDF. Git Cheat Sheet . PDF. LaTeX \u00b6 LaTeX Font Packages . PDF only. LaTeX Cheat Sheet, 4p . PDF only. LaTeX Cheat Sheet, 2p . PDF. Regex \u00b6 Regex . PDF. Tableau \u00b6 Tableau Cheat Sheet . PDF only. Vi/Vim \u00b6 Vi Editor . PDF. Vim Visual . PDF. Wikipedia \u00b6 Wikipedia Syntax . PDF.","title":"Edition Cheat Sheets"},{"location":"Edition_CS/#markdown","text":"GitHub Markdown Syntax . PDF.","title":"Markdown"},{"location":"Edition_CS/#git","text":"","title":"Git"},{"location":"Edition_CS/#principal-commands","text":"git init : creates a new Git repository. git status : inspects the contents of the working directory and staging area. git add <filename_1> <filename_2> : adds files from the working directory to the staging area. git add . , git add --all . git diff <filename_1> <filename_2> : shows the difference between the working directory and the staging area. git commit -m \"<comments>\" : permanently stores file changes from the staging area in the repository. git log : shows a lis t of all previous commits. git show HEAD : show the most recent commit. git checkout HEAD <filename_1> <filename_2> : discards changes in the working directory. git checkout <filename_1> <filename_2> : idem. git reset HEAD <filename_1> <filename_2> : unstages file changes in the staging area, restore the file in your working directory. git reset <SHA> (7 characters) : can be used to reset to a previous commit in your commit history. git branch : lists all a Git project\u2019s branches. git branch <branch_name> : creates a new branch. git checkout <branch_name> : used to switch from one branch to another (the branch_name). git merge <branch_name> : used to join file changes from one branch to another (your in branch A, you merge branch_name with A). git branch -d <branch_name> : deletes the branch specified. git clone <remote_location> <clone_name> : creates a local copy of a remote. git remote -v : lists a Git project\u2019s remotes. git fetch : fetches work from the remote into the local copy (no merging). git merge origin/master : Merges origin/master into your local branch. git push origin <branch_name> : pushes a local branch to the origin remote. Git Cheat Sheet . PDF. Git Cheat Sheet . PDF. Git Cheat Sheet . PDF.","title":"Principal commands"},{"location":"Edition_CS/#latex","text":"LaTeX Font Packages . PDF only. LaTeX Cheat Sheet, 4p . PDF only. LaTeX Cheat Sheet, 2p . PDF.","title":"LaTeX"},{"location":"Edition_CS/#regex","text":"Regex . PDF.","title":"Regex"},{"location":"Edition_CS/#tableau","text":"Tableau Cheat Sheet . PDF only.","title":"Tableau"},{"location":"Edition_CS/#vivim","text":"Vi Editor . PDF. Vim Visual . PDF.","title":"Vi/Vim"},{"location":"Edition_CS/#wikipedia","text":"Wikipedia Syntax . PDF.","title":"Wikipedia"},{"location":"Google_Cloud/","text":"Foreword A step-by-step tutorial to set up a data science environment on Google Cloud: create an instance on Google Compute Engine, install Anaconda and run Jupyter Notebook. From DataCamp. Among the services: Cloud Dataprep; Cloud Datalab; Cloud Machine Learning Engine; BigQuery a data warehouse solution that holds many fascinating Big Data datasets. For a deeper understanding of Google Cloud: Create an Account and Project \u00b6 To create your GCP account with the 12-month free trial and/or free . Once the account is active, the access page is the Web console . Go to the Resource Management page . Create a new project. Go into the project. In the roles section of the IAM page, you can add people with specific roles to your project. Create an instance \u00b6 A Virtual Machine (VM) also called \u201can instance\u201d is an on-demand server that you activate as needed. Go to your dashboard . In the top left menu select \u201cCompute Engine\u201d. Click on \u201cVM instances\u201d. It may take a moment\u2026 In the dialog, click on the \u201cCreate\u201d button. Fill out the fields\u2026 Name starling (or any other name), Region (the rule of thumb is to select the cheapest region closest to you to minimize latency and prices vary significantly by region), Zone, Type (default setup n1-standard-1 with 3.75 Gb RAM and 1vCPU for an estimated price per month), Disk (default Debian GNU/Linux 9 (stretch) OS with 10 Gb), Firewall (access the VM from the internet by allowing http and https traffic). Enable a persistent disk for backup purposes: Click on the \u201cManagement, Disks, networking, SSH keys\u201d link. Unselect the \u201cDeletion Rule\u201d. \u201cCreate\u201d button. It may take a moment\u2026 Notice the link \u201cEquivalent command line\u201d link below the Create button. This link shows the equivalent command line needed to create the same instance from scratch. This is a truly smart feature that facilitates learning the syntax of gcloud SDK . Google\u2019s Cloud Shell \u00b6 Google\u2019s Cloud Shell is a stand alone terminal in your browser from which you can access and manage your resources. You activate the google shell by clicking the >\\_ icon in the upper right part of the console page. This terminal runs on a f1-micro Google Compute Engine virtual machine with a Debian operating system and 5Gb storage. It is created on a per-user, per-session basis. It persists while your cloud shell session is active and is deleted after 20 minutes of inactivity. Since the associated disk is persistent across sessions, your content (files, configurations, \u2026) will be available from session to session. The cloud shell instance comes pre-installed with the gcloud SDK and vim. The instance underlying the cloud shell is just a convenient way to have a resource management environment and store your configurations on an ephemeral instance. The VM instance that you just created, named starling in the above example, is the instance where you want to install your data science environment. Instead of using the Google Cloud shell, you can also install the gcloud SDK on your local machine and manage everything from your local environment. Commands: List your instances, gcloud compute instances list Stop the instance (takes a few seconds), gcloud compute instances stop <instance name> Start the instance (also takes a few seconds), gcloud compute instances start <instance name> and ssh into the starling instance, gcloud compute ssh <instance name> Debian packages \u00b6 These package must be intalled: bzip2, which is required to install Mini/Anaconda, git, which is always useful to have, and libxml2-dev, which is not required at this point but you will often need it when installing further Python libraries. Run the following commands, which work for both Ubuntu and Debian, in the terminal: 1 2 $ sudo apt-get update $ sudo apt-get install bzip2 git libxml2-dev Anaconda / Miniconda \u00b6 Install the lighter Miniconda distribution , run 1 2 $ wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh $ bash Miniconda3-latest-Linux-x86_64.sh It may take a moment\u2026 1 2 3 $ rm Miniconda3-latest-Linux-x86_64.sh $ source ~/.bashrc $ conda install scikit-learn pandas jupyter ipython It may take a moment\u2026 The commands to install the full Anaconda distribution are very similar. Make sure to check the download page to get the latest version of the shell script file: 1 2 3 4 $ wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh $ bash Anaconda3-5.0.1-Linux-x86_64.sh $ rm Anaconda3-5.0.1-Linux-x86_64.sh $ source ~/.bahsrc To verify that everything is installed properly, check your python version with python --version and verify that the right python is called by default with the command which python . Allowing Web Access \u00b6 The third and final step is to configure your VM to allow web access to your Jupyter notebooks. To do that, you will create a firewall rule via the Google Cloud console. Go back to your Instances dashboard and in the top left menu, select \u201cVPC Network > Firewall rules\u201d. Click on the \u201cCREATE FIREWAL RULE\u201d link and fill out the following values: Name: jupyter-rule (you can choose any name) Source IP ranges: 0.0.0.0/0 tags cibles: starling (ou another chosen name) Specified protocols and ports: tcp:8888 and leave all the other variables to their default values. This firewall rule allows all incoming traffic (from all IPs) to hit the port 8888. Now go back to the VM page (top left menu > Compute Engine > VM instances), click on your VM name. Make a note of your VM IP address. This is the IP address that you will use in your browser to access your Jupyter environment. Internal IP and External IP\u2026 Make sure the Firewall rules are checked. Jupyter Configuration \u00b6 In the terminal, run jupyter notebook --generate-config to generate the configuration file. And jupyter notebook password to generate a password. Now edit the configuration file you just created with vim .jupyter/jupyter_notebook_config.py or with nano .jupyter/jupyter_notebook_config.py and add the following line at the top of the file: c.NotebookApp.ip = '*' . (to switch to edit mode in vim, just type the i character). Quit and save with the following sequence ESC : wq . IMPORTANT! This will allow the notebook to be available for all IP addresses on your VM and not just the http://localhost:8888 URL you may be familiar with when working on your local machine. Launch \u00b6 1 $ jupyter-notebook --no-browser --port = 8888 In your browser go to the URL: http:// :8888/ or click on the link to access your newly operational Jupyter notebook. Enter the password. You are Jupyter Notebook. Power \u00b6 For running classic sklearn or Deep Learning models, bump up the instance\u2019s number of CPUs. Up/Download \u00b6 Download from an instance on gcloud to your local folder ./: 1 gcloud compute scp [instance_name]:[path to file on instance] ./ Upload some local file to your instance: 1 gcloud compute scp [ local file ] . [ instance_name ] : [ path ] Automate \u00b6 The trick is to use the full path to the python executable which you can find with which python . Then the following cron job will run your script every hour, 10 mn past the hour: 1 10 * * * * [full_path to python] [full path to your script] &gt; [some log file] iPython \u00b6 Jupyter notebooks are very convenient for online collaborative work. But you can also run an IPython session from your terminal simply with the ipython command. That will open an IPython session which has all the bells and whistles of a Jupyter notebook such as magic commands but without the web interface. R \u00b6 It\u2019s easy to set up your VM to be enable R notebooks in your Jupyter console. Intermezzo \u00b6 A few notes on your current setup: The IP address you used in your browser is ephemeral. Which means that every time you restart your VM, your notebooks will have a different URL. You can make that IP static by going to: Top left menu > VPC Network > External IP addresses and select \u201cstatic\u201d in the drop down menu. The security of your current setup relies on the strength of the Jupyter notebook password that you defined previously. Anyone on the internet can access your Jupyter environment at the same URL you use (and bots will absolutely try). One powerful but very unsecure core feature of Jupyter notebooks is that you can launch a terminal with sudo access directly from the notebook. This means that anyone accessing your notebook could take control of your VM after cracking your notebook password and potentially run anything that would send your bills through the roof. The first level measures to prevent that from happening includes. Making sure your Jupyter password is a strong one. Remember to stop your VM when you\u2019re not working on it. You are also currently running the server over http and not https which is not secure enough. Encrypt provides free SSL/TLS certificates and is the encryption solution recommended in the Jupyter documentation .","title":"Google Cloud"},{"location":"Google_Cloud/#create-an-account-and-project","text":"To create your GCP account with the 12-month free trial and/or free . Once the account is active, the access page is the Web console . Go to the Resource Management page . Create a new project. Go into the project. In the roles section of the IAM page, you can add people with specific roles to your project.","title":"Create an Account and Project"},{"location":"Google_Cloud/#create-an-instance","text":"A Virtual Machine (VM) also called \u201can instance\u201d is an on-demand server that you activate as needed. Go to your dashboard . In the top left menu select \u201cCompute Engine\u201d. Click on \u201cVM instances\u201d. It may take a moment\u2026 In the dialog, click on the \u201cCreate\u201d button. Fill out the fields\u2026 Name starling (or any other name), Region (the rule of thumb is to select the cheapest region closest to you to minimize latency and prices vary significantly by region), Zone, Type (default setup n1-standard-1 with 3.75 Gb RAM and 1vCPU for an estimated price per month), Disk (default Debian GNU/Linux 9 (stretch) OS with 10 Gb), Firewall (access the VM from the internet by allowing http and https traffic). Enable a persistent disk for backup purposes: Click on the \u201cManagement, Disks, networking, SSH keys\u201d link. Unselect the \u201cDeletion Rule\u201d. \u201cCreate\u201d button. It may take a moment\u2026 Notice the link \u201cEquivalent command line\u201d link below the Create button. This link shows the equivalent command line needed to create the same instance from scratch. This is a truly smart feature that facilitates learning the syntax of gcloud SDK .","title":"Create an instance"},{"location":"Google_Cloud/#googles-cloud-shell","text":"Google\u2019s Cloud Shell is a stand alone terminal in your browser from which you can access and manage your resources. You activate the google shell by clicking the >\\_ icon in the upper right part of the console page. This terminal runs on a f1-micro Google Compute Engine virtual machine with a Debian operating system and 5Gb storage. It is created on a per-user, per-session basis. It persists while your cloud shell session is active and is deleted after 20 minutes of inactivity. Since the associated disk is persistent across sessions, your content (files, configurations, \u2026) will be available from session to session. The cloud shell instance comes pre-installed with the gcloud SDK and vim. The instance underlying the cloud shell is just a convenient way to have a resource management environment and store your configurations on an ephemeral instance. The VM instance that you just created, named starling in the above example, is the instance where you want to install your data science environment. Instead of using the Google Cloud shell, you can also install the gcloud SDK on your local machine and manage everything from your local environment. Commands: List your instances, gcloud compute instances list Stop the instance (takes a few seconds), gcloud compute instances stop <instance name> Start the instance (also takes a few seconds), gcloud compute instances start <instance name> and ssh into the starling instance, gcloud compute ssh <instance name>","title":"Google's Cloud Shell"},{"location":"Google_Cloud/#debian-packages","text":"These package must be intalled: bzip2, which is required to install Mini/Anaconda, git, which is always useful to have, and libxml2-dev, which is not required at this point but you will often need it when installing further Python libraries. Run the following commands, which work for both Ubuntu and Debian, in the terminal: 1 2 $ sudo apt-get update $ sudo apt-get install bzip2 git libxml2-dev","title":"Debian packages"},{"location":"Google_Cloud/#anaconda-miniconda","text":"Install the lighter Miniconda distribution , run 1 2 $ wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh $ bash Miniconda3-latest-Linux-x86_64.sh It may take a moment\u2026 1 2 3 $ rm Miniconda3-latest-Linux-x86_64.sh $ source ~/.bashrc $ conda install scikit-learn pandas jupyter ipython It may take a moment\u2026 The commands to install the full Anaconda distribution are very similar. Make sure to check the download page to get the latest version of the shell script file: 1 2 3 4 $ wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh $ bash Anaconda3-5.0.1-Linux-x86_64.sh $ rm Anaconda3-5.0.1-Linux-x86_64.sh $ source ~/.bahsrc To verify that everything is installed properly, check your python version with python --version and verify that the right python is called by default with the command which python .","title":"Anaconda / Miniconda"},{"location":"Google_Cloud/#allowing-web-access","text":"The third and final step is to configure your VM to allow web access to your Jupyter notebooks. To do that, you will create a firewall rule via the Google Cloud console. Go back to your Instances dashboard and in the top left menu, select \u201cVPC Network > Firewall rules\u201d. Click on the \u201cCREATE FIREWAL RULE\u201d link and fill out the following values: Name: jupyter-rule (you can choose any name) Source IP ranges: 0.0.0.0/0 tags cibles: starling (ou another chosen name) Specified protocols and ports: tcp:8888 and leave all the other variables to their default values. This firewall rule allows all incoming traffic (from all IPs) to hit the port 8888. Now go back to the VM page (top left menu > Compute Engine > VM instances), click on your VM name. Make a note of your VM IP address. This is the IP address that you will use in your browser to access your Jupyter environment. Internal IP and External IP\u2026 Make sure the Firewall rules are checked.","title":"Allowing Web Access"},{"location":"Google_Cloud/#jupyter-configuration","text":"In the terminal, run jupyter notebook --generate-config to generate the configuration file. And jupyter notebook password to generate a password. Now edit the configuration file you just created with vim .jupyter/jupyter_notebook_config.py or with nano .jupyter/jupyter_notebook_config.py and add the following line at the top of the file: c.NotebookApp.ip = '*' . (to switch to edit mode in vim, just type the i character). Quit and save with the following sequence ESC : wq . IMPORTANT! This will allow the notebook to be available for all IP addresses on your VM and not just the http://localhost:8888 URL you may be familiar with when working on your local machine.","title":"Jupyter Configuration"},{"location":"Google_Cloud/#launch","text":"1 $ jupyter-notebook --no-browser --port = 8888 In your browser go to the URL: http:// :8888/ or click on the link to access your newly operational Jupyter notebook. Enter the password. You are Jupyter Notebook.","title":"Launch"},{"location":"Google_Cloud/#power","text":"For running classic sklearn or Deep Learning models, bump up the instance\u2019s number of CPUs.","title":"Power"},{"location":"Google_Cloud/#updownload","text":"Download from an instance on gcloud to your local folder ./: 1 gcloud compute scp [instance_name]:[path to file on instance] ./ Upload some local file to your instance: 1 gcloud compute scp [ local file ] . [ instance_name ] : [ path ]","title":"Up/Download"},{"location":"Google_Cloud/#automate","text":"The trick is to use the full path to the python executable which you can find with which python . Then the following cron job will run your script every hour, 10 mn past the hour: 1 10 * * * * [full_path to python] [full path to your script] &gt; [some log file]","title":"Automate"},{"location":"Google_Cloud/#ipython","text":"Jupyter notebooks are very convenient for online collaborative work. But you can also run an IPython session from your terminal simply with the ipython command. That will open an IPython session which has all the bells and whistles of a Jupyter notebook such as magic commands but without the web interface.","title":"iPython"},{"location":"Google_Cloud/#r","text":"It\u2019s easy to set up your VM to be enable R notebooks in your Jupyter console.","title":"R"},{"location":"Google_Cloud/#intermezzo","text":"A few notes on your current setup: The IP address you used in your browser is ephemeral. Which means that every time you restart your VM, your notebooks will have a different URL. You can make that IP static by going to: Top left menu > VPC Network > External IP addresses and select \u201cstatic\u201d in the drop down menu. The security of your current setup relies on the strength of the Jupyter notebook password that you defined previously. Anyone on the internet can access your Jupyter environment at the same URL you use (and bots will absolutely try). One powerful but very unsecure core feature of Jupyter notebooks is that you can launch a terminal with sudo access directly from the notebook. This means that anyone accessing your notebook could take control of your VM after cracking your notebook password and potentially run anything that would send your bills through the roof. The first level measures to prevent that from happening includes. Making sure your Jupyter password is a strong one. Remember to stop your VM when you\u2019re not working on it. You are also currently running the server over http and not https which is not secure enough. Encrypt provides free SSL/TLS certificates and is the encryption solution recommended in the Jupyter documentation .","title":"Intermezzo"},{"location":"HTML and CSS/","text":"Foreword Notes. Option One: Convert \u00b6 Write a document in Word, Markdown and save it in HTML, including a style (no or minimal CSS). Use Pandoc to convert Word, Markdown, and other format to HTML. Option Two: Generate \u00b6 Look for a HTML generator online. Use a PC generator: StaticGen Option Three: Learn and Code \u00b6 Create a webpage with notepad, notepad++ or gedit (among many text editors) using simple HTML and CSS. PDF: Web Page Design . Tutorial: Building your First Web Page . Comprehensive Tutorial: HTML 4.01 Specification . Add CSS \u00b6 Adding CSS . CSS Generators \u00b6 CSS Creator . CSSlayoutgenerator . Uncle Jim\u2019s Web Design . CSS 3.0 Maker . GenerateCSS . HTMLBasix MadSubmitter Web Generator Lists \u00b6 HTML . WebdesignerDepot .","title":"HTML and CSS"},{"location":"HTML and CSS/#option-one-convert","text":"Write a document in Word, Markdown and save it in HTML, including a style (no or minimal CSS). Use Pandoc to convert Word, Markdown, and other format to HTML.","title":"Option One: Convert"},{"location":"HTML and CSS/#option-two-generate","text":"Look for a HTML generator online. Use a PC generator: StaticGen","title":"Option Two: Generate"},{"location":"HTML and CSS/#option-three-learn-and-code","text":"Create a webpage with notepad, notepad++ or gedit (among many text editors) using simple HTML and CSS. PDF: Web Page Design . Tutorial: Building your First Web Page . Comprehensive Tutorial: HTML 4.01 Specification .","title":"Option Three: Learn and Code"},{"location":"HTML and CSS/#add-css","text":"Adding CSS .","title":"Add CSS"},{"location":"HTML and CSS/#css-generators","text":"CSS Creator . CSSlayoutgenerator . Uncle Jim\u2019s Web Design . CSS 3.0 Maker . GenerateCSS . HTMLBasix MadSubmitter","title":"CSS Generators"},{"location":"HTML and CSS/#web-generator-lists","text":"HTML . WebdesignerDepot .","title":"Web Generator Lists"},{"location":"Infographies/","text":"Foreword Notes. An image is worth\u2026 Big \u2013 Data \u2013 Science \u00b6 The rise What? Process Data Science \u00b6 Knowledge base Learning Talent Big Picture Data Scientist Hybrid role\u2026 Cross Over to Data Science Languages and Software \u00b6 Programming Languages Data Software Science \u00b6","title":"Infographies"},{"location":"Infographies/#big-data-science","text":"The rise What? Process","title":"Big -- Data -- Science"},{"location":"Infographies/#data-science","text":"Knowledge base Learning Talent Big Picture Data Scientist Hybrid role\u2026 Cross Over to Data Science","title":"Data Science"},{"location":"Infographies/#languages-and-software","text":"Programming Languages Data Software","title":"Languages and Software"},{"location":"Infographies/#science","text":"","title":"Science"},{"location":"Install_tarball/","text":"Foreword Notes. Install a Tarball on Linux \u00b6 Download the tarball; it can have different extensions: file.tar.gz file.tgz file.tar.bz2 file.tbz2 Unzip the file: CLI: $ tar xvfz sqlite-autoconf-3071502.tar.gz . Double-clicking the triggers an software. Install the tarball on /usr/local : Change directory (where the file is located; however, the installation will be in: /usr/local ). $ cd sqlite-autoconf-3071502 . Open INSTALL or README file for more information (choose). $ vi INSTALL . $ gedit INSTALL . Or another text editor. Configure the software to ensure your system has the necessary functionality and libraries to successfully compile the package. $ ./configure --prefix = /usr/local . Compile all the source files into executable binaries. $ make . Install the binaries and any supporting files into the appropriate locations. $ sudo make install . Run an Executive file on Linux \u00b6 Run a .sh file in the bash. $ bash file.sh .","title":"Install & Run on Linux"},{"location":"Install_tarball/#install-a-tarball-on-linux","text":"Download the tarball; it can have different extensions: file.tar.gz file.tgz file.tar.bz2 file.tbz2 Unzip the file: CLI: $ tar xvfz sqlite-autoconf-3071502.tar.gz . Double-clicking the triggers an software. Install the tarball on /usr/local : Change directory (where the file is located; however, the installation will be in: /usr/local ). $ cd sqlite-autoconf-3071502 . Open INSTALL or README file for more information (choose). $ vi INSTALL . $ gedit INSTALL . Or another text editor. Configure the software to ensure your system has the necessary functionality and libraries to successfully compile the package. $ ./configure --prefix = /usr/local . Compile all the source files into executable binaries. $ make . Install the binaries and any supporting files into the appropriate locations. $ sudo make install .","title":"Install a Tarball on Linux"},{"location":"Install_tarball/#run-an-executive-file-on-linux","text":"Run a .sh file in the bash. $ bash file.sh .","title":"Run an Executive file on Linux"},{"location":"Introduction to SQL JOINs/","text":"Foreword Notes. Using the code \u00b6 Seven different ways you can return data from two relational tables; excluding cross joins and self referencing joins: INNER JOIN LEFT JOIN RIGHT JOIN OUTER JOIN LEFT JOIN excluding INNER JOIN RIGHT JOIN excluding INNER JOIN OUTER JOIN excluding INNER JOIN For the sake of this article, 5, 6, and 7 are LEFT EXCLUDING JOIN , RIGHT Excluding JOIN , and OUTER Excluding JOIN , respectively. Some may argue that 5, 6, and 7 are not really joining the two tables, but for simplicity, let\u2019s refer to these as joins because you use a SQL join in each of these queries (but exclude some records with a WHERE clause). INNER JOIN \u00b6 This is the simplest, most understood join and is the most common. This query will return all of the records in the left table ( Table_A ) that have a matching record in the right table ( Table_B ). This join is written as follows: 1 2 3 4 SELECT < select_list > FROM Table_A A INNER JOIN Table_B B ON A . Key = B . Key LEFT JOIN \u00b6 This query will return all of the records in the left table ( Table_A ) regardless if any of those records have a match in the right table ( Table_B ). It will also return any matching records from the right table. This join is written as follows: 1 2 3 4 SELECT < select_list > FROM Table_A A LEFT JOIN Table_B B ON A . Key = B . Key RIGHT JOIN \u00b6 This query will return all of the records in the right table ( Table_B ) regardless if any of those records have a match in the left table ( Table_A ). It will also return any matching records from the left table. This join is written as follows: 1 2 3 4 SELECT < select_list > FROM Table_A A RIGHT JOIN Table_B B ON A . Key = B . Key OUTER JOIN \u00b6 This Join can also be referred to as a FULL OUTER JOIN or a FULL JOIN . This query will return all of the records from both tables, joining records from the left table ( Table_A ) that match records from the right table ( Table_B ). This join is written as follows: 1 2 3 4 SELECT < select_list > FROM Table_A A FULL OUTER JOIN Table_B B ON A . Key = B . Key LEFT Excluding JOIN \u00b6 This query will return all of the records in the left table ( Table_A ) that do not match any records in the right table ( Table_B ). This join is written as follows: 1 2 3 4 5 SELECT < select_list > FROM Table_A A LEFT JOIN Table_B B ON A . Key = B . Key WHERE B . Key IS NULL RIGHT Excluding JOIN \u00b6 This query will return all of the records in the right table ( Table_B ) that do not match any records in the left table ( Table_A ). This join is written as follows: 1 2 3 4 5 SELECT < select_list > FROM Table_A A RIGHT JOIN Table_B B ON A . Key = B . Key WHERE A . Key IS NULL OUTER Excluding JOIN \u00b6 This query will return all of the records in the left table ( Table_A ) and all of the records in the right table ( Table_B ) that do not match. I have yet to have a need for using this type of join, but all of the others, I use quite frequently. This join is written as follows: 1 2 3 4 5 SELECT < select_list > FROM Table_A A FULL OUTER JOIN Table_B B ON A . Key = B . Key WHERE A . Key IS NULL OR B . Key IS NULL Examples \u00b6 Suppose we have two tables, TABLE_A and TABLE_B . The data in these tables are shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 TABLE_A PK Value ---- ---------- 1 FOX 2 COP 3 TAXI 6 WASHINGTON 7 DELL 5 ARIZONA 4 LINCOLN 10 LUCENT TABLE_B PK Value ---- ---------- 1 TROT 2 CAR 3 CAB 6 MONUMENT 7 PC 8 MICROSOFT 9 APPLE 11 SCOTCH The results of the seven joins are shown below: INNER JOIN 1 2 3 4 5 6 --INNER JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A INNER JOIN Table_B B ON A . PK = B . PK 1 2 3 4 5 6 7 8 9 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- 1 FOX TROT 1 2 COP CAR 2 3 TAXI CAB 3 6 WASHINGTON MONUMENT 6 7 DELL PC 7 ( 5 row ( s ) affected ) LEFT JOIN 1 2 3 4 5 6 --LEFT JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A LEFT JOIN Table_B B ON A . PK = B . PK 1 2 3 4 5 6 7 8 9 10 11 12 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- 1 FOX TROT 1 2 COP CAR 2 3 TAXI CAB 3 4 LINCOLN NULL NULL 5 ARIZONA NULL NULL 6 WASHINGTON MONUMENT 6 7 DELL PC 7 10 LUCENT NULL NULL ( 8 row ( s ) affected ) RIGHT JOIN 1 2 3 4 5 6 --RIGHT JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A RIGHT JOIN Table_B B ON A . PK = B . PK 1 2 3 4 5 6 7 8 9 10 11 12 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- 1 FOX TROT 1 2 COP CAR 2 3 TAXI CAB 3 6 WASHINGTON MONUMENT 6 7 DELL PC 7 NULL NULL MICROSOFT 8 NULL NULL APPLE 9 NULL NULL SCOTCH 11 ( 8 row ( s ) affected ) OUTER JOIN 1 2 3 4 5 6 --OUTER JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A FULL OUTER JOIN Table_B B ON A . PK = B . PK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- 1 FOX TROT 1 2 COP CAR 2 3 TAXI CAB 3 6 WASHINGTON MONUMENT 6 7 DELL PC 7 NULL NULL MICROSOFT 8 NULL NULL APPLE 9 NULL NULL SCOTCH 11 5 ARIZONA NULL NULL 4 LINCOLN NULL NULL 10 LUCENT NULL NULL ( 11 row ( s ) affected ) LEFT Excluding JOIN 1 2 3 4 5 6 7 --LEFT EXCLUDING JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A LEFT JOIN Table_B B ON A . PK = B . PK WHERE B . PK IS NULL 1 2 3 4 5 6 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- 4 LINCOLN NULL NULL 5 ARIZONA NULL NULL 10 LUCENT NULL NULL ( 3 row ( s ) affected ) RIGHT Excluding JOIN 1 2 3 4 5 6 7 - RIGHT EXCLUDING JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A RIGHT JOIN Table_B B ON A . PK = B . PK WHERE A . PK IS NULL 1 2 3 4 5 6 7 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- NULL NULL MICROSOFT 8 NULL NULL APPLE 9 NULL NULL SCOTCH 11 ( 3 row ( s ) affected ) OUTER Excluding JOIN 1 2 3 4 5 6 7 8 --OUTER EXCLUDING JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A FULL OUTER JOIN Table_B B ON A . PK = B . PK WHERE A . PK IS NULL OR B . PK IS NULL 1 2 3 4 5 6 7 8 9 10 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- NULL NULL MICROSOFT 8 NULL NULL APPLE 9 NULL NULL SCOTCH 11 5 ARIZONA NULL NULL 4 LINCOLN NULL NULL 10 LUCENT NULL NULL ( 6 row ( s ) affected ) Conclusion \u00b6 Note on the OUTER JOIN that the inner joined records are returned first, followed by the right joined records, and then finally the left joined records (at least, that\u2019s how my Microsoft SQL Server did it; this, of course, is without using any ORDER BY statement).","title":"Introduction to SQL JOINs"},{"location":"Introduction to SQL JOINs/#using-the-code","text":"Seven different ways you can return data from two relational tables; excluding cross joins and self referencing joins: INNER JOIN LEFT JOIN RIGHT JOIN OUTER JOIN LEFT JOIN excluding INNER JOIN RIGHT JOIN excluding INNER JOIN OUTER JOIN excluding INNER JOIN For the sake of this article, 5, 6, and 7 are LEFT EXCLUDING JOIN , RIGHT Excluding JOIN , and OUTER Excluding JOIN , respectively. Some may argue that 5, 6, and 7 are not really joining the two tables, but for simplicity, let\u2019s refer to these as joins because you use a SQL join in each of these queries (but exclude some records with a WHERE clause).","title":"Using the code"},{"location":"Introduction to SQL JOINs/#inner-join","text":"This is the simplest, most understood join and is the most common. This query will return all of the records in the left table ( Table_A ) that have a matching record in the right table ( Table_B ). This join is written as follows: 1 2 3 4 SELECT < select_list > FROM Table_A A INNER JOIN Table_B B ON A . Key = B . Key","title":"INNER JOIN"},{"location":"Introduction to SQL JOINs/#left-join","text":"This query will return all of the records in the left table ( Table_A ) regardless if any of those records have a match in the right table ( Table_B ). It will also return any matching records from the right table. This join is written as follows: 1 2 3 4 SELECT < select_list > FROM Table_A A LEFT JOIN Table_B B ON A . Key = B . Key","title":"LEFT JOIN"},{"location":"Introduction to SQL JOINs/#right-join","text":"This query will return all of the records in the right table ( Table_B ) regardless if any of those records have a match in the left table ( Table_A ). It will also return any matching records from the left table. This join is written as follows: 1 2 3 4 SELECT < select_list > FROM Table_A A RIGHT JOIN Table_B B ON A . Key = B . Key","title":"RIGHT JOIN"},{"location":"Introduction to SQL JOINs/#outer-join","text":"This Join can also be referred to as a FULL OUTER JOIN or a FULL JOIN . This query will return all of the records from both tables, joining records from the left table ( Table_A ) that match records from the right table ( Table_B ). This join is written as follows: 1 2 3 4 SELECT < select_list > FROM Table_A A FULL OUTER JOIN Table_B B ON A . Key = B . Key","title":"OUTER JOIN"},{"location":"Introduction to SQL JOINs/#left-excluding-join","text":"This query will return all of the records in the left table ( Table_A ) that do not match any records in the right table ( Table_B ). This join is written as follows: 1 2 3 4 5 SELECT < select_list > FROM Table_A A LEFT JOIN Table_B B ON A . Key = B . Key WHERE B . Key IS NULL","title":"LEFT Excluding JOIN"},{"location":"Introduction to SQL JOINs/#right-excluding-join","text":"This query will return all of the records in the right table ( Table_B ) that do not match any records in the left table ( Table_A ). This join is written as follows: 1 2 3 4 5 SELECT < select_list > FROM Table_A A RIGHT JOIN Table_B B ON A . Key = B . Key WHERE A . Key IS NULL","title":"RIGHT Excluding JOIN"},{"location":"Introduction to SQL JOINs/#outer-excluding-join","text":"This query will return all of the records in the left table ( Table_A ) and all of the records in the right table ( Table_B ) that do not match. I have yet to have a need for using this type of join, but all of the others, I use quite frequently. This join is written as follows: 1 2 3 4 5 SELECT < select_list > FROM Table_A A FULL OUTER JOIN Table_B B ON A . Key = B . Key WHERE A . Key IS NULL OR B . Key IS NULL","title":"OUTER Excluding JOIN"},{"location":"Introduction to SQL JOINs/#examples","text":"Suppose we have two tables, TABLE_A and TABLE_B . The data in these tables are shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 TABLE_A PK Value ---- ---------- 1 FOX 2 COP 3 TAXI 6 WASHINGTON 7 DELL 5 ARIZONA 4 LINCOLN 10 LUCENT TABLE_B PK Value ---- ---------- 1 TROT 2 CAR 3 CAB 6 MONUMENT 7 PC 8 MICROSOFT 9 APPLE 11 SCOTCH The results of the seven joins are shown below: INNER JOIN 1 2 3 4 5 6 --INNER JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A INNER JOIN Table_B B ON A . PK = B . PK 1 2 3 4 5 6 7 8 9 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- 1 FOX TROT 1 2 COP CAR 2 3 TAXI CAB 3 6 WASHINGTON MONUMENT 6 7 DELL PC 7 ( 5 row ( s ) affected ) LEFT JOIN 1 2 3 4 5 6 --LEFT JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A LEFT JOIN Table_B B ON A . PK = B . PK 1 2 3 4 5 6 7 8 9 10 11 12 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- 1 FOX TROT 1 2 COP CAR 2 3 TAXI CAB 3 4 LINCOLN NULL NULL 5 ARIZONA NULL NULL 6 WASHINGTON MONUMENT 6 7 DELL PC 7 10 LUCENT NULL NULL ( 8 row ( s ) affected ) RIGHT JOIN 1 2 3 4 5 6 --RIGHT JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A RIGHT JOIN Table_B B ON A . PK = B . PK 1 2 3 4 5 6 7 8 9 10 11 12 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- 1 FOX TROT 1 2 COP CAR 2 3 TAXI CAB 3 6 WASHINGTON MONUMENT 6 7 DELL PC 7 NULL NULL MICROSOFT 8 NULL NULL APPLE 9 NULL NULL SCOTCH 11 ( 8 row ( s ) affected ) OUTER JOIN 1 2 3 4 5 6 --OUTER JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A FULL OUTER JOIN Table_B B ON A . PK = B . PK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- 1 FOX TROT 1 2 COP CAR 2 3 TAXI CAB 3 6 WASHINGTON MONUMENT 6 7 DELL PC 7 NULL NULL MICROSOFT 8 NULL NULL APPLE 9 NULL NULL SCOTCH 11 5 ARIZONA NULL NULL 4 LINCOLN NULL NULL 10 LUCENT NULL NULL ( 11 row ( s ) affected ) LEFT Excluding JOIN 1 2 3 4 5 6 7 --LEFT EXCLUDING JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A LEFT JOIN Table_B B ON A . PK = B . PK WHERE B . PK IS NULL 1 2 3 4 5 6 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- 4 LINCOLN NULL NULL 5 ARIZONA NULL NULL 10 LUCENT NULL NULL ( 3 row ( s ) affected ) RIGHT Excluding JOIN 1 2 3 4 5 6 7 - RIGHT EXCLUDING JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A RIGHT JOIN Table_B B ON A . PK = B . PK WHERE A . PK IS NULL 1 2 3 4 5 6 7 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- NULL NULL MICROSOFT 8 NULL NULL APPLE 9 NULL NULL SCOTCH 11 ( 3 row ( s ) affected ) OUTER Excluding JOIN 1 2 3 4 5 6 7 8 --OUTER EXCLUDING JOIN SELECT A . PK AS A_PK , A . Value AS A_Value , B . Value AS B_Value , B . PK AS B_PK FROM Table_A A FULL OUTER JOIN Table_B B ON A . PK = B . PK WHERE A . PK IS NULL OR B . PK IS NULL 1 2 3 4 5 6 7 8 9 10 A_PK A_Value B_Value B_PK ---- ---------- ---------- ---- NULL NULL MICROSOFT 8 NULL NULL APPLE 9 NULL NULL SCOTCH 11 5 ARIZONA NULL NULL 4 LINCOLN NULL NULL 10 LUCENT NULL NULL ( 6 row ( s ) affected )","title":"Examples"},{"location":"Introduction to SQL JOINs/#conclusion","text":"Note on the OUTER JOIN that the inner joined records are returned first, followed by the right joined records, and then finally the left joined records (at least, that\u2019s how my Microsoft SQL Server did it; this, of course, is without using any ORDER BY statement).","title":"Conclusion"},{"location":"Learn Git/","text":"Foreword Notes. Crash Course \u00b6 Got 15 minutes and want to learn Git? Courses \u00b6 Git - SVN Crash Course . Introduction to Git Extensions . The Beginner\u2019s Guide for GIT Extensions: How to use GIT to clone repository from GitHub and make changes. Managing a repository. Generating SSH Keys as one time activity. How to Clone a Repository? How to open a repository? How to track the changes using Git Extensions? How to perform Commit & Push? Git, Quick Guide . By Tutorialspoint. Git \u00b6 Git, Official Website . Good resources for learning Git and GitHub . References \u00b6 Git Reference . Simple documentation webpage. GitHub Guides (Official) . Understanding the GitHub Flow. Hello World. Contributing to Open Source on GitHub. Getting Started with GitHub Pages. Getting your project on GitHub. Forking Projects. Be Social. Making You Code Citable. Mastering Issues. Mastering Markdown. Documenting your projects on GitHub.","title":"Learn Git"},{"location":"Learn Git/#crash-course","text":"Got 15 minutes and want to learn Git?","title":"Crash Course"},{"location":"Learn Git/#courses","text":"Git - SVN Crash Course . Introduction to Git Extensions . The Beginner\u2019s Guide for GIT Extensions: How to use GIT to clone repository from GitHub and make changes. Managing a repository. Generating SSH Keys as one time activity. How to Clone a Repository? How to open a repository? How to track the changes using Git Extensions? How to perform Commit & Push? Git, Quick Guide . By Tutorialspoint.","title":"Courses"},{"location":"Learn Git/#git","text":"Git, Official Website . Good resources for learning Git and GitHub .","title":"Git"},{"location":"Learn Git/#references","text":"Git Reference . Simple documentation webpage. GitHub Guides (Official) . Understanding the GitHub Flow. Hello World. Contributing to Open Source on GitHub. Getting Started with GitHub Pages. Getting your project on GitHub. Forking Projects. Be Social. Making You Code Citable. Mastering Issues. Mastering Markdown. Documenting your projects on GitHub.","title":"References"},{"location":"Markdown & Pandoc/","text":"Foreword Notes. RMarkdown Documentation \u00b6 RMarkdown . Quicktour . Pandoc, Document Converter \u00b6 Install \u00b6 Pandoc For Windows and Linux, download the packages . Install Pandoc on Ubuntu with haskell Use in a terminal \u00b6 pandoc --version . pandoc --help , man pandoc on Linux. man pandoc_markdown . Pandoc is located in a directory. It is where conversions are done. For example, on Windows: cd D:\\User\\Documents\\Pandoc Snippets","title":"Markdown & Pandoc"},{"location":"Markdown & Pandoc/#rmarkdown-documentation","text":"RMarkdown . Quicktour .","title":"RMarkdown Documentation"},{"location":"Markdown & Pandoc/#pandoc-document-converter","text":"","title":"Pandoc, Document Converter"},{"location":"Markdown & Pandoc/#install","text":"Pandoc For Windows and Linux, download the packages . Install Pandoc on Ubuntu with haskell","title":"Install"},{"location":"Markdown & Pandoc/#use-in-a-terminal","text":"pandoc --version . pandoc --help , man pandoc on Linux. man pandoc_markdown . Pandoc is located in a directory. It is where conversions are done. For example, on Windows: cd D:\\User\\Documents\\Pandoc Snippets","title":"Use in a terminal"},{"location":"OS_CS/","text":"Foreword Cheat sheets. Linux \u00b6 Ubuntu Reference . PDF. Linux Command Reference . PDF. Linux Command Line . PDF. M\u00e9mo Console UNIX . Linux Bash Shell . PDF only. Learning the Shell . bc - An arbitrary precision calculator language . Windows \u00b6 PowerShell Basics . PDF. Windows PowerShell . PDF.","title":"OS Cheat Sheets"},{"location":"OS_CS/#linux","text":"Ubuntu Reference . PDF. Linux Command Reference . PDF. Linux Command Line . PDF. M\u00e9mo Console UNIX . Linux Bash Shell . PDF only. Learning the Shell . bc - An arbitrary precision calculator language .","title":"Linux"},{"location":"OS_CS/#windows","text":"PowerShell Basics . PDF. Windows PowerShell . PDF.","title":"Windows"},{"location":"Resources for Data Science/","text":"Foreword Notes. Data (Open) \u00b6 Donn\u00e9es Qu\u00e9bec . Open Data Canada . Open Data US . Mondo . Data hub. The Dataverse Project . Open source research data repository software. Gapminder . Quandl . High-quality financial and economic data in many formats. Find the Data . FindTheData is a reference site that uses Graphiq\u2019s semantic technology to deliver deep insights via data-driven articles, visualizations and research tools. Knoema . Recherche intelligente avec toutes les statistiques entre vos mains Boardgame Data \u00b6 BGG Data Mining, Data Science Group . BGG Geek Tools . Data Mining, Data Wrangling, and Data Munging \u00b6 R and Data Mining . Links. An Introduction to Data Mining . Intro course. import.io . Extract web data the easy way. The Data Mining Page . Links. ScraPy . Scrape web sites to get information off them. An open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way. Data Visualization and Storytelling \u00b6 Data Visualization with JavaScript . D3.js Gallery . R ggplot2 package . R ggvis package . R shiny package . Demo . Gallery . Gephi . Visualization and exploration software for all kinds of graphs and networks. Gephi is open-source and free. Katy B\u00f6rner . Desislava Hristova . LAB1100.com . Independent research and software development firm. We built high quality applications driven by research questions, educational challenges and cultural encounters. NODEGOAT . ParaView . Open-source, multi-platform data analysis and visualization application. Periscopic . Technology to visualize solutions that engage the public and deliver messages of action. plotly . Platform for agile business intelligence and data science. MicroStrategy.com . Build dashboards. SAP Lumira . Take control and connect to multiple data sources, big and small, such as SAP HANA, SAP Business Warehouse, Excel spreadsheets and more. Weave . Web-based analysis and visualization environment designed to enable visualization of any available data by anyone for any purpose. Blocks (examples) . D3.js gallery. Bokeh . Python package for interactive and web-based data visualization. Callbacks . Panda3D . Panda3D is a game engine, a framework for 3D rendering and game development for Python and C++ programs. Panda3D is Open Source and free for any purpose, including commercial ventures, thanks to its liberal license. Ren\u2019Py . Ren\u2019Py is a visual novel engine used by hundreds of creators from around the world that helps you use words, images, and sounds to tell interactive stories that run on computers and mobile devices. GIS & Mapping \u00b6 A Free and Open Source Geographic Information System . CARTO . CARTO is an open, powerful, and intuitive platform for discovering and predicting the key insights underlying the location data in our world. GUI & Interfaces \u00b6 Kivy . Open source Python library for rapid development of applications that make use of innovative user interfaces, such as multi-touch apps. Image Processing \u00b6 SimpleCV . Computer Vision platform using Python. Making your computer see things in the real world. SimpleCV is an open source framework for building computer vision applications. With it, you get access to several high-powered computer vision libraries, such as OpenCV, without having to first learn about bit depths, file formats, color spaces, buffer management, eigenvalues, or matrix versus bitmap storage. Book : Practical Computer Vision, O\u2019Reilly. Infographics \u00b6 Piktochart . Infographic maker. Canvas . Easy, drag-and-drop infographic creator. Vizualize.me . Create your infographic resume for free. Google Charts . Google chart tools are powerful, simple to use, and free; rich gallery of interactive charts and data tools. easel.ly . Cr\u00e9er et partager des id\u00e9es de visuels . infogr.am . Create and publish beautiful visualizations of your data. Interactive, responsive and engaging. Venngage . verything you need to create and publish infographics is right here. Kids and Coding \u00b6 Codecombat . Multi-language. Educative. Python and web languages for creating games. Lifelong Kindergarten . MIT. Scratch . Introduction to programming. Robotique . Festival. HabiloM\u00e9dias . Digital literacy. ClassCraft . For schools. NLP \u00b6 NLTK . Natural Language Tool Kit for analyzing written text and writing things like spam filters and chat bots. NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. Online Programs, Courses, Lessons, and Tutorials \u00b6 Thinkful . Programs: web development bootcamp, frontend, backend (Flask Python-based frameworks), mobile, design, UX, data science, individual, corporate. Freecodecamp . 4 programs: web development, front-end, back-end, full stack, data visualization. Code School . Paths and courses: web and mobile development, Django Python-based frameworks, git, bash, regex, R, golang. Codecademy Paths and courses: web and mobile development, git, SQL, Java, Python. Coursera . MOOC. Skilledup . eLearning for business. edX . MOOC. video2brain . eLearning. nopaymba . eLearning. Udemy . eLearning. Wiley Online Training . eLearning. Certifications. Springboard . Programs: data science, data analytics, UX design. Udacity . Programs and courses: vr, self-driving car, predictive analytics, Android and iOS development, machine learning, programming, front-end developper, full-stack developer, data analysis. SQL Teaching . SQL lessons. w3scholls . Tutorials: web development. MongoDB University . Courses. Statistics.com . Programs and courses. Cerfications. Tutorialspoint . All topics. Tizag . Coding. Learnshell . 10 languages. Learn Python and\u2026 . 8 other languages. List of Bootcamps . List of Bootcamps . General Assembly . Online Reporting & Publishing \u00b6 RPubs . Write R Markdown documents in RStudio. Share them on RPubs. bl.ocks.org . Simple viewer for sharing code examples hosted on GitHub Gist. Gist GitHub . Code snippets. Bitbucket . GitHub . GitLab . Sourceforge . Parallel and Distributed Computing (Big Data) \u00b6 Bases de donn\u00e9es documentaires et distribu\u00e9es . Parallel computing, NoSQL, XML, MapReduce, distributed computing, indexation, Eleasticsearch, JSON, CouchBase, Pig, Spark, MongoDB, replication, scalability, cloud, virtual machine, markov chaine, page rank, Solr, CouchDB, XQuery, Cloudera . Hadoop ecosystem distribution. Hortonworks . Hadoop ecosystem distribution. Python Web Frameworks \u00b6 Django vs Flask vs Pyramid: Choosing a Python Web Framework . Statistics, Statistical & Machine Learning \u00b6 Cours de programmation sous R . Links, Statistical Textbook . Basic and advanced concepts. An Introduction to Statistical Learning with Applications in R . Cases, codes, datasets. Statistical Consultants . Blog, datasets, cases, codes. Introduction to Stats and Data Analysis . Video courses. Scikit . SciKit-Learn for machine learning applications. Simple and efficient tools for data mining and data analysis. Accessible to everybody, and reusable in various contexts. Built on NumPy, SciPy, and matplotlib. Open source, commercially usable. BSD license. R Documentation . Search all CRAN, BioConductor and Github packages. Virtual Console and Virtual Coding \u00b6 repl.it . Everything you need to teach coding in your classroom. Code in the cloud and interactive environment. 30 languages. Codeanywhere . Cross platform cloud IDE. R-Fiddle . Environment to write, run and share R-code right inside your browser. It even offers the option to include packages. dataiku . A collaborative data science platform Heroku . Platform for building with modern architectures. Innovating quickly and scaling precisely to meet demand. Visualizing and Inspecting the Code \u00b6 Python Tutor . Online; Python, Java, JavaScript, TypeScript, Ruby, C, C++. PyDesk Visualizer . PyDesk Visualizer is the desktop based Python Visualizer. It helps you to visualize Python Code. So one can easily understand execution of program. bl.ocks.org . Simple viewer for sharing code examples hosted on GitHub Gist.","title":"Resources for Data Science"},{"location":"Resources for Data Science/#data-open","text":"Donn\u00e9es Qu\u00e9bec . Open Data Canada . Open Data US . Mondo . Data hub. The Dataverse Project . Open source research data repository software. Gapminder . Quandl . High-quality financial and economic data in many formats. Find the Data . FindTheData is a reference site that uses Graphiq\u2019s semantic technology to deliver deep insights via data-driven articles, visualizations and research tools. Knoema . Recherche intelligente avec toutes les statistiques entre vos mains","title":"Data (Open)"},{"location":"Resources for Data Science/#boardgame-data","text":"BGG Data Mining, Data Science Group . BGG Geek Tools .","title":"Boardgame Data"},{"location":"Resources for Data Science/#data-mining-data-wrangling-and-data-munging","text":"R and Data Mining . Links. An Introduction to Data Mining . Intro course. import.io . Extract web data the easy way. The Data Mining Page . Links. ScraPy . Scrape web sites to get information off them. An open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way.","title":"Data Mining, Data Wrangling, and Data Munging"},{"location":"Resources for Data Science/#data-visualization-and-storytelling","text":"Data Visualization with JavaScript . D3.js Gallery . R ggplot2 package . R ggvis package . R shiny package . Demo . Gallery . Gephi . Visualization and exploration software for all kinds of graphs and networks. Gephi is open-source and free. Katy B\u00f6rner . Desislava Hristova . LAB1100.com . Independent research and software development firm. We built high quality applications driven by research questions, educational challenges and cultural encounters. NODEGOAT . ParaView . Open-source, multi-platform data analysis and visualization application. Periscopic . Technology to visualize solutions that engage the public and deliver messages of action. plotly . Platform for agile business intelligence and data science. MicroStrategy.com . Build dashboards. SAP Lumira . Take control and connect to multiple data sources, big and small, such as SAP HANA, SAP Business Warehouse, Excel spreadsheets and more. Weave . Web-based analysis and visualization environment designed to enable visualization of any available data by anyone for any purpose. Blocks (examples) . D3.js gallery. Bokeh . Python package for interactive and web-based data visualization. Callbacks . Panda3D . Panda3D is a game engine, a framework for 3D rendering and game development for Python and C++ programs. Panda3D is Open Source and free for any purpose, including commercial ventures, thanks to its liberal license. Ren\u2019Py . Ren\u2019Py is a visual novel engine used by hundreds of creators from around the world that helps you use words, images, and sounds to tell interactive stories that run on computers and mobile devices.","title":"Data Visualization and Storytelling"},{"location":"Resources for Data Science/#gis-mapping","text":"A Free and Open Source Geographic Information System . CARTO . CARTO is an open, powerful, and intuitive platform for discovering and predicting the key insights underlying the location data in our world.","title":"GIS &amp; Mapping"},{"location":"Resources for Data Science/#gui-interfaces","text":"Kivy . Open source Python library for rapid development of applications that make use of innovative user interfaces, such as multi-touch apps.","title":"GUI &amp; Interfaces"},{"location":"Resources for Data Science/#image-processing","text":"SimpleCV . Computer Vision platform using Python. Making your computer see things in the real world. SimpleCV is an open source framework for building computer vision applications. With it, you get access to several high-powered computer vision libraries, such as OpenCV, without having to first learn about bit depths, file formats, color spaces, buffer management, eigenvalues, or matrix versus bitmap storage. Book : Practical Computer Vision, O\u2019Reilly.","title":"Image Processing"},{"location":"Resources for Data Science/#infographics","text":"Piktochart . Infographic maker. Canvas . Easy, drag-and-drop infographic creator. Vizualize.me . Create your infographic resume for free. Google Charts . Google chart tools are powerful, simple to use, and free; rich gallery of interactive charts and data tools. easel.ly . Cr\u00e9er et partager des id\u00e9es de visuels . infogr.am . Create and publish beautiful visualizations of your data. Interactive, responsive and engaging. Venngage . verything you need to create and publish infographics is right here.","title":"Infographics"},{"location":"Resources for Data Science/#kids-and-coding","text":"Codecombat . Multi-language. Educative. Python and web languages for creating games. Lifelong Kindergarten . MIT. Scratch . Introduction to programming. Robotique . Festival. HabiloM\u00e9dias . Digital literacy. ClassCraft . For schools.","title":"Kids and Coding"},{"location":"Resources for Data Science/#nlp","text":"NLTK . Natural Language Tool Kit for analyzing written text and writing things like spam filters and chat bots. NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.","title":"NLP"},{"location":"Resources for Data Science/#online-programs-courses-lessons-and-tutorials","text":"Thinkful . Programs: web development bootcamp, frontend, backend (Flask Python-based frameworks), mobile, design, UX, data science, individual, corporate. Freecodecamp . 4 programs: web development, front-end, back-end, full stack, data visualization. Code School . Paths and courses: web and mobile development, Django Python-based frameworks, git, bash, regex, R, golang. Codecademy Paths and courses: web and mobile development, git, SQL, Java, Python. Coursera . MOOC. Skilledup . eLearning for business. edX . MOOC. video2brain . eLearning. nopaymba . eLearning. Udemy . eLearning. Wiley Online Training . eLearning. Certifications. Springboard . Programs: data science, data analytics, UX design. Udacity . Programs and courses: vr, self-driving car, predictive analytics, Android and iOS development, machine learning, programming, front-end developper, full-stack developer, data analysis. SQL Teaching . SQL lessons. w3scholls . Tutorials: web development. MongoDB University . Courses. Statistics.com . Programs and courses. Cerfications. Tutorialspoint . All topics. Tizag . Coding. Learnshell . 10 languages. Learn Python and\u2026 . 8 other languages. List of Bootcamps . List of Bootcamps . General Assembly .","title":"Online Programs, Courses, Lessons, and Tutorials"},{"location":"Resources for Data Science/#online-reporting-publishing","text":"RPubs . Write R Markdown documents in RStudio. Share them on RPubs. bl.ocks.org . Simple viewer for sharing code examples hosted on GitHub Gist. Gist GitHub . Code snippets. Bitbucket . GitHub . GitLab . Sourceforge .","title":"Online Reporting &amp; Publishing"},{"location":"Resources for Data Science/#parallel-and-distributed-computing-big-data","text":"Bases de donn\u00e9es documentaires et distribu\u00e9es . Parallel computing, NoSQL, XML, MapReduce, distributed computing, indexation, Eleasticsearch, JSON, CouchBase, Pig, Spark, MongoDB, replication, scalability, cloud, virtual machine, markov chaine, page rank, Solr, CouchDB, XQuery, Cloudera . Hadoop ecosystem distribution. Hortonworks . Hadoop ecosystem distribution.","title":"Parallel and Distributed Computing (Big Data)"},{"location":"Resources for Data Science/#python-web-frameworks","text":"Django vs Flask vs Pyramid: Choosing a Python Web Framework .","title":"Python Web Frameworks"},{"location":"Resources for Data Science/#statistics-statistical-machine-learning","text":"Cours de programmation sous R . Links, Statistical Textbook . Basic and advanced concepts. An Introduction to Statistical Learning with Applications in R . Cases, codes, datasets. Statistical Consultants . Blog, datasets, cases, codes. Introduction to Stats and Data Analysis . Video courses. Scikit . SciKit-Learn for machine learning applications. Simple and efficient tools for data mining and data analysis. Accessible to everybody, and reusable in various contexts. Built on NumPy, SciPy, and matplotlib. Open source, commercially usable. BSD license. R Documentation . Search all CRAN, BioConductor and Github packages.","title":"Statistics, Statistical &amp; Machine Learning"},{"location":"Resources for Data Science/#virtual-console-and-virtual-coding","text":"repl.it . Everything you need to teach coding in your classroom. Code in the cloud and interactive environment. 30 languages. Codeanywhere . Cross platform cloud IDE. R-Fiddle . Environment to write, run and share R-code right inside your browser. It even offers the option to include packages. dataiku . A collaborative data science platform Heroku . Platform for building with modern architectures. Innovating quickly and scaling precisely to meet demand.","title":"Virtual Console and Virtual Coding"},{"location":"Resources for Data Science/#visualizing-and-inspecting-the-code","text":"Python Tutor . Online; Python, Java, JavaScript, TypeScript, Ruby, C, C++. PyDesk Visualizer . PyDesk Visualizer is the desktop based Python Visualizer. It helps you to visualize Python Code. So one can easily understand execution of program. bl.ocks.org . Simple viewer for sharing code examples hosted on GitHub Gist.","title":"Visualizing and Inspecting the Code"},{"location":"Responsive_HTML/","text":"Foreword Notes. Initial grid looks \u00b6 Here is the HTML: 1 2 3 4 5 6 7 8 < div class = \"container\" > < div > 1 </ div > < div > 2 </ div > < div > 3 </ div > < div > 4 </ div > < div > 5 </ div > < div > 6 </ div > </ div > And the CSS: 1 2 3 4 5 . container { display : grid ; grid-template-columns : 100 px 100 px 100 px ; grid-template-rows : 50 px 50 px ; } Basic responsiveness with the fraction unit \u00b6 CSS Grid brings with it a whole new value called a fraction unit: written like fr . It splits the container into as many fractions. 1 2 3 4 5 . container { display : grid ; grid-template-columns : 1 fr 1 fr 1 fr ; grid-template-rows : 50 px 50 px ; } If we change the grid-template-columns value to 1fr 2fr 1fr , the second column will now be twice as wide as the two other columns: Advanced responsiveness \u00b6 repeat() \u00b6 highlight line 3 1 2 3 4 5 . container { display : grid ; grid-template-columns : repeat ( 3 , 100 px ); grid-template-rows : repeat ( 2 , 50 px ); } In other words, repeat(3, 100px) is identical to 100px 100px 100px : auto-fit \u00b6 highlight line 4 1 2 3 4 5 6 . container { display : grid ; grid-gap : 5 px ; grid-template-columns : repeat ( auto - fit , 100 px ); grid-template-rows : repeat ( 2 , 100 px ); } The grid now varies the amount of columns with the width of the container. minmax() \u00b6 highlight line 4 1 2 3 4 5 6 . container { display : grid ; grid-gap : 5 px ; grid-template-columns : repeat ( auto - fit , minmax ( 100 px , 1 fr )); grid-template-rows : repeat ( 2 , 100 px ); } The minmax() function defines a size range greater than or equal to min and less than or equal to max. So the columns will now always be at least 100px. However if there are more available space, the grid will simply distribute this equally to each of the columns, as the columns turn into a fraction unit instead of 100 px. Adding the images \u00b6 Now the final step is to add the images. 1 < div >< img src = \"img/forest.jpg\" /></ div > To make the image fit into the item, we\u2019ll set the it to be as wide and tall as the item itself, and then use object-fit : cover ; . This will make the image cover its entire container, and the browser will crop it if it\u2019s needed. 1 2 3 4 5 . container > div > img { width : 100 % ; height : 100 % ; object-fit : cover ; }","title":"Responsive HTML"},{"location":"Responsive_HTML/#initial-grid-looks","text":"Here is the HTML: 1 2 3 4 5 6 7 8 < div class = \"container\" > < div > 1 </ div > < div > 2 </ div > < div > 3 </ div > < div > 4 </ div > < div > 5 </ div > < div > 6 </ div > </ div > And the CSS: 1 2 3 4 5 . container { display : grid ; grid-template-columns : 100 px 100 px 100 px ; grid-template-rows : 50 px 50 px ; }","title":"Initial grid looks"},{"location":"Responsive_HTML/#basic-responsiveness-with-the-fraction-unit","text":"CSS Grid brings with it a whole new value called a fraction unit: written like fr . It splits the container into as many fractions. 1 2 3 4 5 . container { display : grid ; grid-template-columns : 1 fr 1 fr 1 fr ; grid-template-rows : 50 px 50 px ; } If we change the grid-template-columns value to 1fr 2fr 1fr , the second column will now be twice as wide as the two other columns:","title":"Basic responsiveness with the fraction unit"},{"location":"Responsive_HTML/#advanced-responsiveness","text":"","title":"Advanced responsiveness"},{"location":"Responsive_HTML/#repeat","text":"highlight line 3 1 2 3 4 5 . container { display : grid ; grid-template-columns : repeat ( 3 , 100 px ); grid-template-rows : repeat ( 2 , 50 px ); } In other words, repeat(3, 100px) is identical to 100px 100px 100px :","title":"repeat()"},{"location":"Responsive_HTML/#auto-fit","text":"highlight line 4 1 2 3 4 5 6 . container { display : grid ; grid-gap : 5 px ; grid-template-columns : repeat ( auto - fit , 100 px ); grid-template-rows : repeat ( 2 , 100 px ); } The grid now varies the amount of columns with the width of the container.","title":"auto-fit"},{"location":"Responsive_HTML/#minmax","text":"highlight line 4 1 2 3 4 5 6 . container { display : grid ; grid-gap : 5 px ; grid-template-columns : repeat ( auto - fit , minmax ( 100 px , 1 fr )); grid-template-rows : repeat ( 2 , 100 px ); } The minmax() function defines a size range greater than or equal to min and less than or equal to max. So the columns will now always be at least 100px. However if there are more available space, the grid will simply distribute this equally to each of the columns, as the columns turn into a fraction unit instead of 100 px.","title":"minmax()"},{"location":"Responsive_HTML/#adding-the-images","text":"Now the final step is to add the images. 1 < div >< img src = \"img/forest.jpg\" /></ div > To make the image fit into the item, we\u2019ll set the it to be as wide and tall as the item itself, and then use object-fit : cover ; . This will make the image cover its entire container, and the browser will crop it if it\u2019s needed. 1 2 3 4 5 . container > div > img { width : 100 % ; height : 100 % ; object-fit : cover ; }","title":"Adding the images"},{"location":"SQL-NoSQL_CS/","text":"Foreword Cheat sheets. SQL \u00b6 SQL . PDF. See the notes below . SQL . PDF. SQL notes \u00b6 Basic Queries \u00b6 1 SELECT col1 , col2 , col3 , ... FROM table1 ; Commands are not case-sensitive: SELECT or select ; ; is not required; Dividing an integer by an integer gives an integer; use floats with decimals. Add conditions \u00b6 1 2 SELECT col1 FROM table1 WHERE ( col4 = \"A\" ) AND ( col5 = \"B\" OR col6 = \"C\" ); 1 2 SELECT col1 FROM table1 WHERE ( col4 BETWEEN 10 AND 20 ); < > = <> <= >= AND OR . Filter out missing values (or non-missing values) \u00b6 1 2 SELECT col1 FROM table1 WHERE col4 IS NULL ; 1 2 SELECT col1 FROM table1 WHERE col4 IS NOT NULL ; Set limits \u00b6 1 2 3 SELECT col1 FROM table1 WHERE ( col4 BETWEEN 10 AND 20 ) LIMIT 3 ; BETWEEN is inclusive (like <= and >= ); Find patterns \u00b6 1 2 SELECT col1 FROM table1 WHERE col4 LIKE 'a%' ; regex patterns like 'a%' for \u201cbegins with a\u201d, \"%B\" for \u201cends with capital B\u201d; % is a joker. 1 2 SELECT col1 FROM table1 WHERE col4 IN ( 'Germany' , \"France\" , 'UK' ); IN subgroups, limited to a set. Command hierarchy \u00b6 1 2 3 4 5 6 SELECT col1 , col2 , col3 , ... FROM table1 WHERE col4 = 1 AND col5 = 2 LIMIT 3 GROUP by ... HAVING count ( * ) > 1 ORDER BY col2 , col3 DESC ; DESC or ascending by default. Aggregate (two ways) \u00b6 Opening aggregation: 1 SELECT COUNT ( col1 ) FROM table1 ; COUNT (not missing or not null values), SUM , AVG , MIN , MAX . Closing aggregation: 1 2 SELECT col1 FROM table1 HAVING count ( col2 ) > 1 ; - count , sum , avg , min , max . Unique values \u00b6 1 SELECT DISTINCT col2 FROM table1 ; 1 SELECT COUNT ( col1 ) COUNT ( DISTINCT col2 ) FROM table1 ; Compute, transform, create new columns \u00b6 1 SELECT col1 + col2 AS col1b FROM table1 ; + - * / . Add views \u00b6 1 2 CREATE VIEW view1 AS V1 SELECT col1 FROM table1 ; Aliases for the headers. SQLite \u00b6 SQLite . PDF. MySQL \u00b6 MySQL . PDF. Essential MySQL . PDF. Essential Admin for MySQL . PDF only. PostgreSQL \u00b6 PostgreSQL . PDF. PostgreSQL Interactive Terminal Commands . PDF. Essential PostgreSQL . PDF only. NoSQL \u00b6 NoSQL and Data Scalability . PDF only. MongoDB . PDF only.","title":"SQL-NoSQL Cheat Sheets"},{"location":"SQL-NoSQL_CS/#sql","text":"SQL . PDF. See the notes below . SQL . PDF.","title":"SQL"},{"location":"SQL-NoSQL_CS/#sql-notes","text":"","title":"SQL notes"},{"location":"SQL-NoSQL_CS/#basic-queries","text":"1 SELECT col1 , col2 , col3 , ... FROM table1 ; Commands are not case-sensitive: SELECT or select ; ; is not required; Dividing an integer by an integer gives an integer; use floats with decimals.","title":"Basic Queries"},{"location":"SQL-NoSQL_CS/#add-conditions","text":"1 2 SELECT col1 FROM table1 WHERE ( col4 = \"A\" ) AND ( col5 = \"B\" OR col6 = \"C\" ); 1 2 SELECT col1 FROM table1 WHERE ( col4 BETWEEN 10 AND 20 ); < > = <> <= >= AND OR .","title":"Add conditions"},{"location":"SQL-NoSQL_CS/#filter-out-missing-values-or-non-missing-values","text":"1 2 SELECT col1 FROM table1 WHERE col4 IS NULL ; 1 2 SELECT col1 FROM table1 WHERE col4 IS NOT NULL ;","title":"Filter out missing values (or non-missing values)"},{"location":"SQL-NoSQL_CS/#set-limits","text":"1 2 3 SELECT col1 FROM table1 WHERE ( col4 BETWEEN 10 AND 20 ) LIMIT 3 ; BETWEEN is inclusive (like <= and >= );","title":"Set limits"},{"location":"SQL-NoSQL_CS/#find-patterns","text":"1 2 SELECT col1 FROM table1 WHERE col4 LIKE 'a%' ; regex patterns like 'a%' for \u201cbegins with a\u201d, \"%B\" for \u201cends with capital B\u201d; % is a joker. 1 2 SELECT col1 FROM table1 WHERE col4 IN ( 'Germany' , \"France\" , 'UK' ); IN subgroups, limited to a set.","title":"Find patterns"},{"location":"SQL-NoSQL_CS/#command-hierarchy","text":"1 2 3 4 5 6 SELECT col1 , col2 , col3 , ... FROM table1 WHERE col4 = 1 AND col5 = 2 LIMIT 3 GROUP by ... HAVING count ( * ) > 1 ORDER BY col2 , col3 DESC ; DESC or ascending by default.","title":"Command hierarchy"},{"location":"SQL-NoSQL_CS/#aggregate-two-ways","text":"Opening aggregation: 1 SELECT COUNT ( col1 ) FROM table1 ; COUNT (not missing or not null values), SUM , AVG , MIN , MAX . Closing aggregation: 1 2 SELECT col1 FROM table1 HAVING count ( col2 ) > 1 ; - count , sum , avg , min , max .","title":"Aggregate (two ways)"},{"location":"SQL-NoSQL_CS/#unique-values","text":"1 SELECT DISTINCT col2 FROM table1 ; 1 SELECT COUNT ( col1 ) COUNT ( DISTINCT col2 ) FROM table1 ;","title":"Unique values"},{"location":"SQL-NoSQL_CS/#compute-transform-create-new-columns","text":"1 SELECT col1 + col2 AS col1b FROM table1 ; + - * / .","title":"Compute, transform, create new columns"},{"location":"SQL-NoSQL_CS/#add-views","text":"1 2 CREATE VIEW view1 AS V1 SELECT col1 FROM table1 ; Aliases for the headers.","title":"Add views"},{"location":"SQL-NoSQL_CS/#sqlite","text":"SQLite . PDF.","title":"SQLite"},{"location":"SQL-NoSQL_CS/#mysql","text":"MySQL . PDF. Essential MySQL . PDF. Essential Admin for MySQL . PDF only.","title":"MySQL"},{"location":"SQL-NoSQL_CS/#postgresql","text":"PostgreSQL . PDF. PostgreSQL Interactive Terminal Commands . PDF. Essential PostgreSQL . PDF only.","title":"PostgreSQL"},{"location":"SQL-NoSQL_CS/#nosql","text":"NoSQL and Data Scalability . PDF only. MongoDB . PDF only.","title":"NoSQL"},{"location":"Storytelling with Data/","text":"Foreword Notes. The importance of context \u00b6 Exploratory vs explanatory graphics. Exploratory is for the analyst. Exploratory is for the audience. Who is the audience, who is the stakeholder? What do you need your audience to know or do? How will you communicate to your audience? Live or written by email? What tone do you want your communication to set? What data is available that will help make your point? If you had 3 minutes to tell a story, what would be the big idea : the unique point of view in one sentence? Let\u2019s now move to storyboarding. Choosing an effective visual \u00b6 Go with a simple text or a figure. Available charts: scatterplot, table, lines, slopegraph, vertical bars, horizontal bars, stacked vertical bars, stacked horizontal bars, waterfall, square areas. Table? Never in a live presentation (too dense). Heatmap? Never! A better alternative is a slopegraph. Color saturation is for visual contrast: put everything in B&W except the one thing you want to draw attention on. Speak in relative, not in absolute: scatterplot, average, percentage, waterfall (change, increment), areas for comparing categories. Avoid clutter and 3D, avoid saturated series and secondary axes. Prefer a zero baseline and equal scales among several graphics. Prefer square forms over round forms. Avoid pie charts or donut charts. Prefer a horizontal bar chart. Clutter is your enemy ! \u00b6 Avoid cognitive load: a simple chart, a few line, one image. Avoid borders (limited to the L axes), avoid gridlines and background shading. The y-axis is often facultative. Clean up axis labels; summarize labels (Jan vs January) Bold and colors is for making the important stand out. Limit to 2 or 3 colors. Add white space. Add details to the important areas (annotate). Add the legend on the graph, not outside. Focus your audience\u2019s attention \u00b6 Use preattentive attributes in text : bold, italics, separate spatially, underlined, more tone or color (B&W), data stand out, numbered data, size. Avoid shades of red and green. Use preattentive attributes in visual : difference in orientation, shape, length, width, size, curvature, added marks, enclosure, hue, intensity, spatial position, motion Position on the page follows the 1-2-3-4 loop. Consult: Cultural Color Connotations, David McCandless. The Visual Miscellaneum : A Colourful Guide to the World\u2019s Most Consequential Trivia. Think like a designer \u00b6 Eliminate distractions. Not all data are equally important. When detail is not needed, summarize. Would eliminating \u2018this\u2019 change anything? Push necessary, but non-message-impacting items to the background. The power of subcategories, subdivisions. Legible, clean, straightforward language. Action titles. Lessons in storytelling \u00b6 Write a story with a schema (in 3 acts). Write a narrative structure (the hero\u2019s journey). Use the power of repetition: 3, 7, 12, 30, 40, 100, 500, 1000. Final thoughts \u00b6 Learn your tools well. Google Spreadsheet, Tableau, R, D3 (JavaScript), Processing, Python, etc. Iterate. Devote time to storytelling with data. Seek inspiration throught good examples. Find your style. Consult websites. Eagereyes, Robert Kosara . FiveThirdyEight\u2019s Data Lab . Flowing Data, Nathan Yau . The Functional Art, Alberto Cairo . The Gardian Data Blog . HelpMeViz, Jon Schwabish . Junk Carts, Kaiser Fung . Make a Powerful Point, Gavin McMahon . Perceptual Edge, Stephen Few . Visualising Data, Andy Kirk . VizWiz, Andy Kriebel . Storytelling with data, Cole Nussbaumer Knaflic . and many more. Wrap up \u00b6 Understand the context. Choose an appropriate visual display. Eliminate clutter. Focus attention where you want it. Think like a designer. Tell a story.","title":"Storytelling with Data"},{"location":"Storytelling with Data/#the-importance-of-context","text":"Exploratory vs explanatory graphics. Exploratory is for the analyst. Exploratory is for the audience. Who is the audience, who is the stakeholder? What do you need your audience to know or do? How will you communicate to your audience? Live or written by email? What tone do you want your communication to set? What data is available that will help make your point? If you had 3 minutes to tell a story, what would be the big idea : the unique point of view in one sentence? Let\u2019s now move to storyboarding.","title":"The importance of context"},{"location":"Storytelling with Data/#choosing-an-effective-visual","text":"Go with a simple text or a figure. Available charts: scatterplot, table, lines, slopegraph, vertical bars, horizontal bars, stacked vertical bars, stacked horizontal bars, waterfall, square areas. Table? Never in a live presentation (too dense). Heatmap? Never! A better alternative is a slopegraph. Color saturation is for visual contrast: put everything in B&W except the one thing you want to draw attention on. Speak in relative, not in absolute: scatterplot, average, percentage, waterfall (change, increment), areas for comparing categories. Avoid clutter and 3D, avoid saturated series and secondary axes. Prefer a zero baseline and equal scales among several graphics. Prefer square forms over round forms. Avoid pie charts or donut charts. Prefer a horizontal bar chart.","title":"Choosing an effective visual"},{"location":"Storytelling with Data/#clutter-is-your-enemy","text":"Avoid cognitive load: a simple chart, a few line, one image. Avoid borders (limited to the L axes), avoid gridlines and background shading. The y-axis is often facultative. Clean up axis labels; summarize labels (Jan vs January) Bold and colors is for making the important stand out. Limit to 2 or 3 colors. Add white space. Add details to the important areas (annotate). Add the legend on the graph, not outside.","title":"Clutter is your enemy !"},{"location":"Storytelling with Data/#focus-your-audiences-attention","text":"Use preattentive attributes in text : bold, italics, separate spatially, underlined, more tone or color (B&W), data stand out, numbered data, size. Avoid shades of red and green. Use preattentive attributes in visual : difference in orientation, shape, length, width, size, curvature, added marks, enclosure, hue, intensity, spatial position, motion Position on the page follows the 1-2-3-4 loop. Consult: Cultural Color Connotations, David McCandless. The Visual Miscellaneum : A Colourful Guide to the World\u2019s Most Consequential Trivia.","title":"Focus your audience\u2019s attention"},{"location":"Storytelling with Data/#think-like-a-designer","text":"Eliminate distractions. Not all data are equally important. When detail is not needed, summarize. Would eliminating \u2018this\u2019 change anything? Push necessary, but non-message-impacting items to the background. The power of subcategories, subdivisions. Legible, clean, straightforward language. Action titles.","title":"Think like a designer"},{"location":"Storytelling with Data/#lessons-in-storytelling","text":"Write a story with a schema (in 3 acts). Write a narrative structure (the hero\u2019s journey). Use the power of repetition: 3, 7, 12, 30, 40, 100, 500, 1000.","title":"Lessons in storytelling"},{"location":"Storytelling with Data/#final-thoughts","text":"Learn your tools well. Google Spreadsheet, Tableau, R, D3 (JavaScript), Processing, Python, etc. Iterate. Devote time to storytelling with data. Seek inspiration throught good examples. Find your style. Consult websites. Eagereyes, Robert Kosara . FiveThirdyEight\u2019s Data Lab . Flowing Data, Nathan Yau . The Functional Art, Alberto Cairo . The Gardian Data Blog . HelpMeViz, Jon Schwabish . Junk Carts, Kaiser Fung . Make a Powerful Point, Gavin McMahon . Perceptual Edge, Stephen Few . Visualising Data, Andy Kirk . VizWiz, Andy Kriebel . Storytelling with data, Cole Nussbaumer Knaflic . and many more.","title":"Final thoughts"},{"location":"Storytelling with Data/#wrap-up","text":"Understand the context. Choose an appropriate visual display. Eliminate clutter. Focus attention where you want it. Think like a designer. Tell a story.","title":"Wrap up"},{"location":"The Linux Command Line/","text":"Foreword Notes. The 537-page volume covers the same material as LinuxCommand.org , but in much greater detail. In addition to the basics of command line use and shell scripting, The Linux Command Line includes chapters on many common programs used on the command line, as well as more advanced topics. 1.0 Learning the Shell \u00b6 1.1 What Is \u201cThe Shell\u201d? , terminal 1.2 Navigation , file, system, organization, working directory 1.3 Looking Around , list, file, size, group, owner, permission, create text file, long format, classify, examine 1.4 A Guided Tour , directory, root, boot, etc, bin, usr, local, var, lib, home, root, tmp, dev, proc, media 1.5 Manipulating Files , copy, move, remove, create directory 1.6 Working With Commands , type, display information, which, locate, help, command, manual 1.7 I/O Redirection , input, output, pipeline, pipe, filter, sort, uniq, pattern, read text, grep, fmt, split, page break, header, footer, pr, first line, last line, head, tail, translate, tr, editor, sed, awk 1.8 Expansion , echo, pathname, quote, double-quote, escape, backslash 1.9 Permissions , permission, chmod, su, sudo, chown, chgrp, directory 1.10 Job Control , job, process, ps, kill, jobs, background, bg, foreground, fg 2.0 Writing Shell Scripts \u00b6 2.1 Writing Your First Script And Getting It To Work , editor, vi, vim, emacs, nano, gedit, kwrite, path 2.2 Editing The Scripts You Already Have , edit, shell, location, path, export path, alias, environmental variable, today 2.3 Here Scripts , script, here, shebang, output, append 2.4 Variables , script, variable, create, environment 2.5 Command Substitution And Constants , command substitution, constant, assigning, assign 2.6 Shell Functions , shell function, open page, head section, right title, close, body section, timestamp, right, time, macro, 2.7 Some Real Work , uptime, time, show_uptime, drive, space, drive_space, home, space, home_space 2.8 Flow Control - Part 1 , if, test, testing, exit, clear 2.9 Stay Out Of Trouble , empty variable, missing quote, isolating, isolate, run script, watch 2.10 Keyboard Input And Arithmetic , read, arithmetic 2.11 Flow Control - Part 2 , branch, branching, case, loop, while, building an menu 2.12 Positional Parameters , positional parameters, command line, arguments, options, command line processor into script, interactive 2.13 Flow Control - Part 3 , loop, for, if, else, find, wc, system_info 2.14 Errors And Signals And Traps (Oh My!) - Part 1 , check, checking, exit status, error, and, or 2.15 Errors And Signals And Traps (Oh My!) - Part 2 , clean, cleaning, trap, kill, clean_up, temporary, file 3.0 Resources (some) \u00b6 Adventures \u00b6 3.1 Midnight Commander A directory browser and file manager for command line users. gui, file, directory, manipulation, copy, move, rename, delete, permission, remote system, ftp, ssh, .tar, .zip, local, hotlist, name, content 3.2 Terminal Multiplexers Give your terminal some serious muscle. like gnome, konsole, gnu, desktop, multiple shell session single terminal, display, several computer 3.3 Less Typing Fingers getting tired? Let\u2019s look at ways to save our digits! alias, shell functions, script, program, alias, cli, editor, control 3.4 More Redirection We take a deeper look at this powerful feature. i/o, input, output, pipeline, pipe, exec 3.5 tput Our scripts can have more visual appeal! tput, ncurses, terminal, manipulate, change, color, effect, text 3.6 dialog Let\u2019s give our scripts a better user interface. incorporate, script, build list, calendar, checklist, directory, box, form, file select, gauge, pause, gui, simple, simplify 3.7 AWK Pattern scanning and text processing language. One of the truly classic Unix tools. awk, programming language, coding, code, script 3.8 Other Shells While most Linux users rely on the bash shell every day, it\u2019s not the only game in town. Let\u2019s look at some of the others. dash, debian almquist shell, tcsh, texex c shell, ksh, korn shell, zsh, z shell Shell Scripts \u00b6 3.9 new_script A bash shell script template generator. Use this script to help write your own shell scripts. Generated templates include useful shell functions, error and signal handling, and command-line option and argument parsing. script, example, case 3.10 my_cloud Implements a primitive cloud storage system using any available remote host running an SSH server. script, example, case, dropbox 3.11 photo2mail Re-sizes large image files (photos) for use as attachments to email messages, blog postings, etc. script, example, case, read image file, convert, size 3.12 program_list Creates an annotated list of programs in a directory. Useful for exploring your system. script, example, case, program_list, list, whatis, what Manuals \u00b6 Granneman, Scott, Linux Phrasebook, 2015. Parker, Steve, Shell Scripting: Expert Recipes for Linux, Bash and more, 2011. Ward, Brian, How Linux Works: What Every Superuser Should Know, 2014.","title":"The Linux Command Line"},{"location":"The Linux Command Line/#10-learning-the-shell","text":"1.1 What Is \u201cThe Shell\u201d? , terminal 1.2 Navigation , file, system, organization, working directory 1.3 Looking Around , list, file, size, group, owner, permission, create text file, long format, classify, examine 1.4 A Guided Tour , directory, root, boot, etc, bin, usr, local, var, lib, home, root, tmp, dev, proc, media 1.5 Manipulating Files , copy, move, remove, create directory 1.6 Working With Commands , type, display information, which, locate, help, command, manual 1.7 I/O Redirection , input, output, pipeline, pipe, filter, sort, uniq, pattern, read text, grep, fmt, split, page break, header, footer, pr, first line, last line, head, tail, translate, tr, editor, sed, awk 1.8 Expansion , echo, pathname, quote, double-quote, escape, backslash 1.9 Permissions , permission, chmod, su, sudo, chown, chgrp, directory 1.10 Job Control , job, process, ps, kill, jobs, background, bg, foreground, fg","title":"1.0 Learning the Shell"},{"location":"The Linux Command Line/#20-writing-shell-scripts","text":"2.1 Writing Your First Script And Getting It To Work , editor, vi, vim, emacs, nano, gedit, kwrite, path 2.2 Editing The Scripts You Already Have , edit, shell, location, path, export path, alias, environmental variable, today 2.3 Here Scripts , script, here, shebang, output, append 2.4 Variables , script, variable, create, environment 2.5 Command Substitution And Constants , command substitution, constant, assigning, assign 2.6 Shell Functions , shell function, open page, head section, right title, close, body section, timestamp, right, time, macro, 2.7 Some Real Work , uptime, time, show_uptime, drive, space, drive_space, home, space, home_space 2.8 Flow Control - Part 1 , if, test, testing, exit, clear 2.9 Stay Out Of Trouble , empty variable, missing quote, isolating, isolate, run script, watch 2.10 Keyboard Input And Arithmetic , read, arithmetic 2.11 Flow Control - Part 2 , branch, branching, case, loop, while, building an menu 2.12 Positional Parameters , positional parameters, command line, arguments, options, command line processor into script, interactive 2.13 Flow Control - Part 3 , loop, for, if, else, find, wc, system_info 2.14 Errors And Signals And Traps (Oh My!) - Part 1 , check, checking, exit status, error, and, or 2.15 Errors And Signals And Traps (Oh My!) - Part 2 , clean, cleaning, trap, kill, clean_up, temporary, file","title":"2.0 Writing Shell Scripts"},{"location":"The Linux Command Line/#30-resources-some","text":"","title":"3.0 Resources (some)"},{"location":"The Linux Command Line/#adventures","text":"3.1 Midnight Commander A directory browser and file manager for command line users. gui, file, directory, manipulation, copy, move, rename, delete, permission, remote system, ftp, ssh, .tar, .zip, local, hotlist, name, content 3.2 Terminal Multiplexers Give your terminal some serious muscle. like gnome, konsole, gnu, desktop, multiple shell session single terminal, display, several computer 3.3 Less Typing Fingers getting tired? Let\u2019s look at ways to save our digits! alias, shell functions, script, program, alias, cli, editor, control 3.4 More Redirection We take a deeper look at this powerful feature. i/o, input, output, pipeline, pipe, exec 3.5 tput Our scripts can have more visual appeal! tput, ncurses, terminal, manipulate, change, color, effect, text 3.6 dialog Let\u2019s give our scripts a better user interface. incorporate, script, build list, calendar, checklist, directory, box, form, file select, gauge, pause, gui, simple, simplify 3.7 AWK Pattern scanning and text processing language. One of the truly classic Unix tools. awk, programming language, coding, code, script 3.8 Other Shells While most Linux users rely on the bash shell every day, it\u2019s not the only game in town. Let\u2019s look at some of the others. dash, debian almquist shell, tcsh, texex c shell, ksh, korn shell, zsh, z shell","title":"Adventures"},{"location":"The Linux Command Line/#shell-scripts","text":"3.9 new_script A bash shell script template generator. Use this script to help write your own shell scripts. Generated templates include useful shell functions, error and signal handling, and command-line option and argument parsing. script, example, case 3.10 my_cloud Implements a primitive cloud storage system using any available remote host running an SSH server. script, example, case, dropbox 3.11 photo2mail Re-sizes large image files (photos) for use as attachments to email messages, blog postings, etc. script, example, case, read image file, convert, size 3.12 program_list Creates an annotated list of programs in a directory. Useful for exploring your system. script, example, case, program_list, list, whatis, what","title":"Shell Scripts"},{"location":"The Linux Command Line/#manuals","text":"Granneman, Scott, Linux Phrasebook, 2015. Parker, Steve, Shell Scripting: Expert Recipes for Linux, Bash and more, 2011. Ward, Brian, How Linux Works: What Every Superuser Should Know, 2014.","title":"Manuals"},{"location":"Ubuntu/","text":"Foreword Notes. Install Ubuntu 16.04 \u00b6 Ubuntu . Following the Installation (check list) \u00b6 VirtualBox guest additions . (for a virtual installation). Tweaks . Move launcher to bottom. Remove guest session. Show/Enable UserName on Unity app panel. Stylish your desktop. Enable one-click minimize. Install Linux graphics drivers. Install media codecs. Install punch of apps. Install more browsers. Install other desktop environments. Install Flash player. Install openJDK. Install additional softwares. Install Synaptic Package Manager. Install Java& openJDK. Set up your online accounts. Install BleachBit (system cleaner). \u2026 Get started . Things to do after installing . Tweaks to do after installing . Install software\u2026","title":"Ubuntu 16.04"},{"location":"Ubuntu/#install-ubuntu-1604","text":"Ubuntu .","title":"Install Ubuntu 16.04"},{"location":"Ubuntu/#following-the-installation-check-list","text":"VirtualBox guest additions . (for a virtual installation). Tweaks . Move launcher to bottom. Remove guest session. Show/Enable UserName on Unity app panel. Stylish your desktop. Enable one-click minimize. Install Linux graphics drivers. Install media codecs. Install punch of apps. Install more browsers. Install other desktop environments. Install Flash player. Install openJDK. Install additional softwares. Install Synaptic Package Manager. Install Java& openJDK. Set up your online accounts. Install BleachBit (system cleaner). \u2026 Get started . Things to do after installing . Tweaks to do after installing . Install software\u2026","title":"Following the Installation (check list)"},{"location":"Usefull+Shell+Commands+for+Data+Science/","text":"Foreword Code snippets and excerpts from the tutorial. bash. From DataCamp. Quick notes \u00b6 Use bash or alternative such as zsh. Examples are based on the adult dataset from the UCI Machine Learning repository (Census Income dataset). This data set is commonly used to predict whether income exceeds $50K/yr based on census data. With 48842 rows and 14 attributes. Move to the directory with cd <dir> and print the current working directory with pwd . Move up with cd .. . Count with wc \u00b6 1 2 # count lines wc -l adult.data 1 32562 adult.data 1 2 # count words wc -w adult.data 1 488415 adult.data 1 ls -l 1 2 3 4 5 total 9540 -rw-rw-r-- 1 ugo ugo 3974305 jan 17 19:47 adult.data drwxrwxr-x 2 ugo ugo 4096 jan 17 19:59 adult -rw-rw-r-- 1 ugo ugo 5776165 jan 19 14:37 os -rw-rw-r-- 1 ugo ugo 6619 jan 19 14:37 Usefull shell commands for Data Science.ipynb 1 ls -l folder 1 2 3 total 0 -rw-rw-r-- 1 ugo ugo 0 jan 17 19:57 Nouveau document 1 -rw-rw-r-- 1 ugo ugo 0 jan 17 19:57 Nouveau document 2 1 2 # count files ls -l folder | wc -l 1 3 1 2 # print head (10 by default or -n) head -n 2 adult.data 1 2 39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, &lt;=50K 50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, &lt;=50K Concatenate with cat \u00b6 Print a file content with cat adult.data . Concatenate files and create (replace) a file with > . >> will appends. 1 cat adult.data adult.data > target_file.csv Add the header to the original file. 1 echo \"age,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,native-country,class\" > header.csv Add the header and rename the file. 1 cat header.csv adult.data > adult.csv Check the first and last row (the default is 10 lines unless specified otherwise). 1 head -n 1 adult.csv 1 age,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,native-country,class 1 tail -n 2 adult.csv 1 52, Self-emp-inc, 287927, HS-grad, 9, Married-civ-spouse, Exec-managerial, Wife, White, Female, 15024, 0, 40, United-States, &gt;50K Modify with sed \u00b6 When a file is corrupted or badly formatted, with no UTF-8 characters or misplaced comma. 1 sed \"s/<string to replace>/<string to replace it with>/g\" <source_file> > <target_file>. Replace ? for missing value with NaN . First, count the instances. 1 grep \", ?,\" adult.csv | wc -l 1 2399 Second, replace all the columns with ? \u2026 \"s/<string to replace>/\" . by an empty string\u2026 \"/<string to replace it with>/g\" . Use column delimiter , . 1 sed \"s/, ?,/,,/g\" adult.csv > adult_v2.csv Subset \u00b6 Large file (30M rows and more). Sample the head or the tail. Extract the head (120 lines). 1 head -n 120 adult_v2.csv > adult_v3.csv Extract the tail (12 lines). 1 tail -n 12 adult_v2.csv > adult_v4.csv Extract 20 lines starting at line 100. 1 head -n 120 adult_v2.csv | tail -n 20 > adult_sample.csv Add the header to the file without. 1 cat header.csv adult_v4.csv > adult_v4_with_header.csv 1 cat header.csv adult_sample.csv > adult_sample_with_header.csv Find duplicates with uniq \u00b6 Find adjacent repeated lines in a file. uniq -c adds the repetition count to each line. uniq -d only outputs duplicate lines. uniq -u only outputs unique lines. First, sort the file to bunch duplicates together. Second, count the duplicates. 1 sort adult_v2.csv | uniq -d | wc -l 1 23 Third, sort the file again, find the duplicates, sort the results in reverse, and output the first 3 duplicates. 1 sort adult_v2.csv | uniq -c | sort -r | head -n 3 1 2 3 3 25, Private, 195994, 1st-4th, 2, Never-married, Priv-house-serv, Not-in-family, White, Female, 0, 0, 40, Guatemala, &lt;=50K 2 90, Private, 52386, Some-college, 10, Never-married, Other-service, Not-in-family, Asian-Pac-Islander, Male, 0, 0, 35, United-States, &lt;=50K 2 49, Self-emp-not-inc, 43479, Some-college, 10, Married-civ-spouse, Craft-repair, Husband, White, Male, 0, 0, 40, United-States, &lt;=50K Select columns with cut \u00b6 Select a particular column. -d specifies the column delimiter and -f specifies the columns. Find the number of unique values taken by the categorical variable workclass (2 nd column of the file) and print the head of the results. 1 cut -d \",\" -f 2 adult_v2.csv | head -3 1 2 3 workclass State-gov Self-emp-not-inc Repeat, but this time, sort the results and find the duplicates. 1 cut -d \",\" -f 2 adult_v2.csv | sort | uniq -c 1 2 3 4 5 6 7 8 9 10 1837 960 Federal-gov 2093 Local-gov 7 Never-worked 22696 Private 1116 Self-emp-inc 2541 Self-emp-not-inc 1298 State-gov 14 Without-pay 1 workclass Loop \u00b6 Work of several files with loops. Replace one character with another within all files inside a directory. Here is how to declare variables and call variables. 1 2 3 varname1 = 10 varname2 = 'hello' varname3 = 123 .4 1 echo $varname1 1 10 1 echo $varname2 1 hello 1 echo $varname3 1 123.4 First, declare two variables. Second, loop through the folder with for . Third, replace the character. Finally, for each file, create a new file. 1 2 3 4 5 6 replace_source = ' ' replace_target = '_' for filename in ./*.csv ; do new_filename = ${ filename // $replace_source / $replace_target } mv \" $filename \" \" $new_filename \" done With the while loop. However, for loops are faster. 1 2 3 4 5 6 replace_source = ' ' replace_target = '_' while true ; do new_filename = ${ filename // $replace_source / $replace_target } mv \" $filename \" \" $new_filename \" done","title":"Useful Shell Commands for Data Science"},{"location":"Usefull+Shell+Commands+for+Data+Science/#quick-notes","text":"Use bash or alternative such as zsh. Examples are based on the adult dataset from the UCI Machine Learning repository (Census Income dataset). This data set is commonly used to predict whether income exceeds $50K/yr based on census data. With 48842 rows and 14 attributes. Move to the directory with cd <dir> and print the current working directory with pwd . Move up with cd .. .","title":"Quick notes"},{"location":"Usefull+Shell+Commands+for+Data+Science/#count-with-wc","text":"1 2 # count lines wc -l adult.data 1 32562 adult.data 1 2 # count words wc -w adult.data 1 488415 adult.data 1 ls -l 1 2 3 4 5 total 9540 -rw-rw-r-- 1 ugo ugo 3974305 jan 17 19:47 adult.data drwxrwxr-x 2 ugo ugo 4096 jan 17 19:59 adult -rw-rw-r-- 1 ugo ugo 5776165 jan 19 14:37 os -rw-rw-r-- 1 ugo ugo 6619 jan 19 14:37 Usefull shell commands for Data Science.ipynb 1 ls -l folder 1 2 3 total 0 -rw-rw-r-- 1 ugo ugo 0 jan 17 19:57 Nouveau document 1 -rw-rw-r-- 1 ugo ugo 0 jan 17 19:57 Nouveau document 2 1 2 # count files ls -l folder | wc -l 1 3 1 2 # print head (10 by default or -n) head -n 2 adult.data 1 2 39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, &lt;=50K 50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, &lt;=50K","title":"Count with wc"},{"location":"Usefull+Shell+Commands+for+Data+Science/#concatenate-with-cat","text":"Print a file content with cat adult.data . Concatenate files and create (replace) a file with > . >> will appends. 1 cat adult.data adult.data > target_file.csv Add the header to the original file. 1 echo \"age,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,native-country,class\" > header.csv Add the header and rename the file. 1 cat header.csv adult.data > adult.csv Check the first and last row (the default is 10 lines unless specified otherwise). 1 head -n 1 adult.csv 1 age,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,native-country,class 1 tail -n 2 adult.csv 1 52, Self-emp-inc, 287927, HS-grad, 9, Married-civ-spouse, Exec-managerial, Wife, White, Female, 15024, 0, 40, United-States, &gt;50K","title":"Concatenate with cat"},{"location":"Usefull+Shell+Commands+for+Data+Science/#modify-with-sed","text":"When a file is corrupted or badly formatted, with no UTF-8 characters or misplaced comma. 1 sed \"s/<string to replace>/<string to replace it with>/g\" <source_file> > <target_file>. Replace ? for missing value with NaN . First, count the instances. 1 grep \", ?,\" adult.csv | wc -l 1 2399 Second, replace all the columns with ? \u2026 \"s/<string to replace>/\" . by an empty string\u2026 \"/<string to replace it with>/g\" . Use column delimiter , . 1 sed \"s/, ?,/,,/g\" adult.csv > adult_v2.csv","title":"Modify with sed"},{"location":"Usefull+Shell+Commands+for+Data+Science/#subset","text":"Large file (30M rows and more). Sample the head or the tail. Extract the head (120 lines). 1 head -n 120 adult_v2.csv > adult_v3.csv Extract the tail (12 lines). 1 tail -n 12 adult_v2.csv > adult_v4.csv Extract 20 lines starting at line 100. 1 head -n 120 adult_v2.csv | tail -n 20 > adult_sample.csv Add the header to the file without. 1 cat header.csv adult_v4.csv > adult_v4_with_header.csv 1 cat header.csv adult_sample.csv > adult_sample_with_header.csv","title":"Subset"},{"location":"Usefull+Shell+Commands+for+Data+Science/#find-duplicates-with-uniq","text":"Find adjacent repeated lines in a file. uniq -c adds the repetition count to each line. uniq -d only outputs duplicate lines. uniq -u only outputs unique lines. First, sort the file to bunch duplicates together. Second, count the duplicates. 1 sort adult_v2.csv | uniq -d | wc -l 1 23 Third, sort the file again, find the duplicates, sort the results in reverse, and output the first 3 duplicates. 1 sort adult_v2.csv | uniq -c | sort -r | head -n 3 1 2 3 3 25, Private, 195994, 1st-4th, 2, Never-married, Priv-house-serv, Not-in-family, White, Female, 0, 0, 40, Guatemala, &lt;=50K 2 90, Private, 52386, Some-college, 10, Never-married, Other-service, Not-in-family, Asian-Pac-Islander, Male, 0, 0, 35, United-States, &lt;=50K 2 49, Self-emp-not-inc, 43479, Some-college, 10, Married-civ-spouse, Craft-repair, Husband, White, Male, 0, 0, 40, United-States, &lt;=50K","title":"Find duplicates with uniq"},{"location":"Usefull+Shell+Commands+for+Data+Science/#select-columns-with-cut","text":"Select a particular column. -d specifies the column delimiter and -f specifies the columns. Find the number of unique values taken by the categorical variable workclass (2 nd column of the file) and print the head of the results. 1 cut -d \",\" -f 2 adult_v2.csv | head -3 1 2 3 workclass State-gov Self-emp-not-inc Repeat, but this time, sort the results and find the duplicates. 1 cut -d \",\" -f 2 adult_v2.csv | sort | uniq -c 1 2 3 4 5 6 7 8 9 10 1837 960 Federal-gov 2093 Local-gov 7 Never-worked 22696 Private 1116 Self-emp-inc 2541 Self-emp-not-inc 1298 State-gov 14 Without-pay 1 workclass","title":"Select columns with cut"},{"location":"Usefull+Shell+Commands+for+Data+Science/#loop","text":"Work of several files with loops. Replace one character with another within all files inside a directory. Here is how to declare variables and call variables. 1 2 3 varname1 = 10 varname2 = 'hello' varname3 = 123 .4 1 echo $varname1 1 10 1 echo $varname2 1 hello 1 echo $varname3 1 123.4 First, declare two variables. Second, loop through the folder with for . Third, replace the character. Finally, for each file, create a new file. 1 2 3 4 5 6 replace_source = ' ' replace_target = '_' for filename in ./*.csv ; do new_filename = ${ filename // $replace_source / $replace_target } mv \" $filename \" \" $new_filename \" done With the while loop. However, for loops are faster. 1 2 3 4 5 6 replace_source = ' ' replace_target = '_' while true ; do new_filename = ${ filename // $replace_source / $replace_target } mv \" $filename \" \" $new_filename \" done","title":"Loop"},{"location":"Web_CS/","text":"Foreword Cheat sheets. HTML \u00b6 HTML Markups . PDF. HTML Basics . PDF. Core HTML . PDF only. HTML Character Entities 1 . PDF. HTML Character Entities 2 . PDF. HTML5 Quick Reference . PDF only. CSS \u00b6 CSS Basics . PDF. Blueprint CSS . PDF. Web Programming \u00b6 Web Programming . PDF. HTTP Status Code 1 . PDF. HTTP Status Code 2 . PDF. Project Management \u00b6 Scrum . PDF only.","title":"Web Cheat Sheets"},{"location":"Web_CS/#html","text":"HTML Markups . PDF. HTML Basics . PDF. Core HTML . PDF only. HTML Character Entities 1 . PDF. HTML Character Entities 2 . PDF. HTML5 Quick Reference . PDF only.","title":"HTML"},{"location":"Web_CS/#css","text":"CSS Basics . PDF. Blueprint CSS . PDF.","title":"CSS"},{"location":"Web_CS/#web-programming","text":"Web Programming . PDF. HTTP Status Code 1 . PDF. HTTP Status Code 2 . PDF.","title":"Web Programming"},{"location":"Web_CS/#project-management","text":"Scrum . PDF only.","title":"Project Management"},{"location":"embedding/","text":"Foreword Notes, snippets, and results. Embedding outputs \u00b6 Embedding static visualizations from packages can be done with image outputs (.png, .pdf) or HTML outputs. We can embed just about any HTML snippet with <iframe> or <embed> . 1 <iframe seamless src= \"../leaflet_frag.html\" width= 520px height= 520px ></iframe> 1 <embed seamless src= \"../leaflet_frag.html\" width= 520px height= 520px ></embed>","title":"Embedding HTML into HTML"},{"location":"embedding/#embedding-outputs","text":"Embedding static visualizations from packages can be done with image outputs (.png, .pdf) or HTML outputs. We can embed just about any HTML snippet with <iframe> or <embed> . 1 <iframe seamless src= \"../leaflet_frag.html\" width= 520px height= 520px ></iframe> 1 <embed seamless src= \"../leaflet_frag.html\" width= 520px height= 520px ></embed>","title":"Embedding outputs"},{"location":"latex_snippets/","text":"Foreword Snippets and notes. Cheat Sheet \u00b6 LaTeX Font Packages . PDF only. LaTeX Cheat Sheet, 4p . PDF only. LaTeX Cheat Sheet, 2p . PDF. Resources \u00b6 The LATEX Project . TEX stack exchange. Write memos . TeXcount web service . Pense-bete pour natbib . TEXample . CTAN . LaTeX Wikibook . TeX . Classes \u00b6 Article \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \\documentclass [12pt] { article } \\title {} \\author {} \\date {} \\begin { document } \\maketitle \\begin { abstract } ... \\end { abstract } \\section {} \\subsection {} \\subsubsection {} \\paragraph {} \\subparagraph {} \\end { document } Report \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \\documentclass [12pt] { report } \\title {} \\author {} \\date {} \\begin { document } \\maketitle \\begin { abstract } ... \\end { abstract } \\tableofcontents \\listoffigures \\listoftables \\chapter {} \\section {} \\subsection {} \\subsubsection {} \\paragraph {} \\subparagraph {} \\end { document } Book \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \\documentclass [12pt] { book } \\title {} \\author {} \\date {} \\begin { document } \\maketitle \\tableofcontents \\listoffigures \\listoftables \\part {} \\chapter {} \\section {} \\subsection {} \\subsubsection {} \\paragraph {} \\subparagraph {} \\end { document } Figure \u00b6 Standard figure \u00b6 1 2 3 4 5 6 7 8 9 10 11 %Preamble \\usepackage { graphicx } %Document \\begin { figure } [htbp] \\centering \\includegraphics [width=\\linewidth] { figure.png } \\caption [short for lof] { long figure caption } \\label { fig:default } \\end { figure } \\ref { fig:default } Side-by-side figure (1x2) \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 %Preamble \\usepackage [lofdepth] { subfig } \\usepackage { graphicx } %Document \\begin { figure } [htbp] % h:here; t:top; b:bottom; p:page; default:ht \\centering \\subfloat [short for lof][long subfigure1 caption] { \\includegraphics [width=0.45\\linewidth] { figure1.png } \\label { subfig:fig1 } } \\subfloat [short for lof][long subfigure2 caption] { \\includegraphics [width=0.45\\linewidth] { figure2.png } \\label { subfig:fig2 } } \\caption [short for lof] { long figure caption } \\label { fig:fig1 } \\end { figure } \\ref { fig:fig1 } and \\subref { subfig:fig1 } Side-by-side figure (2x2) \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 %Preamble \\usepackage [lofdepth,lotdepth] { subfig } \\usepackage { graphicx } %Document \\begin { figure } [ht] \\centering \\subfloat [short for lof][long subfig1 caption] { \\includegraphics [width=0.45\\linewidth] { figure1.png } \\label { subfig:fig1 } } \\subfloat [short for lof][long subfig2 caption] { \\includegraphics [width=0.45\\linewidth] { figure2.png } \\label { subfig:fig2 } } \\subfloat [short for lof][long subfig3 caption] { \\includegraphics [width=0.45\\linewidth] { figure3.png } \\label { subfig:fig3 } } \\subfloat [short for lof][long subfig4 caption] { \\includegraphics [width=0.45\\linewidth] { figure4.png } \\label { subfig:fig4 } } \\caption [short for lof] { long figure caption } \\label { fig:fig1 } \\end { figure } \\ref { fig:fig1 } and \\subref { subfig:fig1 } Sideways-figure 1 2 3 4 5 6 7 8 9 10 % Preamble \\usepackage { rotating } % Document \\begin { sidewaysfigure } \\centering \\includegraphics { figure.png } \\caption { Sideways figure. } \\label { fig:swfig } \\end { sidewaysfigure } Wrap text around figure 1 2 3 4 5 6 7 8 9 10 %Preamble \\usepackage { wrapfig, graphicx } % Document \\begin { wrapfigure }{ l }{ 0.5 \\textwidth } % l for left, r for right \\centering \\includegraphics [width=0.45\\textwidth] { test } \\caption { Text wrapped around figure } \\label { fig:wrapfig } \\end { wrapfigure } Tables \u00b6 Standard table \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 \\begin { table } [htbp] % h:here; t:top; b:bottom; p:page; default:ht \\caption { default } \\centering \\begin { tabular }{ lccr } \\hline - & - & - & - \\\\ \\hline - & - & - & - \\\\ - & - & - & - \\\\ \\hline \\end { tabular } \\label { tab:def } \\end { table } % Side-by-side table \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 %Preamble \\usepackage [lotdepth] { subfig } %Document \\begin { table } [htbp] % h:here; t:top; b:bottom; p:page; default:ht \\centering \\subfloat [short for lot subtable1][long subtable1 caption] { \\begin { tabular }{ lccr } \\hline - & - & - & - \\\\ \\hline - & - & - & - \\\\ - & - & - & - \\\\ \\hline \\end { tabular } \\label { subtab:tab1 } } \\subfloat [short for lot subtable2][long subtable2 caption] { \\begin { tabular }{ lccr } \\hline - & - & - & - \\\\ \\hline - & - & - & - \\\\ - & - & - & - \\\\ \\hline \\end { tabular } \\label { subtab:tab2 } } \\caption [short for lot] { long table caption } \\label { tab:tab1 } \\end { table } \\ref { tab:tab1 } and \\subref { subtab:tab1 } Sideways-table \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 % Preamble \\usepackage { rotating } % Document \\begin { sidewaystable } \\caption { Sideways table. } \\centering \\begin { tabular }{ lccr } \\hline - & - & - & - \\\\ \\hline - & - & - & - \\\\ - & - & - & - \\\\ \\hline \\end { tabular } \\label { tab:swtab } \\end { sidewaystable } Multi-page table (longtable) \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 %Preamble \\usepackage { longtable } % Document \\begin { center } \\begin { longtable }{ |c|c|c|c| } \\caption { A simple longtable example } \\\\ \\hline \\textbf { First entry } & \\textbf { Second entry } & \\textbf { Third entry } & \\textbf { Fourth entry } \\\\ \\hline \\endfirsthead \\multicolumn { 4 }{ c } % { \\tablename\\ \\thetable\\ -- \\textit { Continued from previous page }} \\\\ \\hline \\textbf { First entry } & \\textbf { Second entry } & \\textbf { Third entry } & \\textbf { Fourth entry } \\\\ \\hline \\endhead \\hline \\multicolumn { 4 }{ r }{ \\textit { Continued on next page }} \\\\ \\endfoot \\hline \\multicolumn { 4 }{ r }{ \\textit { End of table }} \\\\ \\endlastfoot 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ \\end { longtable } \\end { center } List \u00b6 Bulleted list: itemize \u00b6 1 2 3 4 5 \\begin { itemize } \\item \\item \\item \\end { itemize } Numbered list: enumerate \u00b6 1 2 3 4 5 \\begin { enumerate } \\item \\item \\item \\end { enumerate } Description \u00b6 1 2 3 4 5 \\begin { description } \\item [] \\item [] \\item [] \\end { description } Inline enumeration \u00b6 1 2 3 4 5 6 7 8 9 %Preamble \\usepackage { paralist } %Document \\begin { inparaenum } \\item \\item \\item \\end { inparaenum } Beamer Presentation \u00b6 Table of Content \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \\documentclass { beamer } \\usetheme { Berkeley } \\begin { document } \\begin { frame } \\tableofcontents \\end { frame } \\section { First } \\begin { frame }{ First section slide } ... \\end { frame } \\section { Second } %Contents with current section highlighted \\frame { \\tableofcontents [currentsection] } \\begin { frame }{ Second section slide } ... \\end { frame } \\end { document } List \u00b6 1 2 3 4 5 6 7 \\begin { frame }{ Enumerate } \\begin { enumerate } \\item \\item \\item \\end { enumerate } \\end { frame } 1 2 3 4 5 6 7 \\begin { frame }{ Itemize } \\begin { itemize } \\item \\item \\item \\end { itemize } \\end { frame } Side-by-side figure/table/list with columns \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \\begin { frame }{ Side-by-side figure/table/text with columns } \\begin { columns } \\begin { column } [C] { .45 \\textwidth } % the T argument instead of C will align the top of the two columns, instead of the centers \\rule { \\textwidth }{ 0.75 \\textwidth } \\end { column } \\begin { column } [C] { .45 \\textwidth } \\begin { enumerate } % numbered list \\item \\item \\item \\end { enumerate } \\end { column } \\end { columns } \\end { frame } Side-by-side figure/table/list with minipage 1 2 3 4 5 6 7 8 9 10 11 12 13 \\begin { frame }{ Side-by-side figure/table/text with minipage } \\begin { minipage }{ 0.45 \\textwidth } % figure \\rule { \\textwidth }{ 0.75 \\textwidth } \\end { minipage } \\qquad \\begin { minipage }{ 0.45 \\textwidth } \\begin { enumerate } % numbered list \\item \\item \\item \\end { enumerate } \\end { minipage } \\end { frame } Installation \u00b6 Windows. proTeXt (MiKTeX-based + the editor TeXnicCenter). MiKTeX . TeXnicCenter (from proTeXt). LEd . TeXstudio. Texmaker. LyX. JabRef. Linux. TeX Live . TeXstudio. Texmaker. Kile. Latexila. LyX.","title":"LaTeX Snippets"},{"location":"latex_snippets/#cheat-sheet","text":"LaTeX Font Packages . PDF only. LaTeX Cheat Sheet, 4p . PDF only. LaTeX Cheat Sheet, 2p . PDF.","title":"Cheat Sheet"},{"location":"latex_snippets/#resources","text":"The LATEX Project . TEX stack exchange. Write memos . TeXcount web service . Pense-bete pour natbib . TEXample . CTAN . LaTeX Wikibook . TeX .","title":"Resources"},{"location":"latex_snippets/#classes","text":"","title":"Classes"},{"location":"latex_snippets/#article","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \\documentclass [12pt] { article } \\title {} \\author {} \\date {} \\begin { document } \\maketitle \\begin { abstract } ... \\end { abstract } \\section {} \\subsection {} \\subsubsection {} \\paragraph {} \\subparagraph {} \\end { document }","title":"Article"},{"location":"latex_snippets/#report","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \\documentclass [12pt] { report } \\title {} \\author {} \\date {} \\begin { document } \\maketitle \\begin { abstract } ... \\end { abstract } \\tableofcontents \\listoffigures \\listoftables \\chapter {} \\section {} \\subsection {} \\subsubsection {} \\paragraph {} \\subparagraph {} \\end { document }","title":"Report"},{"location":"latex_snippets/#book","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \\documentclass [12pt] { book } \\title {} \\author {} \\date {} \\begin { document } \\maketitle \\tableofcontents \\listoffigures \\listoftables \\part {} \\chapter {} \\section {} \\subsection {} \\subsubsection {} \\paragraph {} \\subparagraph {} \\end { document }","title":"Book"},{"location":"latex_snippets/#figure","text":"","title":"Figure"},{"location":"latex_snippets/#standard-figure","text":"1 2 3 4 5 6 7 8 9 10 11 %Preamble \\usepackage { graphicx } %Document \\begin { figure } [htbp] \\centering \\includegraphics [width=\\linewidth] { figure.png } \\caption [short for lof] { long figure caption } \\label { fig:default } \\end { figure } \\ref { fig:default }","title":"Standard figure"},{"location":"latex_snippets/#side-by-side-figure-1x2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 %Preamble \\usepackage [lofdepth] { subfig } \\usepackage { graphicx } %Document \\begin { figure } [htbp] % h:here; t:top; b:bottom; p:page; default:ht \\centering \\subfloat [short for lof][long subfigure1 caption] { \\includegraphics [width=0.45\\linewidth] { figure1.png } \\label { subfig:fig1 } } \\subfloat [short for lof][long subfigure2 caption] { \\includegraphics [width=0.45\\linewidth] { figure2.png } \\label { subfig:fig2 } } \\caption [short for lof] { long figure caption } \\label { fig:fig1 } \\end { figure } \\ref { fig:fig1 } and \\subref { subfig:fig1 }","title":"Side-by-side figure (1x2)"},{"location":"latex_snippets/#side-by-side-figure-2x2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 %Preamble \\usepackage [lofdepth,lotdepth] { subfig } \\usepackage { graphicx } %Document \\begin { figure } [ht] \\centering \\subfloat [short for lof][long subfig1 caption] { \\includegraphics [width=0.45\\linewidth] { figure1.png } \\label { subfig:fig1 } } \\subfloat [short for lof][long subfig2 caption] { \\includegraphics [width=0.45\\linewidth] { figure2.png } \\label { subfig:fig2 } } \\subfloat [short for lof][long subfig3 caption] { \\includegraphics [width=0.45\\linewidth] { figure3.png } \\label { subfig:fig3 } } \\subfloat [short for lof][long subfig4 caption] { \\includegraphics [width=0.45\\linewidth] { figure4.png } \\label { subfig:fig4 } } \\caption [short for lof] { long figure caption } \\label { fig:fig1 } \\end { figure } \\ref { fig:fig1 } and \\subref { subfig:fig1 } Sideways-figure 1 2 3 4 5 6 7 8 9 10 % Preamble \\usepackage { rotating } % Document \\begin { sidewaysfigure } \\centering \\includegraphics { figure.png } \\caption { Sideways figure. } \\label { fig:swfig } \\end { sidewaysfigure } Wrap text around figure 1 2 3 4 5 6 7 8 9 10 %Preamble \\usepackage { wrapfig, graphicx } % Document \\begin { wrapfigure }{ l }{ 0.5 \\textwidth } % l for left, r for right \\centering \\includegraphics [width=0.45\\textwidth] { test } \\caption { Text wrapped around figure } \\label { fig:wrapfig } \\end { wrapfigure }","title":"Side-by-side figure (2x2)"},{"location":"latex_snippets/#tables","text":"","title":"Tables"},{"location":"latex_snippets/#standard-table","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 \\begin { table } [htbp] % h:here; t:top; b:bottom; p:page; default:ht \\caption { default } \\centering \\begin { tabular }{ lccr } \\hline - & - & - & - \\\\ \\hline - & - & - & - \\\\ - & - & - & - \\\\ \\hline \\end { tabular } \\label { tab:def } \\end { table } %","title":"Standard table"},{"location":"latex_snippets/#side-by-side-table","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 %Preamble \\usepackage [lotdepth] { subfig } %Document \\begin { table } [htbp] % h:here; t:top; b:bottom; p:page; default:ht \\centering \\subfloat [short for lot subtable1][long subtable1 caption] { \\begin { tabular }{ lccr } \\hline - & - & - & - \\\\ \\hline - & - & - & - \\\\ - & - & - & - \\\\ \\hline \\end { tabular } \\label { subtab:tab1 } } \\subfloat [short for lot subtable2][long subtable2 caption] { \\begin { tabular }{ lccr } \\hline - & - & - & - \\\\ \\hline - & - & - & - \\\\ - & - & - & - \\\\ \\hline \\end { tabular } \\label { subtab:tab2 } } \\caption [short for lot] { long table caption } \\label { tab:tab1 } \\end { table } \\ref { tab:tab1 } and \\subref { subtab:tab1 }","title":"Side-by-side table"},{"location":"latex_snippets/#sideways-table","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 % Preamble \\usepackage { rotating } % Document \\begin { sidewaystable } \\caption { Sideways table. } \\centering \\begin { tabular }{ lccr } \\hline - & - & - & - \\\\ \\hline - & - & - & - \\\\ - & - & - & - \\\\ \\hline \\end { tabular } \\label { tab:swtab } \\end { sidewaystable }","title":"Sideways-table"},{"location":"latex_snippets/#multi-page-table-longtable","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 %Preamble \\usepackage { longtable } % Document \\begin { center } \\begin { longtable }{ |c|c|c|c| } \\caption { A simple longtable example } \\\\ \\hline \\textbf { First entry } & \\textbf { Second entry } & \\textbf { Third entry } & \\textbf { Fourth entry } \\\\ \\hline \\endfirsthead \\multicolumn { 4 }{ c } % { \\tablename\\ \\thetable\\ -- \\textit { Continued from previous page }} \\\\ \\hline \\textbf { First entry } & \\textbf { Second entry } & \\textbf { Third entry } & \\textbf { Fourth entry } \\\\ \\hline \\endhead \\hline \\multicolumn { 4 }{ r }{ \\textit { Continued on next page }} \\\\ \\endfoot \\hline \\multicolumn { 4 }{ r }{ \\textit { End of table }} \\\\ \\endlastfoot 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 3 & 4 \\\\ \\end { longtable } \\end { center }","title":"Multi-page table (longtable)"},{"location":"latex_snippets/#list","text":"","title":"List"},{"location":"latex_snippets/#bulleted-list-itemize","text":"1 2 3 4 5 \\begin { itemize } \\item \\item \\item \\end { itemize }","title":"Bulleted list: itemize"},{"location":"latex_snippets/#numbered-list-enumerate","text":"1 2 3 4 5 \\begin { enumerate } \\item \\item \\item \\end { enumerate }","title":"Numbered list: enumerate"},{"location":"latex_snippets/#description","text":"1 2 3 4 5 \\begin { description } \\item [] \\item [] \\item [] \\end { description }","title":"Description"},{"location":"latex_snippets/#inline-enumeration","text":"1 2 3 4 5 6 7 8 9 %Preamble \\usepackage { paralist } %Document \\begin { inparaenum } \\item \\item \\item \\end { inparaenum }","title":"Inline enumeration"},{"location":"latex_snippets/#beamer-presentation","text":"","title":"Beamer Presentation"},{"location":"latex_snippets/#table-of-content","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \\documentclass { beamer } \\usetheme { Berkeley } \\begin { document } \\begin { frame } \\tableofcontents \\end { frame } \\section { First } \\begin { frame }{ First section slide } ... \\end { frame } \\section { Second } %Contents with current section highlighted \\frame { \\tableofcontents [currentsection] } \\begin { frame }{ Second section slide } ... \\end { frame } \\end { document }","title":"Table of Content"},{"location":"latex_snippets/#list_1","text":"1 2 3 4 5 6 7 \\begin { frame }{ Enumerate } \\begin { enumerate } \\item \\item \\item \\end { enumerate } \\end { frame } 1 2 3 4 5 6 7 \\begin { frame }{ Itemize } \\begin { itemize } \\item \\item \\item \\end { itemize } \\end { frame }","title":"List"},{"location":"latex_snippets/#side-by-side-figuretablelist-with-columns","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \\begin { frame }{ Side-by-side figure/table/text with columns } \\begin { columns } \\begin { column } [C] { .45 \\textwidth } % the T argument instead of C will align the top of the two columns, instead of the centers \\rule { \\textwidth }{ 0.75 \\textwidth } \\end { column } \\begin { column } [C] { .45 \\textwidth } \\begin { enumerate } % numbered list \\item \\item \\item \\end { enumerate } \\end { column } \\end { columns } \\end { frame } Side-by-side figure/table/list with minipage 1 2 3 4 5 6 7 8 9 10 11 12 13 \\begin { frame }{ Side-by-side figure/table/text with minipage } \\begin { minipage }{ 0.45 \\textwidth } % figure \\rule { \\textwidth }{ 0.75 \\textwidth } \\end { minipage } \\qquad \\begin { minipage }{ 0.45 \\textwidth } \\begin { enumerate } % numbered list \\item \\item \\item \\end { enumerate } \\end { minipage } \\end { frame }","title":"Side-by-side figure/table/list with columns"},{"location":"latex_snippets/#installation","text":"Windows. proTeXt (MiKTeX-based + the editor TeXnicCenter). MiKTeX . TeXnicCenter (from proTeXt). LEd . TeXstudio. Texmaker. LyX. JabRef. Linux. TeX Live . TeXstudio. Texmaker. Kile. Latexila. LyX.","title":"Installation"},{"location":"tests/","text":"Foreword This page tests possibilities. Sane list \u00b6 \u2018Apprenez \u00e0 programmer en Python\u2019 \u201cAutomate the Boring Stuff with Python\u201d \u2018Apprenez \u00e0 programmer en Python\u2019 \u201cAutomate the Boring Stuff with Python\u201d \u2018Apprenez \u00e0 programmer en Python\u2019 \u201cAutomate the Boring Stuff with Python\u201d Smart symbols \u00b6 \u00b1 \u2192 \u2190 \u2194 \u2260 \u00bc 1 st 2 nd 3 rd 4 th \u00a9 \u2122 \u2018a\u2019 \u201cb\u201d \u00abc\u00bb \u2026 - \u2013 \u2014 Sub/Superscripts \u00b6 Enter superscript: 2 10 is 1024. superscript pip install MarkdownSuperscript Enter subscript: The molecular composition of water is H 2 O. subscript pip install MarkdownSubscript Embed videos \u00b6 !embed pip install pyembed-markdown . Add pyembed.markdown . Build tables \u00b6 a b c d Without outside borders: First Header Second Header Third Header Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell With outside borders: First Header Second Header Third Header Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell Aligned: First Header Second Header Third Header Left Center Right Left Center Right Arithmatex \u00b6 Inline: \\frac{1}{n} \\frac{1}{n} and\u2026 Block: \\sqrt{9} \\sqrt{9} Footnotes \u00b6 Lorem ipsum 1 Lorem ipsum Lorem ipsum 2 Admonition \u00b6 Tip Lorem ipsum Code block \u00b6 1 2 3 a = 1 b = 1 print ( a ) Code hilite \u00b6 1 2 3 a = 1 b = 1 print ( a ) Inline code hilite \u00b6 a = 1 Bold italic \u00b6 Lorem ipsum Underline \u00b6 Lorem ipsum Highlight \u00b6 Lorem ipsum Emoji \u00b6 Lorem ipsum \u21a9 Lorem ipsum 2.1 Lorem ipsum 2.2 \u21a9","title":"Tests"},{"location":"tests/#sane-list","text":"\u2018Apprenez \u00e0 programmer en Python\u2019 \u201cAutomate the Boring Stuff with Python\u201d \u2018Apprenez \u00e0 programmer en Python\u2019 \u201cAutomate the Boring Stuff with Python\u201d \u2018Apprenez \u00e0 programmer en Python\u2019 \u201cAutomate the Boring Stuff with Python\u201d","title":"Sane list"},{"location":"tests/#smart-symbols","text":"\u00b1 \u2192 \u2190 \u2194 \u2260 \u00bc 1 st 2 nd 3 rd 4 th \u00a9 \u2122 \u2018a\u2019 \u201cb\u201d \u00abc\u00bb \u2026 - \u2013 \u2014","title":"Smart symbols"},{"location":"tests/#subsuperscripts","text":"Enter superscript: 2 10 is 1024. superscript pip install MarkdownSuperscript Enter subscript: The molecular composition of water is H 2 O. subscript pip install MarkdownSubscript","title":"Sub/Superscripts"},{"location":"tests/#embed-videos","text":"!embed pip install pyembed-markdown . Add pyembed.markdown .","title":"Embed videos"},{"location":"tests/#build-tables","text":"a b c d Without outside borders: First Header Second Header Third Header Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell With outside borders: First Header Second Header Third Header Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell Aligned: First Header Second Header Third Header Left Center Right Left Center Right","title":"Build tables"},{"location":"tests/#arithmatex","text":"Inline: \\frac{1}{n} \\frac{1}{n} and\u2026 Block: \\sqrt{9} \\sqrt{9}","title":"Arithmatex"},{"location":"tests/#footnotes","text":"Lorem ipsum 1 Lorem ipsum Lorem ipsum 2","title":"Footnotes"},{"location":"tests/#admonition","text":"Tip Lorem ipsum","title":"Admonition"},{"location":"tests/#code-block","text":"1 2 3 a = 1 b = 1 print ( a )","title":"Code block"},{"location":"tests/#code-hilite","text":"1 2 3 a = 1 b = 1 print ( a )","title":"Code hilite"},{"location":"tests/#inline-code-hilite","text":"a = 1","title":"Inline code hilite"},{"location":"tests/#bold-italic","text":"Lorem ipsum","title":"Bold italic"},{"location":"tests/#underline","text":"Lorem ipsum","title":"Underline"},{"location":"tests/#highlight","text":"Lorem ipsum","title":"Highlight"},{"location":"tests/#emoji","text":"Lorem ipsum \u21a9 Lorem ipsum 2.1 Lorem ipsum 2.2 \u21a9","title":"Emoji"},{"location":"visual_perception/","text":"Foreword Notes. A summary from this article . From Light to Thought \u00b6 We don\u2019t actually see physical objects; we see light, either emitted by objects or reflected off of their surfaces. Role and Limitations of Memory \u00b6 Our brains use various types of storage to hold information while it\u2019s being processed and, in some cases, to store it for later use. A common problem in graph design: the meaning of the nine separate data sets \u2014 represented by the nine differently colored lines \u2014 can\u2019t be concurrently held in short-term memory. The readers are forced to shift attention back and forth between the legend and the lines of data to remind themselves over and over what each line represents. If you want someone to make sense of the graph as a whole, then you must limit the number of data components that encode distinct meanings to seven at most \u2014 and safer yet, to no more than five . Fundamental Attributes of Sight \u00b6 The visual differences between the shapes of the various numbers that appear in the figure below are too complex to process preattentively. To count all the 5\u2019s involves serial attentive processing. Now count the 5\u2019s in the same set of numbers in the figure below. This time perception was easy and immediate, because the 5\u2019s were encoded with a different preattentive visual attribute from the other numbers. If you want some of the data to stand out from the rest, you should encode it using different preattentive attributes . Here\u2019s a list of the preattentive attributes that are of particular use in visual displays of data: Color intensity , such as different shades of gray ranging from white to black (that is, \u201cgrayscale\u201d) can be quantitatively perceived to a degree\u2014by making one value darker, for example, we can tell that it is greater than another , but not well enough to decode specific shades into specific values without a lot of work. Attributes that can\u2019t be perceived quantitatively can still be used in graphs, but their use is restricted to distinguishing categorical differences , such as the use of hue to distinguish different lines in a line graph, sets of bars in a bar graph, or sets of points in a scatter plot. Thriving With Information \u00b6 The better you understand the strengths and weaknesses of visual perception, the better equipped you\u2019ll be to make use of your readers\u2019 abilities to detect structure and patterns in data when it\u2019s visually displayed. Core Principles (recap) \u00b6 Core Principles (best practices) \u00b6","title":"Visual Perception (visualisation)"},{"location":"visual_perception/#from-light-to-thought","text":"We don\u2019t actually see physical objects; we see light, either emitted by objects or reflected off of their surfaces.","title":"From Light to Thought"},{"location":"visual_perception/#role-and-limitations-of-memory","text":"Our brains use various types of storage to hold information while it\u2019s being processed and, in some cases, to store it for later use. A common problem in graph design: the meaning of the nine separate data sets \u2014 represented by the nine differently colored lines \u2014 can\u2019t be concurrently held in short-term memory. The readers are forced to shift attention back and forth between the legend and the lines of data to remind themselves over and over what each line represents. If you want someone to make sense of the graph as a whole, then you must limit the number of data components that encode distinct meanings to seven at most \u2014 and safer yet, to no more than five .","title":"Role and Limitations of Memory"},{"location":"visual_perception/#fundamental-attributes-of-sight","text":"The visual differences between the shapes of the various numbers that appear in the figure below are too complex to process preattentively. To count all the 5\u2019s involves serial attentive processing. Now count the 5\u2019s in the same set of numbers in the figure below. This time perception was easy and immediate, because the 5\u2019s were encoded with a different preattentive visual attribute from the other numbers. If you want some of the data to stand out from the rest, you should encode it using different preattentive attributes . Here\u2019s a list of the preattentive attributes that are of particular use in visual displays of data: Color intensity , such as different shades of gray ranging from white to black (that is, \u201cgrayscale\u201d) can be quantitatively perceived to a degree\u2014by making one value darker, for example, we can tell that it is greater than another , but not well enough to decode specific shades into specific values without a lot of work. Attributes that can\u2019t be perceived quantitatively can still be used in graphs, but their use is restricted to distinguishing categorical differences , such as the use of hue to distinguish different lines in a line graph, sets of bars in a bar graph, or sets of points in a scatter plot.","title":"Fundamental Attributes of Sight"},{"location":"visual_perception/#thriving-with-information","text":"The better you understand the strengths and weaknesses of visual perception, the better equipped you\u2019ll be to make use of your readers\u2019 abilities to detect structure and patterns in data when it\u2019s visually displayed.","title":"Thriving With Information"},{"location":"visual_perception/#core-principles-recap","text":"","title":"Core Principles (recap)"},{"location":"visual_perception/#core-principles-best-practices","text":"","title":"Core Principles (best practices)"}]}